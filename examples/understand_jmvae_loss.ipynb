{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example to show the effect of the flows and of the functions $f_i$ on the JMVAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an artificial dataset with a 2D latent space\n",
    "\n",
    "# Fix the random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "N = 10000\n",
    "x = dist.Normal(0,1).sample([N])\n",
    "y = dist.Normal(0,1).sample([N])\n",
    "print(x.shape)\n",
    "plt.scatter(x,y)\n",
    "data=[x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10000])\n"
     ]
    }
   ],
   "source": [
    "# We define the joint posterior to be a 2D gaussian with diagonal covariance matrix\n",
    "s1 = 0.1\n",
    "s2 = 0.1\n",
    "\n",
    "def sample_from_joint_posterior(x,y,K=1):\n",
    "    z1 = dist.Normal(x,s1).sample([K])\n",
    "    z2 = dist.Normal(y,s2).sample([K])\n",
    "    \n",
    "    return torch.stack([z1, z2])\n",
    "\n",
    "def sample_from_uni_posterior_x(x,K=1):\n",
    "    z1 = dist.Normal(x,s1).sample([K])\n",
    "    z2 = dist.Normal(0,np.sqrt(1+s2**2)).sample([K])\n",
    "    \n",
    "    return torch.stack([z1,z2])\n",
    "\n",
    "def sample_from_uni_posterior_y(y,K=1):\n",
    "    z1 = dist.Normal(0,np.sqrt(1+s1**2)).sample([K])\n",
    "    z2 = dist.Normal(y,s2).sample([K])\n",
    "    \n",
    "    return torch.stack([z1,z2])\n",
    "\n",
    "z1,z2 = sample_from_joint_posterior(x,y,K=100)\n",
    "print(z1.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from bivae.my_pythae.models.vae_maf import my_VAE_MAF, VAE_MAF_Config\n",
    "from bivae.my_pythae.models.vae import my_VAE\n",
    "from pythae.models.vae import VAEConfig\n",
    "from pythae.models.nn.base_architectures import BaseDecoder, BaseEncoder\n",
    "from pythae.models.base.base_utils import ModelOutput\n",
    "\n",
    "n_hidden_dim = 100\n",
    "\n",
    "class Encoder_VAE_MLP(BaseEncoder):\n",
    "    def __init__(self, args: dict):\n",
    "        BaseEncoder.__init__(self)\n",
    "        self.input_dim = args.input_dim\n",
    "        self.latent_dim = args.latent_dim\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        layers.append(nn.Sequential(nn.Linear(np.prod(args.input_dim), n_hidden_dim), nn.ReLU()))\n",
    "\n",
    "        self.layers = layers\n",
    "        self.depth = len(layers)\n",
    "\n",
    "        self.embedding = nn.Linear(n_hidden_dim, self.latent_dim)\n",
    "        self.log_var = nn.Linear(n_hidden_dim, self.latent_dim)\n",
    "\n",
    "    def forward(self, x, output_layer_levels = None):\n",
    "        output = ModelOutput()\n",
    "\n",
    "        max_depth = self.depth\n",
    "\n",
    "        if output_layer_levels is not None:\n",
    "\n",
    "            assert all(\n",
    "                self.depth >= levels > 0 or levels == -1\n",
    "                for levels in output_layer_levels\n",
    "            ), (\n",
    "                f\"Cannot output layer deeper than depth ({self.depth}). \"\n",
    "                f\"Got ({output_layer_levels}).\"\n",
    "            )\n",
    "\n",
    "            if -1 in output_layer_levels:\n",
    "                max_depth = self.depth\n",
    "            else:\n",
    "                max_depth = max(output_layer_levels)\n",
    "\n",
    "        out = x.reshape(-1, np.prod(self.input_dim))\n",
    "\n",
    "        for i in range(max_depth):\n",
    "            out = self.layers[i](out)\n",
    "\n",
    "            if output_layer_levels is not None:\n",
    "                if i + 1 in output_layer_levels:\n",
    "                    output[f\"embedding_layer_{i+1}\"] = out\n",
    "            if i + 1 == self.depth:\n",
    "                output[\"embedding\"] = self.embedding(out)\n",
    "                output[\"log_covariance\"] = self.log_var(out)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Decoder_AE_MLP(BaseDecoder):\n",
    "    def __init__(self, args: dict):\n",
    "        BaseDecoder.__init__(self)\n",
    "\n",
    "        self.input_dim = args.input_dim\n",
    "\n",
    "        # assert 0, np.prod(args.input_dim)\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        layers.append(nn.Sequential(nn.Linear(args.latent_dim, n_hidden_dim), nn.ReLU()))\n",
    "\n",
    "        layers.append(\n",
    "            nn.Sequential(nn.Linear(n_hidden_dim, int(np.prod(args.input_dim))), nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "        self.layers = layers\n",
    "        self.depth = len(layers)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, output_layer_levels=None):\n",
    "\n",
    "        output = ModelOutput()\n",
    "\n",
    "        max_depth = self.depth\n",
    "\n",
    "        if output_layer_levels is not None:\n",
    "\n",
    "            assert all(\n",
    "                self.depth >= levels > 0 or levels == -1\n",
    "                for levels in output_layer_levels\n",
    "            ), (\n",
    "                f\"Cannot output layer deeper than depth ({self.depth}). \"\n",
    "                f\"Got ({output_layer_levels}).\"\n",
    "            )\n",
    "\n",
    "            if -1 in output_layer_levels:\n",
    "                max_depth = self.depth\n",
    "            else:\n",
    "                max_depth = max(output_layer_levels)\n",
    "\n",
    "        out = z\n",
    "\n",
    "        for i in range(max_depth):\n",
    "            out = self.layers[i](out)\n",
    "\n",
    "            if output_layer_levels is not None:\n",
    "                if i + 1 in output_layer_levels:\n",
    "                    output[f\"reconstruction_layer_{i+1}\"] = out\n",
    "            if i + 1 == self.depth:\n",
    "                output[\"reconstruction\"] = out.reshape((z.shape[0],) + self.input_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class my_model(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        vae = my_VAE_MAF\n",
    "        vae_config = VAE_MAF_Config((1,1),latent_dim=2)\n",
    "        e1,e2 = Encoder_VAE_MLP(vae_config), Encoder_VAE_MLP(vae_config)\n",
    "        d1,d2 =Decoder_AE_MLP(vae_config), Decoder_AE_MLP(vae_config)\n",
    "        self.vaes = nn.ModuleList([\n",
    "            vae(model_config=vae_config, encoder=e1, decoder=d1),\n",
    "            vae(model_config=vae_config, encoder=e2, decoder=d2)\n",
    "\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def compute_kld(self, data):\n",
    "        \"\"\" Computes KL(q(z|x,y) || q(z|x)) + KL(q(z|x,y) || q(z|y))\"\"\"\n",
    "\n",
    "        z_xy=sample_from_joint_posterior(data[0],data[1]).squeeze(1).permute(1,0)\n",
    "        # print(z_xy.size())\n",
    "        reg = 0\n",
    "        details_reg = {}\n",
    "        for m, vae in enumerate(self.vaes):\n",
    "            # print(z_xy.shape)\n",
    "            flow_output = vae.flow(z_xy) if hasattr(vae, \"flow\") else vae.inverse_flow(z_xy)\n",
    "            vae_output = vae.encoder(data[m].unsqueeze(1))\n",
    "            mu, log_var, z0 = vae_output.embedding, vae_output.log_covariance, flow_output.out\n",
    "            log_q_z0 = (-0.5 * (log_var + np.log(2*np.pi) + torch.pow(z0 - mu, 2) / torch.exp(log_var))).sum(dim=1)\n",
    "\n",
    "            # kld -= log_q_z0 + flow_output.log_abs_det_jac\n",
    "            # details_reg[f'kld_{m}'] = qz_xy.sum() - (log_q_z0 + flow_output.log_abs_det_jac).sum()\n",
    "            details_reg[f'kld_{m}'] =  - (log_q_z0 + flow_output.log_abs_det_jac).sum()\n",
    "\n",
    "            reg += details_reg[f'kld_{m}']\n",
    "            \n",
    "        return reg, details_reg\n",
    "    \n",
    "    def sample_from_x(self, x, K=100):\n",
    "        \n",
    "        d = torch.stack([x]*K)\n",
    "        z = self.vaes[0](d).z\n",
    "        return z\n",
    "    \n",
    "    def sample_from_y(self, y, K=100):\n",
    "        d = torch.stack([y]*K)\n",
    "        z = self.vaes[1](d).z\n",
    "        return z\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model\n",
      "Epoch 0 : train_loss 6.962843451605902\n",
      "Epoch 0 : test_loss 6.830992065429688\n",
      "saved model\n",
      "Epoch 1 : train_loss 6.791622884114584\n",
      "Epoch 1 : test_loss 6.6606904296875\n",
      "saved model\n",
      "Epoch 2 : train_loss 6.611930962456597\n",
      "Epoch 2 : test_loss 6.506016357421875\n",
      "saved model\n",
      "Epoch 3 : train_loss 6.463098931206598\n",
      "Epoch 3 : test_loss 6.3498558349609375\n",
      "saved model\n",
      "Epoch 4 : train_loss 6.321701917860243\n",
      "Epoch 4 : test_loss 6.228921264648437\n",
      "saved model\n",
      "Epoch 5 : train_loss 6.21149016655816\n",
      "Epoch 5 : test_loss 6.145284484863281\n",
      "saved model\n",
      "Epoch 6 : train_loss 6.126471245659722\n",
      "Epoch 6 : test_loss 6.041126037597656\n",
      "saved model\n",
      "Epoch 7 : train_loss 6.0501984320746525\n",
      "Epoch 7 : test_loss 5.990611267089844\n",
      "saved model\n",
      "Epoch 8 : train_loss 5.996305460611979\n",
      "Epoch 8 : test_loss 5.951221862792969\n",
      "saved model\n",
      "Epoch 9 : train_loss 5.967364759657118\n",
      "Epoch 9 : test_loss 5.92538916015625\n",
      "saved model\n",
      "Epoch 10 : train_loss 5.936485039605035\n",
      "Epoch 10 : test_loss 5.887101135253906\n",
      "saved model\n",
      "Epoch 11 : train_loss 5.919546034071181\n",
      "Epoch 11 : test_loss 5.860193176269531\n",
      "saved model\n",
      "Epoch 12 : train_loss 5.899316528320313\n",
      "Epoch 12 : test_loss 5.854329833984375\n",
      "saved model\n",
      "Epoch 13 : train_loss 5.887372395833333\n",
      "Epoch 13 : test_loss 5.836351989746094\n",
      "saved model\n",
      "Epoch 14 : train_loss 5.873372504340278\n",
      "Epoch 14 : test_loss 5.810130310058594\n",
      "Epoch 15 : train_loss 5.855400675455729\n",
      "Epoch 15 : test_loss 5.8120730590820315\n",
      "saved model\n",
      "Epoch 16 : train_loss 5.831895521375868\n",
      "Epoch 16 : test_loss 5.7653720092773435\n",
      "saved model\n",
      "Epoch 17 : train_loss 5.808888319227431\n",
      "Epoch 17 : test_loss 5.759743041992188\n",
      "saved model\n",
      "Epoch 18 : train_loss 5.7832211507161455\n",
      "Epoch 18 : test_loss 5.73024951171875\n",
      "saved model\n",
      "Epoch 19 : train_loss 5.759069742838542\n",
      "Epoch 19 : test_loss 5.699439697265625\n",
      "saved model\n",
      "Epoch 20 : train_loss 5.717593221028646\n",
      "Epoch 20 : test_loss 5.6682392578125\n",
      "saved model\n",
      "Epoch 21 : train_loss 5.693960272894965\n",
      "Epoch 21 : test_loss 5.6356640625\n",
      "saved model\n",
      "Epoch 22 : train_loss 5.658587415907118\n",
      "Epoch 22 : test_loss 5.608702331542969\n",
      "saved model\n",
      "Epoch 23 : train_loss 5.620909559461806\n",
      "Epoch 23 : test_loss 5.555483764648438\n",
      "saved model\n",
      "Epoch 24 : train_loss 5.5888427734375\n",
      "Epoch 24 : test_loss 5.528430541992187\n",
      "saved model\n",
      "Epoch 25 : train_loss 5.553322998046875\n",
      "Epoch 25 : test_loss 5.493742919921875\n",
      "saved model\n",
      "Epoch 26 : train_loss 5.521428276909722\n",
      "Epoch 26 : test_loss 5.469413818359375\n",
      "saved model\n",
      "Epoch 27 : train_loss 5.482338636610243\n",
      "Epoch 27 : test_loss 5.429801147460937\n",
      "saved model\n",
      "Epoch 28 : train_loss 5.450626925998264\n",
      "Epoch 28 : test_loss 5.409153442382813\n",
      "saved model\n",
      "Epoch 29 : train_loss 5.419139553493924\n",
      "Epoch 29 : test_loss 5.35615380859375\n",
      "saved model\n",
      "Epoch 30 : train_loss 5.38836168077257\n",
      "Epoch 30 : test_loss 5.339345581054688\n",
      "saved model\n",
      "Epoch 31 : train_loss 5.360910101996528\n",
      "Epoch 31 : test_loss 5.326525451660157\n",
      "saved model\n",
      "Epoch 32 : train_loss 5.3343365342881945\n",
      "Epoch 32 : test_loss 5.281244506835938\n",
      "saved model\n",
      "Epoch 33 : train_loss 5.305917249891493\n",
      "Epoch 33 : test_loss 5.241522705078125\n",
      "saved model\n",
      "Epoch 34 : train_loss 5.274644829644097\n",
      "Epoch 34 : test_loss 5.234727111816406\n",
      "saved model\n",
      "Epoch 35 : train_loss 5.248678344726563\n",
      "Epoch 35 : test_loss 5.202351379394531\n",
      "saved model\n",
      "Epoch 36 : train_loss 5.222060397677952\n",
      "Epoch 36 : test_loss 5.170845520019531\n",
      "saved model\n",
      "Epoch 37 : train_loss 5.201408298068577\n",
      "Epoch 37 : test_loss 5.163538818359375\n",
      "saved model\n",
      "Epoch 38 : train_loss 5.178807169596354\n",
      "Epoch 38 : test_loss 5.127799743652344\n",
      "saved model\n",
      "Epoch 39 : train_loss 5.1491304660373265\n",
      "Epoch 39 : test_loss 5.0957511596679685\n",
      "saved model\n",
      "Epoch 40 : train_loss 5.129150105794271\n",
      "Epoch 40 : test_loss 5.087947143554688\n",
      "saved model\n",
      "Epoch 41 : train_loss 5.102618855794271\n",
      "Epoch 41 : test_loss 5.054193969726563\n",
      "saved model\n",
      "Epoch 42 : train_loss 5.086833062065972\n",
      "Epoch 42 : test_loss 5.028689575195313\n",
      "saved model\n",
      "Epoch 43 : train_loss 5.0568512641059025\n",
      "Epoch 43 : test_loss 5.022043334960937\n",
      "saved model\n",
      "Epoch 44 : train_loss 5.042074340820313\n",
      "Epoch 44 : test_loss 4.975584533691406\n",
      "saved model\n",
      "Epoch 45 : train_loss 5.020599324544271\n",
      "Epoch 45 : test_loss 4.97422314453125\n",
      "saved model\n",
      "Epoch 46 : train_loss 4.999731580946181\n",
      "Epoch 46 : test_loss 4.934752502441406\n",
      "saved model\n",
      "Epoch 47 : train_loss 4.978871595594618\n",
      "Epoch 47 : test_loss 4.920059265136719\n",
      "saved model\n",
      "Epoch 48 : train_loss 4.954181423611111\n",
      "Epoch 48 : test_loss 4.910774780273438\n",
      "saved model\n",
      "Epoch 49 : train_loss 4.938585856119792\n",
      "Epoch 49 : test_loss 4.892331420898437\n",
      "saved model\n",
      "Epoch 50 : train_loss 4.914813598632812\n",
      "Epoch 50 : test_loss 4.852761352539063\n",
      "Epoch 51 : train_loss 4.897272935655382\n",
      "Epoch 51 : test_loss 4.8564755249023435\n",
      "saved model\n",
      "Epoch 52 : train_loss 4.872833414713542\n",
      "Epoch 52 : test_loss 4.817047485351562\n",
      "saved model\n",
      "Epoch 53 : train_loss 4.854080268012153\n",
      "Epoch 53 : test_loss 4.792959808349609\n",
      "Epoch 54 : train_loss 4.83756305609809\n",
      "Epoch 54 : test_loss 4.804141357421875\n",
      "saved model\n",
      "Epoch 55 : train_loss 4.81877057562934\n",
      "Epoch 55 : test_loss 4.760329315185547\n",
      "saved model\n",
      "Epoch 56 : train_loss 4.797111206054687\n",
      "Epoch 56 : test_loss 4.7265146484375\n",
      "saved model\n",
      "Epoch 57 : train_loss 4.776022827148437\n",
      "Epoch 57 : test_loss 4.713508911132813\n",
      "saved model\n",
      "Epoch 58 : train_loss 4.7560013970269095\n",
      "Epoch 58 : test_loss 4.677101501464843\n",
      "Epoch 59 : train_loss 4.733017903645833\n",
      "Epoch 59 : test_loss 4.685810729980469\n",
      "saved model\n",
      "Epoch 60 : train_loss 4.712786159939236\n",
      "Epoch 60 : test_loss 4.660309692382812\n",
      "saved model\n",
      "Epoch 61 : train_loss 4.683804063585069\n",
      "Epoch 61 : test_loss 4.631375579833985\n",
      "saved model\n",
      "Epoch 62 : train_loss 4.670014458550347\n",
      "Epoch 62 : test_loss 4.6241739501953125\n",
      "saved model\n",
      "Epoch 63 : train_loss 4.652727376302083\n",
      "Epoch 63 : test_loss 4.6054969482421875\n",
      "saved model\n",
      "Epoch 64 : train_loss 4.629263088650173\n",
      "Epoch 64 : test_loss 4.59204931640625\n",
      "saved model\n",
      "Epoch 65 : train_loss 4.6129542507595485\n",
      "Epoch 65 : test_loss 4.56302474975586\n",
      "saved model\n",
      "Epoch 66 : train_loss 4.5939432508680556\n",
      "Epoch 66 : test_loss 4.537766906738281\n",
      "saved model\n",
      "Epoch 67 : train_loss 4.571530571831597\n",
      "Epoch 67 : test_loss 4.531258972167969\n",
      "saved model\n",
      "Epoch 68 : train_loss 4.554575520833334\n",
      "Epoch 68 : test_loss 4.487057434082031\n",
      "Epoch 69 : train_loss 4.531983452690972\n",
      "Epoch 69 : test_loss 4.490962768554687\n",
      "saved model\n",
      "Epoch 70 : train_loss 4.511565443250868\n",
      "Epoch 70 : test_loss 4.466605590820312\n",
      "saved model\n",
      "Epoch 71 : train_loss 4.489916558159722\n",
      "Epoch 71 : test_loss 4.4521441040039065\n",
      "saved model\n",
      "Epoch 72 : train_loss 4.4748792453342014\n",
      "Epoch 72 : test_loss 4.438979736328125\n",
      "saved model\n",
      "Epoch 73 : train_loss 4.4528086615668405\n",
      "Epoch 73 : test_loss 4.3835947265625\n",
      "Epoch 74 : train_loss 4.439323391384549\n",
      "Epoch 74 : test_loss 4.386327911376953\n",
      "saved model\n",
      "Epoch 75 : train_loss 4.4079137505425345\n",
      "Epoch 75 : test_loss 4.372431701660156\n",
      "saved model\n",
      "Epoch 76 : train_loss 4.394118082682292\n",
      "Epoch 76 : test_loss 4.343607543945312\n",
      "saved model\n",
      "Epoch 77 : train_loss 4.375283203125\n",
      "Epoch 77 : test_loss 4.331488464355469\n",
      "saved model\n",
      "Epoch 78 : train_loss 4.358471530490451\n",
      "Epoch 78 : test_loss 4.31808837890625\n",
      "saved model\n",
      "Epoch 79 : train_loss 4.338604248046875\n",
      "Epoch 79 : test_loss 4.293860778808594\n",
      "saved model\n",
      "Epoch 80 : train_loss 4.319032335069444\n",
      "Epoch 80 : test_loss 4.271803314208984\n",
      "saved model\n",
      "Epoch 81 : train_loss 4.299650838216146\n",
      "Epoch 81 : test_loss 4.263816284179687\n",
      "saved model\n",
      "Epoch 82 : train_loss 4.280524820963541\n",
      "Epoch 82 : test_loss 4.222108337402344\n",
      "Epoch 83 : train_loss 4.257025227864584\n",
      "Epoch 83 : test_loss 4.222325439453125\n",
      "saved model\n",
      "Epoch 84 : train_loss 4.229832817925347\n",
      "Epoch 84 : test_loss 4.200834411621094\n",
      "saved model\n",
      "Epoch 85 : train_loss 4.216466430664062\n",
      "Epoch 85 : test_loss 4.171488342285156\n",
      "saved model\n",
      "Epoch 86 : train_loss 4.19564885796441\n",
      "Epoch 86 : test_loss 4.14827978515625\n",
      "saved model\n",
      "Epoch 87 : train_loss 4.178304050021701\n",
      "Epoch 87 : test_loss 4.142461975097656\n",
      "saved model\n",
      "Epoch 88 : train_loss 4.156284749348958\n",
      "Epoch 88 : test_loss 4.1319242858886716\n",
      "saved model\n",
      "Epoch 89 : train_loss 4.134564860026042\n",
      "Epoch 89 : test_loss 4.089372253417968\n",
      "saved model\n",
      "Epoch 90 : train_loss 4.113471801757813\n",
      "Epoch 90 : test_loss 4.081752655029297\n",
      "saved model\n",
      "Epoch 91 : train_loss 4.09180419921875\n",
      "Epoch 91 : test_loss 4.052496551513672\n",
      "saved model\n",
      "Epoch 92 : train_loss 4.075579535590278\n",
      "Epoch 92 : test_loss 4.039272521972657\n",
      "saved model\n",
      "Epoch 93 : train_loss 4.051066297743056\n",
      "Epoch 93 : test_loss 3.995702392578125\n",
      "Epoch 94 : train_loss 4.0367131618923615\n",
      "Epoch 94 : test_loss 3.99660302734375\n",
      "saved model\n",
      "Epoch 95 : train_loss 4.015146172417535\n",
      "Epoch 95 : test_loss 3.9740339050292968\n",
      "saved model\n",
      "Epoch 96 : train_loss 3.991103108723958\n",
      "Epoch 96 : test_loss 3.9491572265625\n",
      "saved model\n",
      "Epoch 97 : train_loss 3.9722901882595485\n",
      "Epoch 97 : test_loss 3.9424554138183594\n",
      "saved model\n",
      "Epoch 98 : train_loss 3.9539974229600694\n",
      "Epoch 98 : test_loss 3.9163240966796873\n",
      "saved model\n",
      "Epoch 99 : train_loss 3.9346048719618056\n",
      "Epoch 99 : test_loss 3.899346984863281\n",
      "saved model\n",
      "Epoch 100 : train_loss 3.9169605848524305\n",
      "Epoch 100 : test_loss 3.862615234375\n",
      "saved model\n",
      "Epoch 101 : train_loss 3.888676445855035\n",
      "Epoch 101 : test_loss 3.845377380371094\n",
      "saved model\n",
      "Epoch 102 : train_loss 3.8771333279079863\n",
      "Epoch 102 : test_loss 3.8326835327148436\n",
      "saved model\n",
      "Epoch 103 : train_loss 3.856186726888021\n",
      "Epoch 103 : test_loss 3.820976501464844\n",
      "saved model\n",
      "Epoch 104 : train_loss 3.8365052625868055\n",
      "Epoch 104 : test_loss 3.790930450439453\n",
      "saved model\n",
      "Epoch 105 : train_loss 3.816613972981771\n",
      "Epoch 105 : test_loss 3.7896184997558593\n",
      "saved model\n",
      "Epoch 106 : train_loss 3.798656032986111\n",
      "Epoch 106 : test_loss 3.7593074340820314\n",
      "saved model\n",
      "Epoch 107 : train_loss 3.7824576009114583\n",
      "Epoch 107 : test_loss 3.7482584228515625\n",
      "saved model\n",
      "Epoch 108 : train_loss 3.7608076985677084\n",
      "Epoch 108 : test_loss 3.721783630371094\n",
      "saved model\n",
      "Epoch 109 : train_loss 3.7380924207899304\n",
      "Epoch 109 : test_loss 3.7128181762695314\n",
      "saved model\n",
      "Epoch 110 : train_loss 3.7260272759331596\n",
      "Epoch 110 : test_loss 3.6778641357421873\n",
      "saved model\n",
      "Epoch 111 : train_loss 3.7006317409939236\n",
      "Epoch 111 : test_loss 3.673847961425781\n",
      "saved model\n",
      "Epoch 112 : train_loss 3.685844482421875\n",
      "Epoch 112 : test_loss 3.632650726318359\n",
      "saved model\n",
      "Epoch 113 : train_loss 3.6704947645399306\n",
      "Epoch 113 : test_loss 3.623637939453125\n",
      "saved model\n",
      "Epoch 114 : train_loss 3.6472408040364583\n",
      "Epoch 114 : test_loss 3.5991749572753906\n",
      "saved model\n",
      "Epoch 115 : train_loss 3.631375257703993\n",
      "Epoch 115 : test_loss 3.575924987792969\n",
      "saved model\n",
      "Epoch 116 : train_loss 3.60780224609375\n",
      "Epoch 116 : test_loss 3.5682894287109375\n",
      "saved model\n",
      "Epoch 117 : train_loss 3.5916828884548613\n",
      "Epoch 117 : test_loss 3.538868347167969\n",
      "Epoch 118 : train_loss 3.565729200575087\n",
      "Epoch 118 : test_loss 3.544878723144531\n",
      "saved model\n",
      "Epoch 119 : train_loss 3.550269015842014\n",
      "Epoch 119 : test_loss 3.507219085693359\n",
      "saved model\n",
      "Epoch 120 : train_loss 3.5331947292751735\n",
      "Epoch 120 : test_loss 3.497253723144531\n",
      "saved model\n",
      "Epoch 121 : train_loss 3.514469448513455\n",
      "Epoch 121 : test_loss 3.467075927734375\n",
      "saved model\n",
      "Epoch 122 : train_loss 3.493408901638455\n",
      "Epoch 122 : test_loss 3.444172119140625\n",
      "saved model\n",
      "Epoch 123 : train_loss 3.468913784450955\n",
      "Epoch 123 : test_loss 3.443338653564453\n",
      "saved model\n",
      "Epoch 124 : train_loss 3.4504013671875\n",
      "Epoch 124 : test_loss 3.4073744812011717\n",
      "saved model\n",
      "Epoch 125 : train_loss 3.4315854424370658\n",
      "Epoch 125 : test_loss 3.3804837646484374\n",
      "saved model\n",
      "Epoch 126 : train_loss 3.4158455878363716\n",
      "Epoch 126 : test_loss 3.376550354003906\n",
      "saved model\n",
      "Epoch 127 : train_loss 3.391842753092448\n",
      "Epoch 127 : test_loss 3.3623936767578124\n",
      "saved model\n",
      "Epoch 128 : train_loss 3.3715743882921005\n",
      "Epoch 128 : test_loss 3.34539404296875\n",
      "saved model\n",
      "Epoch 129 : train_loss 3.3537286716037324\n",
      "Epoch 129 : test_loss 3.3137600402832033\n",
      "saved model\n",
      "Epoch 130 : train_loss 3.3364537692599825\n",
      "Epoch 130 : test_loss 3.3009757690429686\n",
      "saved model\n",
      "Epoch 131 : train_loss 3.3154047241210938\n",
      "Epoch 131 : test_loss 3.2825552368164064\n",
      "saved model\n",
      "Epoch 132 : train_loss 3.3011602851019965\n",
      "Epoch 132 : test_loss 3.243332489013672\n",
      "saved model\n",
      "Epoch 133 : train_loss 3.281098158094618\n",
      "Epoch 133 : test_loss 3.239656982421875\n",
      "saved model\n",
      "Epoch 134 : train_loss 3.256467732747396\n",
      "Epoch 134 : test_loss 3.211735290527344\n",
      "Epoch 135 : train_loss 3.235654439290365\n",
      "Epoch 135 : test_loss 3.2140662536621094\n",
      "saved model\n",
      "Epoch 136 : train_loss 3.220650187174479\n",
      "Epoch 136 : test_loss 3.1806593322753907\n",
      "saved model\n",
      "Epoch 137 : train_loss 3.193294250488281\n",
      "Epoch 137 : test_loss 3.164268768310547\n",
      "saved model\n",
      "Epoch 138 : train_loss 3.171140448676215\n",
      "Epoch 138 : test_loss 3.1551043395996095\n",
      "saved model\n",
      "Epoch 139 : train_loss 3.158549309624566\n",
      "Epoch 139 : test_loss 3.1233713989257814\n",
      "saved model\n",
      "Epoch 140 : train_loss 3.1418558281792537\n",
      "Epoch 140 : test_loss 3.091549133300781\n",
      "saved model\n",
      "Epoch 141 : train_loss 3.118827412923177\n",
      "Epoch 141 : test_loss 3.077988098144531\n",
      "saved model\n",
      "Epoch 142 : train_loss 3.1009768880208335\n",
      "Epoch 142 : test_loss 3.0581160278320314\n",
      "saved model\n",
      "Epoch 143 : train_loss 3.078728515625\n",
      "Epoch 143 : test_loss 3.0468960876464846\n",
      "saved model\n",
      "Epoch 144 : train_loss 3.0613256022135418\n",
      "Epoch 144 : test_loss 3.0134664306640624\n",
      "saved model\n",
      "Epoch 145 : train_loss 3.0446588270399304\n",
      "Epoch 145 : test_loss 3.0051109008789063\n",
      "saved model\n",
      "Epoch 146 : train_loss 3.0207313435872396\n",
      "Epoch 146 : test_loss 2.965625244140625\n",
      "Epoch 147 : train_loss 2.9970714518229165\n",
      "Epoch 147 : test_loss 2.9728748779296876\n",
      "saved model\n",
      "Epoch 148 : train_loss 2.98514446343316\n",
      "Epoch 148 : test_loss 2.94622509765625\n",
      "saved model\n",
      "Epoch 149 : train_loss 2.9669015502929685\n",
      "Epoch 149 : test_loss 2.9317807006835936\n",
      "saved model\n",
      "Epoch 150 : train_loss 2.94781103515625\n",
      "Epoch 150 : test_loss 2.9014820251464846\n",
      "saved model\n",
      "Epoch 151 : train_loss 2.9310414021809894\n",
      "Epoch 151 : test_loss 2.896230895996094\n",
      "saved model\n",
      "Epoch 152 : train_loss 2.9108822631835936\n",
      "Epoch 152 : test_loss 2.8622118225097655\n",
      "Epoch 153 : train_loss 2.897292032877604\n",
      "Epoch 153 : test_loss 2.8689979248046873\n",
      "saved model\n",
      "Epoch 154 : train_loss 2.870765672471788\n",
      "Epoch 154 : test_loss 2.839062713623047\n",
      "saved model\n",
      "Epoch 155 : train_loss 2.8539972330729166\n",
      "Epoch 155 : test_loss 2.8287463073730468\n",
      "saved model\n",
      "Epoch 156 : train_loss 2.83060055202908\n",
      "Epoch 156 : test_loss 2.8044584655761717\n",
      "saved model\n",
      "Epoch 157 : train_loss 2.818708970811632\n",
      "Epoch 157 : test_loss 2.774691619873047\n",
      "saved model\n",
      "Epoch 158 : train_loss 2.795166042751736\n",
      "Epoch 158 : test_loss 2.763650604248047\n",
      "saved model\n",
      "Epoch 159 : train_loss 2.7766874457465276\n",
      "Epoch 159 : test_loss 2.742547332763672\n",
      "Epoch 160 : train_loss 2.763042914496528\n",
      "Epoch 160 : test_loss 2.7437640380859376\n",
      "saved model\n",
      "Epoch 161 : train_loss 2.7418529120551214\n",
      "Epoch 161 : test_loss 2.7012645568847655\n",
      "saved model\n",
      "Epoch 162 : train_loss 2.7215497029622395\n",
      "Epoch 162 : test_loss 2.681814208984375\n",
      "saved model\n",
      "Epoch 163 : train_loss 2.7104870334201387\n",
      "Epoch 163 : test_loss 2.6743424072265625\n",
      "saved model\n",
      "Epoch 164 : train_loss 2.6909060736762154\n",
      "Epoch 164 : test_loss 2.638776580810547\n",
      "saved model\n",
      "Epoch 165 : train_loss 2.672372829861111\n",
      "Epoch 165 : test_loss 2.636204406738281\n",
      "saved model\n",
      "Epoch 166 : train_loss 2.6475499538845484\n",
      "Epoch 166 : test_loss 2.6192796020507814\n",
      "saved model\n",
      "Epoch 167 : train_loss 2.6339014417860245\n",
      "Epoch 167 : test_loss 2.597879913330078\n",
      "saved model\n",
      "Epoch 168 : train_loss 2.6120607435438368\n",
      "Epoch 168 : test_loss 2.5655216064453126\n",
      "saved model\n",
      "Epoch 169 : train_loss 2.601729234483507\n",
      "Epoch 169 : test_loss 2.5540758056640627\n",
      "saved model\n",
      "Epoch 170 : train_loss 2.5767474500868057\n",
      "Epoch 170 : test_loss 2.542305755615234\n",
      "saved model\n",
      "Epoch 171 : train_loss 2.5568943956163195\n",
      "Epoch 171 : test_loss 2.5135252380371096\n",
      "saved model\n",
      "Epoch 172 : train_loss 2.535461113823785\n",
      "Epoch 172 : test_loss 2.500561065673828\n",
      "saved model\n",
      "Epoch 173 : train_loss 2.5205767822265623\n",
      "Epoch 173 : test_loss 2.4824505615234376\n",
      "saved model\n",
      "Epoch 174 : train_loss 2.5011297946506077\n",
      "Epoch 174 : test_loss 2.476700897216797\n",
      "saved model\n",
      "Epoch 175 : train_loss 2.483711188422309\n",
      "Epoch 175 : test_loss 2.455886169433594\n",
      "saved model\n",
      "Epoch 176 : train_loss 2.4781646525065106\n",
      "Epoch 176 : test_loss 2.4387515563964843\n",
      "saved model\n",
      "Epoch 177 : train_loss 2.4544959513346356\n",
      "Epoch 177 : test_loss 2.4189451293945314\n",
      "saved model\n",
      "Epoch 178 : train_loss 2.4364423014322916\n",
      "Epoch 178 : test_loss 2.3982807006835936\n",
      "Epoch 179 : train_loss 2.4228131578233505\n",
      "Epoch 179 : test_loss 2.4066910400390626\n",
      "saved model\n",
      "Epoch 180 : train_loss 2.4006764594184027\n",
      "Epoch 180 : test_loss 2.357205047607422\n",
      "saved model\n",
      "Epoch 181 : train_loss 2.3790251057942706\n",
      "Epoch 181 : test_loss 2.3384225463867185\n",
      "Epoch 182 : train_loss 2.3580319010416666\n",
      "Epoch 182 : test_loss 2.3417115783691407\n",
      "saved model\n",
      "Epoch 183 : train_loss 2.353094190809462\n",
      "Epoch 183 : test_loss 2.328990997314453\n",
      "saved model\n",
      "Epoch 184 : train_loss 2.334828877766927\n",
      "Epoch 184 : test_loss 2.3205633850097658\n",
      "saved model\n",
      "Epoch 185 : train_loss 2.3266185370551216\n",
      "Epoch 185 : test_loss 2.2712805633544924\n",
      "saved model\n",
      "Epoch 186 : train_loss 2.2951060791015623\n",
      "Epoch 186 : test_loss 2.270514862060547\n",
      "saved model\n",
      "Epoch 187 : train_loss 2.2823744710286458\n",
      "Epoch 187 : test_loss 2.2409954833984376\n",
      "Epoch 188 : train_loss 2.2791655815972223\n",
      "Epoch 188 : test_loss 2.253630844116211\n",
      "saved model\n",
      "Epoch 189 : train_loss 2.2577393391927085\n",
      "Epoch 189 : test_loss 2.215369354248047\n",
      "saved model\n",
      "Epoch 190 : train_loss 2.2290134616427952\n",
      "Epoch 190 : test_loss 2.202806121826172\n",
      "saved model\n",
      "Epoch 191 : train_loss 2.214248514811198\n",
      "Epoch 191 : test_loss 2.194218566894531\n",
      "saved model\n",
      "Epoch 192 : train_loss 2.208559875488281\n",
      "Epoch 192 : test_loss 2.1678665466308593\n",
      "saved model\n",
      "Epoch 193 : train_loss 2.1948822564019097\n",
      "Epoch 193 : test_loss 2.1631932067871094\n",
      "saved model\n",
      "Epoch 194 : train_loss 2.1727960205078123\n",
      "Epoch 194 : test_loss 2.1453809204101564\n",
      "saved model\n",
      "Epoch 195 : train_loss 2.1405906168619793\n",
      "Epoch 195 : test_loss 2.112384490966797\n",
      "Epoch 196 : train_loss 2.140663330078125\n",
      "Epoch 196 : test_loss 2.1210294342041016\n",
      "Epoch 197 : train_loss 2.1455716417100694\n",
      "Epoch 197 : test_loss 2.1337486572265627\n",
      "saved model\n",
      "Epoch 198 : train_loss 2.1319680921766495\n",
      "Epoch 198 : test_loss 2.0837838592529296\n",
      "saved model\n",
      "Epoch 199 : train_loss 2.107608201768663\n",
      "Epoch 199 : test_loss 2.065306182861328\n",
      "saved model\n",
      "Epoch 200 : train_loss 2.0714534776475695\n",
      "Epoch 200 : test_loss 2.059202896118164\n",
      "saved model\n",
      "Epoch 201 : train_loss 2.074162116156684\n",
      "Epoch 201 : test_loss 2.0224615478515626\n",
      "Epoch 202 : train_loss 2.0885601806640626\n",
      "Epoch 202 : test_loss 2.055315673828125\n",
      "saved model\n",
      "Epoch 203 : train_loss 2.048717861599392\n",
      "Epoch 203 : test_loss 2.0135741577148436\n",
      "saved model\n",
      "Epoch 204 : train_loss 2.0150025634765627\n",
      "Epoch 204 : test_loss 1.9602001190185547\n",
      "Epoch 205 : train_loss 2.010727735731337\n",
      "Epoch 205 : test_loss 1.9931161499023438\n",
      "Epoch 206 : train_loss 2.028687255859375\n",
      "Epoch 206 : test_loss 1.9984998474121094\n",
      "Epoch 207 : train_loss 2.0346331854926216\n",
      "Epoch 207 : test_loss 1.9912431335449219\n",
      "Epoch 208 : train_loss 2.0166028442382813\n",
      "Epoch 208 : test_loss 1.9737102203369141\n",
      "saved model\n",
      "Epoch 209 : train_loss 1.9872753974066841\n",
      "Epoch 209 : test_loss 1.9535902862548828\n",
      "saved model\n",
      "Epoch 210 : train_loss 1.9539431627061632\n",
      "Epoch 210 : test_loss 1.886985107421875\n",
      "Epoch 211 : train_loss 1.9346436292860243\n",
      "Epoch 211 : test_loss 1.9370212707519532\n",
      "Epoch 212 : train_loss 1.9413213568793404\n",
      "Epoch 212 : test_loss 1.9202866973876953\n",
      "Epoch 213 : train_loss 1.967573221842448\n",
      "Epoch 213 : test_loss 1.9522445068359375\n",
      "Epoch 214 : train_loss 1.9454024590386285\n",
      "Epoch 214 : test_loss 1.8976558227539062\n",
      "saved model\n",
      "Epoch 215 : train_loss 1.8941517401801216\n",
      "Epoch 215 : test_loss 1.8616910552978516\n",
      "saved model\n",
      "Epoch 216 : train_loss 1.839049058702257\n",
      "Epoch 216 : test_loss 1.8088938903808593\n",
      "Epoch 217 : train_loss 1.8327859836154514\n",
      "Epoch 217 : test_loss 1.8307400665283202\n",
      "Epoch 218 : train_loss 1.8494613579644097\n",
      "Epoch 218 : test_loss 1.8389190521240235\n",
      "Epoch 219 : train_loss 1.8835728420681424\n",
      "Epoch 219 : test_loss 1.8579474487304688\n",
      "Epoch 220 : train_loss 1.8657754991319444\n",
      "Epoch 220 : test_loss 1.8432875061035157\n",
      "saved model\n",
      "Epoch 221 : train_loss 1.8305347561306424\n",
      "Epoch 221 : test_loss 1.7943992919921874\n",
      "Epoch 222 : train_loss 1.8365752393934462\n",
      "Epoch 222 : test_loss 1.810861343383789\n",
      "Epoch 223 : train_loss 1.861498514811198\n",
      "Epoch 223 : test_loss 1.819461151123047\n",
      "saved model\n",
      "Epoch 224 : train_loss 1.849966288248698\n",
      "Epoch 224 : test_loss 1.7755377502441407\n",
      "saved model\n",
      "Epoch 225 : train_loss 1.7788152838812934\n",
      "Epoch 225 : test_loss 1.7124056854248046\n",
      "saved model\n",
      "Epoch 226 : train_loss 1.7229340074327257\n",
      "Epoch 226 : test_loss 1.6765300903320313\n",
      "Epoch 227 : train_loss 1.7066937696668836\n",
      "Epoch 227 : test_loss 1.7268336181640624\n",
      "Epoch 228 : train_loss 1.7225606045193143\n",
      "Epoch 228 : test_loss 1.7334063720703126\n",
      "Epoch 229 : train_loss 1.745870361328125\n",
      "Epoch 229 : test_loss 1.7459917907714844\n",
      "Epoch 230 : train_loss 1.7486871236165364\n",
      "Epoch 230 : test_loss 1.698583770751953\n",
      "Epoch 231 : train_loss 1.720929243299696\n",
      "Epoch 231 : test_loss 1.7144212799072265\n",
      "Epoch 232 : train_loss 1.698777570936415\n",
      "Epoch 232 : test_loss 1.71317529296875\n",
      "Epoch 233 : train_loss 1.7191531711154513\n",
      "Epoch 233 : test_loss 1.6998035888671874\n",
      "Epoch 234 : train_loss 1.7430398796929254\n",
      "Epoch 234 : test_loss 1.7146191253662109\n",
      "Epoch 235 : train_loss 1.7501380004882812\n",
      "Epoch 235 : test_loss 1.681622039794922\n",
      "Epoch 236 : train_loss 1.7132865905761718\n",
      "Epoch 236 : test_loss 1.6962299041748048\n",
      "saved model\n",
      "Epoch 237 : train_loss 1.6731929490831163\n",
      "Epoch 237 : test_loss 1.5899175720214844\n",
      "Epoch 238 : train_loss 1.656246348063151\n",
      "Epoch 238 : test_loss 1.6012441864013671\n",
      "Epoch 239 : train_loss 1.6737737087673612\n",
      "Epoch 239 : test_loss 1.6723012390136718\n",
      "Epoch 240 : train_loss 1.683462880452474\n",
      "Epoch 240 : test_loss 1.6398321838378906\n",
      "Epoch 241 : train_loss 1.6668861253526475\n",
      "Epoch 241 : test_loss 1.6299119415283203\n",
      "saved model\n",
      "Epoch 242 : train_loss 1.6324754503038195\n",
      "Epoch 242 : test_loss 1.5863521728515626\n",
      "saved model\n",
      "Epoch 243 : train_loss 1.5867535841200087\n",
      "Epoch 243 : test_loss 1.5659779052734375\n",
      "Epoch 244 : train_loss 1.5841317240397135\n",
      "Epoch 244 : test_loss 1.6005994873046876\n",
      "Epoch 245 : train_loss 1.6261895989312065\n",
      "Epoch 245 : test_loss 1.650930374145508\n",
      "Epoch 246 : train_loss 1.6887749532063803\n",
      "Epoch 246 : test_loss 1.6872313537597656\n",
      "Epoch 247 : train_loss 1.721828121609158\n",
      "Epoch 247 : test_loss 1.6864940490722655\n",
      "Epoch 248 : train_loss 1.6807829996744792\n",
      "Epoch 248 : test_loss 1.5943843383789063\n",
      "saved model\n",
      "Epoch 249 : train_loss 1.5929472283257378\n",
      "Epoch 249 : test_loss 1.547382568359375\n",
      "saved model\n",
      "Epoch 250 : train_loss 1.4978960537380643\n",
      "Epoch 250 : test_loss 1.4519043884277343\n",
      "Epoch 251 : train_loss 1.4775016513400607\n",
      "Epoch 251 : test_loss 1.4831827545166016\n",
      "Epoch 252 : train_loss 1.5272279527452257\n",
      "Epoch 252 : test_loss 1.5140535888671875\n",
      "Epoch 253 : train_loss 1.60914796617296\n",
      "Epoch 253 : test_loss 1.6143705749511719\n",
      "Epoch 254 : train_loss 1.6449155748155382\n",
      "Epoch 254 : test_loss 1.5916046752929687\n",
      "Epoch 255 : train_loss 1.624072041829427\n",
      "Epoch 255 : test_loss 1.5315573272705079\n",
      "Epoch 256 : train_loss 1.5438450113932292\n",
      "Epoch 256 : test_loss 1.4952475128173828\n",
      "Epoch 257 : train_loss 1.4963833584255641\n",
      "Epoch 257 : test_loss 1.4644485473632813\n",
      "Epoch 258 : train_loss 1.4906326836480035\n",
      "Epoch 258 : test_loss 1.5022691192626954\n",
      "Epoch 259 : train_loss 1.5461618414984808\n",
      "Epoch 259 : test_loss 1.5014277191162109\n",
      "Epoch 260 : train_loss 1.5887389763726127\n",
      "Epoch 260 : test_loss 1.6098941040039063\n",
      "Epoch 261 : train_loss 1.6089966871473524\n",
      "Epoch 261 : test_loss 1.6162607879638673\n",
      "Epoch 262 : train_loss 1.6170309143066406\n",
      "Epoch 262 : test_loss 1.586380615234375\n",
      "Epoch 263 : train_loss 1.5981447414822048\n",
      "Epoch 263 : test_loss 1.5822247924804687\n",
      "Epoch 264 : train_loss 1.5915021124945747\n",
      "Epoch 264 : test_loss 1.6116385650634766\n",
      "Epoch 265 : train_loss 1.5600358242458767\n",
      "Epoch 265 : test_loss 1.5292598724365234\n",
      "Epoch 266 : train_loss 1.539149434407552\n",
      "Epoch 266 : test_loss 1.526989501953125\n",
      "Epoch 267 : train_loss 1.5242362060546875\n",
      "Epoch 267 : test_loss 1.4714520263671875\n",
      "Epoch 268 : train_loss 1.5125208129882812\n",
      "Epoch 268 : test_loss 1.536236831665039\n",
      "Epoch 269 : train_loss 1.46528269788954\n",
      "Epoch 269 : test_loss 1.461159912109375\n",
      "saved model\n",
      "Epoch 270 : train_loss 1.4555278116861978\n",
      "Epoch 270 : test_loss 1.3954020233154296\n",
      "Epoch 271 : train_loss 1.445709242078993\n",
      "Epoch 271 : test_loss 1.433522705078125\n",
      "Epoch 272 : train_loss 1.4211112196180555\n",
      "Epoch 272 : test_loss 1.4582245178222657\n",
      "saved model\n",
      "Epoch 273 : train_loss 1.3925286119249132\n",
      "Epoch 273 : test_loss 1.3885073852539063\n",
      "saved model\n",
      "Epoch 274 : train_loss 1.3946321377224393\n",
      "Epoch 274 : test_loss 1.3882647705078126\n",
      "Epoch 275 : train_loss 1.3830620693630642\n",
      "Epoch 275 : test_loss 1.408580078125\n",
      "Epoch 276 : train_loss 1.3826041191948786\n",
      "Epoch 276 : test_loss 1.390413101196289\n",
      "saved model\n",
      "Epoch 277 : train_loss 1.37488720703125\n",
      "Epoch 277 : test_loss 1.329218292236328\n",
      "Epoch 278 : train_loss 1.3688909233940971\n",
      "Epoch 278 : test_loss 1.3550237426757812\n",
      "saved model\n",
      "Epoch 279 : train_loss 1.3608651360405817\n",
      "Epoch 279 : test_loss 1.30859765625\n",
      "Epoch 280 : train_loss 1.343764692518446\n",
      "Epoch 280 : test_loss 1.3473004150390624\n",
      "Epoch 281 : train_loss 1.347757347954644\n",
      "Epoch 281 : test_loss 1.3233746032714844\n",
      "Epoch 282 : train_loss 1.348974850124783\n",
      "Epoch 282 : test_loss 1.3395975494384766\n",
      "Epoch 283 : train_loss 1.3472317979600694\n",
      "Epoch 283 : test_loss 1.3645197143554688\n",
      "Epoch 284 : train_loss 1.3390262281629774\n",
      "Epoch 284 : test_loss 1.3103729553222656\n",
      "Epoch 285 : train_loss 1.3389028625488282\n",
      "Epoch 285 : test_loss 1.341404296875\n",
      "Epoch 286 : train_loss 1.3355866292317708\n",
      "Epoch 286 : test_loss 1.3187095184326172\n",
      "saved model\n",
      "Epoch 287 : train_loss 1.3335388827853734\n",
      "Epoch 287 : test_loss 1.2839725036621095\n",
      "saved model\n",
      "Epoch 288 : train_loss 1.3322805921766494\n",
      "Epoch 288 : test_loss 1.2834254455566407\n",
      "Epoch 289 : train_loss 1.3205590481228298\n",
      "Epoch 289 : test_loss 1.3011714782714843\n",
      "Epoch 290 : train_loss 1.343746836344401\n",
      "Epoch 290 : test_loss 1.3175265655517578\n",
      "Epoch 291 : train_loss 1.3252841152615018\n",
      "Epoch 291 : test_loss 1.3186479034423828\n",
      "Epoch 292 : train_loss 1.3242123548719618\n",
      "Epoch 292 : test_loss 1.3513576354980468\n",
      "Epoch 293 : train_loss 1.3379828050401477\n",
      "Epoch 293 : test_loss 1.2896539306640624\n",
      "Epoch 294 : train_loss 1.327366488986545\n",
      "Epoch 294 : test_loss 1.3180818786621094\n",
      "Epoch 295 : train_loss 1.3431554565429686\n",
      "Epoch 295 : test_loss 1.3103450317382812\n",
      "Epoch 296 : train_loss 1.3354639858669706\n",
      "Epoch 296 : test_loss 1.3244502868652344\n",
      "Epoch 297 : train_loss 1.3324122789171007\n",
      "Epoch 297 : test_loss 1.3205820007324218\n",
      "Epoch 298 : train_loss 1.3403312547471788\n",
      "Epoch 298 : test_loss 1.3215051879882813\n",
      "Epoch 299 : train_loss 1.3443542005750868\n",
      "Epoch 299 : test_loss 1.2858736877441406\n",
      "Epoch 300 : train_loss 1.3568832397460937\n",
      "Epoch 300 : test_loss 1.3171428833007812\n",
      "Epoch 301 : train_loss 1.3551561109754775\n",
      "Epoch 301 : test_loss 1.3875667724609375\n",
      "Epoch 302 : train_loss 1.350127675374349\n",
      "Epoch 302 : test_loss 1.3090431060791015\n",
      "Epoch 303 : train_loss 1.3420613844129774\n",
      "Epoch 303 : test_loss 1.29832177734375\n",
      "Epoch 304 : train_loss 1.3296100565592448\n",
      "Epoch 304 : test_loss 1.3209993896484375\n",
      "Epoch 305 : train_loss 1.3383583068847655\n",
      "Epoch 305 : test_loss 1.2996997680664062\n",
      "Epoch 306 : train_loss 1.3311138610839843\n",
      "Epoch 306 : test_loss 1.3536408081054687\n",
      "Epoch 307 : train_loss 1.3199642401801215\n",
      "Epoch 307 : test_loss 1.3305860290527343\n",
      "Epoch 308 : train_loss 1.339140662299262\n",
      "Epoch 308 : test_loss 1.3763753204345703\n",
      "Epoch 309 : train_loss 1.3567769775390626\n",
      "Epoch 309 : test_loss 1.3260886840820312\n",
      "Epoch 310 : train_loss 1.323525631374783\n",
      "Epoch 310 : test_loss 1.333591049194336\n",
      "Epoch 311 : train_loss 1.3254402398003473\n",
      "Epoch 311 : test_loss 1.349386184692383\n",
      "Epoch 312 : train_loss 1.3170096537272136\n",
      "Epoch 312 : test_loss 1.2917052307128907\n",
      "Epoch 313 : train_loss 1.3308052164713542\n",
      "Epoch 313 : test_loss 1.3317513122558593\n",
      "Epoch 314 : train_loss 1.3630051608615452\n",
      "Epoch 314 : test_loss 1.3022808685302734\n",
      "Epoch 315 : train_loss 1.334841047498915\n",
      "Epoch 315 : test_loss 1.3286715698242189\n",
      "Epoch 316 : train_loss 1.3409538879394531\n",
      "Epoch 316 : test_loss 1.3084188079833985\n",
      "Epoch 317 : train_loss 1.3422930908203126\n",
      "Epoch 317 : test_loss 1.3569776611328126\n",
      "saved model\n",
      "Epoch 318 : train_loss 1.3337460462782118\n",
      "Epoch 318 : test_loss 1.2786177062988282\n",
      "Epoch 319 : train_loss 1.329801032172309\n",
      "Epoch 319 : test_loss 1.3748365173339843\n",
      "Epoch 320 : train_loss 1.3451612752278646\n",
      "Epoch 320 : test_loss 1.3131014099121094\n",
      "Epoch 321 : train_loss 1.3399090610080295\n",
      "Epoch 321 : test_loss 1.3056238403320313\n",
      "Epoch 322 : train_loss 1.341896976047092\n",
      "Epoch 322 : test_loss 1.3285471801757813\n",
      "Epoch 323 : train_loss 1.3335140923394098\n",
      "Epoch 323 : test_loss 1.3737296295166015\n",
      "Epoch 324 : train_loss 1.3386620076497395\n",
      "Epoch 324 : test_loss 1.3151526184082032\n",
      "Epoch 325 : train_loss 1.3358478257921007\n",
      "Epoch 325 : test_loss 1.349076904296875\n",
      "Epoch 326 : train_loss 1.339258076985677\n",
      "Epoch 326 : test_loss 1.3372198181152344\n",
      "Epoch 327 : train_loss 1.3263250257703993\n",
      "Epoch 327 : test_loss 1.3249140167236328\n",
      "Epoch 328 : train_loss 1.3367813415527343\n",
      "Epoch 328 : test_loss 1.3316457214355468\n",
      "Epoch 329 : train_loss 1.3490095893012153\n",
      "Epoch 329 : test_loss 1.345121368408203\n",
      "Epoch 330 : train_loss 1.340950242784288\n",
      "Epoch 330 : test_loss 1.353639404296875\n",
      "Epoch 331 : train_loss 1.3366806030273437\n",
      "Epoch 331 : test_loss 1.3310023498535157\n",
      "Epoch 332 : train_loss 1.34201804945204\n",
      "Epoch 332 : test_loss 1.3470504455566406\n",
      "Epoch 333 : train_loss 1.3434648776584202\n",
      "Epoch 333 : test_loss 1.3145482482910156\n",
      "Epoch 334 : train_loss 1.343536102294922\n",
      "Epoch 334 : test_loss 1.2863703002929687\n",
      "Epoch 335 : train_loss 1.328624508327908\n",
      "Epoch 335 : test_loss 1.319445541381836\n",
      "Epoch 336 : train_loss 1.3354524502224392\n",
      "Epoch 336 : test_loss 1.3113645629882813\n",
      "Epoch 337 : train_loss 1.3523710259331598\n",
      "Epoch 337 : test_loss 1.3263036193847657\n",
      "Epoch 338 : train_loss 1.3357094184027778\n",
      "Epoch 338 : test_loss 1.3008997955322266\n",
      "Epoch 339 : train_loss 1.3615897793240017\n",
      "Epoch 339 : test_loss 1.3350816650390624\n",
      "Epoch 340 : train_loss 1.350602291531033\n",
      "Epoch 340 : test_loss 1.2978946380615235\n",
      "Epoch 341 : train_loss 1.3390980055067274\n",
      "Epoch 341 : test_loss 1.3079850158691406\n",
      "Epoch 342 : train_loss 1.3644537523057725\n",
      "Epoch 342 : test_loss 1.3514537811279297\n",
      "Epoch 343 : train_loss 1.3360022345648872\n",
      "Epoch 343 : test_loss 1.3056888732910157\n",
      "Epoch 344 : train_loss 1.3343608601888022\n",
      "Epoch 344 : test_loss 1.3407589874267578\n",
      "Epoch 345 : train_loss 1.3367729797363281\n",
      "Epoch 345 : test_loss 1.3361515808105469\n",
      "Epoch 346 : train_loss 1.3379774780273437\n",
      "Epoch 346 : test_loss 1.321254119873047\n",
      "Epoch 347 : train_loss 1.3288017171223958\n",
      "Epoch 347 : test_loss 1.322790267944336\n",
      "Epoch 348 : train_loss 1.3477694532606337\n",
      "Epoch 348 : test_loss 1.3146641845703124\n",
      "Epoch 349 : train_loss 1.3480616624620225\n",
      "Epoch 349 : test_loss 1.362779510498047\n",
      "Epoch 350 : train_loss 1.3445124037000868\n",
      "Epoch 350 : test_loss 1.3649083862304687\n",
      "Epoch 351 : train_loss 1.3316648593478733\n",
      "Epoch 351 : test_loss 1.317535675048828\n",
      "Epoch 352 : train_loss 1.3431167738172742\n",
      "Epoch 352 : test_loss 1.3464834442138671\n",
      "Epoch 353 : train_loss 1.3302884589301216\n",
      "Epoch 353 : test_loss 1.3324931488037108\n",
      "Epoch 354 : train_loss 1.3308653733995226\n",
      "Epoch 354 : test_loss 1.3327607116699218\n",
      "Epoch 355 : train_loss 1.3325831943088107\n",
      "Epoch 355 : test_loss 1.2916190795898437\n",
      "Epoch 356 : train_loss 1.3341234978569878\n",
      "Epoch 356 : test_loss 1.3488901519775391\n",
      "Epoch 357 : train_loss 1.336962446424696\n",
      "Epoch 357 : test_loss 1.3215621185302735\n",
      "Epoch 358 : train_loss 1.3264827202690972\n",
      "Epoch 358 : test_loss 1.321359161376953\n",
      "Epoch 359 : train_loss 1.341504896375868\n",
      "Epoch 359 : test_loss 1.3411661071777343\n",
      "Epoch 360 : train_loss 1.3431763271755643\n",
      "Epoch 360 : test_loss 1.3232823486328125\n",
      "Epoch 361 : train_loss 1.3287899509006076\n",
      "Epoch 361 : test_loss 1.327053207397461\n",
      "Epoch 362 : train_loss 1.3263829210069444\n",
      "Epoch 362 : test_loss 1.3198738708496094\n",
      "Epoch 363 : train_loss 1.3428973185221353\n",
      "Epoch 363 : test_loss 1.2958688354492187\n",
      "Epoch 364 : train_loss 1.334077880859375\n",
      "Epoch 364 : test_loss 1.313395294189453\n",
      "Epoch 365 : train_loss 1.3515281711154514\n",
      "Epoch 365 : test_loss 1.3484230651855469\n",
      "Epoch 366 : train_loss 1.334213585747613\n",
      "Epoch 366 : test_loss 1.3457868041992187\n",
      "Epoch 367 : train_loss 1.3448949652777777\n",
      "Epoch 367 : test_loss 1.2796602172851563\n",
      "Epoch 368 : train_loss 1.3272122870551215\n",
      "Epoch 368 : test_loss 1.342773712158203\n",
      "Epoch 369 : train_loss 1.3354702928331164\n",
      "Epoch 369 : test_loss 1.3124815368652343\n",
      "Epoch 370 : train_loss 1.3581859944661459\n",
      "Epoch 370 : test_loss 1.2963118286132813\n",
      "Epoch 371 : train_loss 1.3325667622884114\n",
      "Epoch 371 : test_loss 1.29695751953125\n",
      "Epoch 372 : train_loss 1.3437171936035157\n",
      "Epoch 372 : test_loss 1.2915706787109376\n",
      "Epoch 373 : train_loss 1.3388304070366754\n",
      "Epoch 373 : test_loss 1.3206528778076172\n",
      "Epoch 374 : train_loss 1.3333819173177084\n",
      "Epoch 374 : test_loss 1.338082015991211\n",
      "Epoch 375 : train_loss 1.3476002773708768\n",
      "Epoch 375 : test_loss 1.3386145629882813\n",
      "Epoch 376 : train_loss 1.3405692240397136\n",
      "Epoch 376 : test_loss 1.3421422576904296\n",
      "Epoch 377 : train_loss 1.332123775906033\n",
      "Epoch 377 : test_loss 1.301166275024414\n",
      "Epoch 378 : train_loss 1.3371882629394531\n",
      "Epoch 378 : test_loss 1.3688563232421875\n",
      "Epoch 379 : train_loss 1.3437240091959635\n",
      "Epoch 379 : test_loss 1.283912567138672\n",
      "Epoch 380 : train_loss 1.327198520236545\n",
      "Epoch 380 : test_loss 1.2835346832275392\n",
      "Epoch 381 : train_loss 1.3336095445421008\n",
      "Epoch 381 : test_loss 1.3465784606933593\n",
      "Epoch 382 : train_loss 1.330664791531033\n",
      "Epoch 382 : test_loss 1.3474083862304687\n",
      "Epoch 383 : train_loss 1.341876722547743\n",
      "Epoch 383 : test_loss 1.3146498107910156\n",
      "Epoch 384 : train_loss 1.3305091654459635\n",
      "Epoch 384 : test_loss 1.3159504699707032\n",
      "Epoch 385 : train_loss 1.3349637654622395\n",
      "Epoch 385 : test_loss 1.3301765899658202\n",
      "Epoch 386 : train_loss 1.347675303141276\n",
      "Epoch 386 : test_loss 1.32893017578125\n",
      "saved model\n",
      "Epoch 387 : train_loss 1.3305215962727865\n",
      "Epoch 387 : test_loss 1.2692512817382813\n",
      "Epoch 388 : train_loss 1.3291260748969185\n",
      "Epoch 388 : test_loss 1.2929854125976563\n",
      "Epoch 389 : train_loss 1.3350633680555555\n",
      "Epoch 389 : test_loss 1.3198941040039063\n",
      "Epoch 390 : train_loss 1.3199165547688803\n",
      "Epoch 390 : test_loss 1.2968100891113281\n",
      "Epoch 391 : train_loss 1.332281775580512\n",
      "Epoch 391 : test_loss 1.2982899780273438\n",
      "Epoch 392 : train_loss 1.3433113878038194\n",
      "Epoch 392 : test_loss 1.3379577026367186\n",
      "Epoch 393 : train_loss 1.3431470540364583\n",
      "Epoch 393 : test_loss 1.3350601959228516\n",
      "Epoch 394 : train_loss 1.3350072767469618\n",
      "Epoch 394 : test_loss 1.3405404357910156\n",
      "Epoch 395 : train_loss 1.3220901930067275\n",
      "Epoch 395 : test_loss 1.3365466766357421\n",
      "Epoch 396 : train_loss 1.3424127027723525\n",
      "Epoch 396 : test_loss 1.3577510681152343\n",
      "Epoch 397 : train_loss 1.3240846082899305\n",
      "Epoch 397 : test_loss 1.3549827575683593\n",
      "Epoch 398 : train_loss 1.3352348022460938\n",
      "Epoch 398 : test_loss 1.3136609039306641\n",
      "Epoch 399 : train_loss 1.3298401319715711\n",
      "Epoch 399 : test_loss 1.3244854125976562\n",
      "Epoch 400 : train_loss 1.328097171359592\n",
      "Epoch 400 : test_loss 1.3426914672851562\n",
      "Epoch 401 : train_loss 1.3404558681911893\n",
      "Epoch 401 : test_loss 1.3588228454589844\n",
      "Epoch 402 : train_loss 1.3320134819878473\n",
      "Epoch 402 : test_loss 1.3227142639160157\n",
      "Epoch 403 : train_loss 1.332068583170573\n",
      "Epoch 403 : test_loss 1.286375259399414\n",
      "Epoch 404 : train_loss 1.3357493794759114\n",
      "Epoch 404 : test_loss 1.2727276000976562\n",
      "Epoch 405 : train_loss 1.3346150071885852\n",
      "Epoch 405 : test_loss 1.2835604400634766\n",
      "Epoch 406 : train_loss 1.3463608805338543\n",
      "Epoch 406 : test_loss 1.3042137298583985\n",
      "Epoch 407 : train_loss 1.318173590766059\n",
      "Epoch 407 : test_loss 1.3545305938720704\n",
      "Epoch 408 : train_loss 1.3392030504014758\n",
      "Epoch 408 : test_loss 1.3131822814941407\n",
      "Epoch 409 : train_loss 1.3472049153645833\n",
      "Epoch 409 : test_loss 1.319856903076172\n",
      "Epoch 410 : train_loss 1.3290112982855902\n",
      "Epoch 410 : test_loss 1.3412315063476563\n",
      "Epoch 411 : train_loss 1.3158110995822483\n",
      "Epoch 411 : test_loss 1.2799756774902344\n",
      "Epoch 412 : train_loss 1.3282777642144097\n",
      "Epoch 412 : test_loss 1.2894418029785155\n",
      "Epoch 413 : train_loss 1.3504300638834636\n",
      "Epoch 413 : test_loss 1.3110367279052735\n",
      "Epoch 414 : train_loss 1.3264817097981771\n",
      "Epoch 414 : test_loss 1.3118251342773437\n",
      "saved model\n",
      "Epoch 415 : train_loss 1.3231624789767795\n",
      "Epoch 415 : test_loss 1.268575714111328\n",
      "Epoch 416 : train_loss 1.323530734592014\n",
      "Epoch 416 : test_loss 1.3494230041503905\n",
      "Epoch 417 : train_loss 1.3427569546169704\n",
      "Epoch 417 : test_loss 1.336695327758789\n",
      "Epoch 418 : train_loss 1.3353094923231337\n",
      "Epoch 418 : test_loss 1.3290430297851563\n",
      "Epoch 419 : train_loss 1.3259820963541666\n",
      "Epoch 419 : test_loss 1.3305288391113281\n",
      "Epoch 420 : train_loss 1.3240808512369793\n",
      "Epoch 420 : test_loss 1.2963600311279297\n",
      "Epoch 421 : train_loss 1.3559236857096355\n",
      "Epoch 421 : test_loss 1.2952519073486328\n",
      "Epoch 422 : train_loss 1.330693118625217\n",
      "Epoch 422 : test_loss 1.314662551879883\n",
      "saved model\n",
      "Epoch 423 : train_loss 1.342963392469618\n",
      "Epoch 423 : test_loss 1.2569601135253907\n",
      "Epoch 424 : train_loss 1.3352386169433594\n",
      "Epoch 424 : test_loss 1.3279757080078125\n",
      "Epoch 425 : train_loss 1.3292223239474827\n",
      "Epoch 425 : test_loss 1.3367042694091797\n",
      "Epoch 426 : train_loss 1.3344629143608941\n",
      "Epoch 426 : test_loss 1.3590675201416016\n",
      "Epoch 427 : train_loss 1.3405477905273437\n",
      "Epoch 427 : test_loss 1.3240053405761718\n",
      "Epoch 428 : train_loss 1.3397055019802517\n",
      "Epoch 428 : test_loss 1.3124766235351562\n",
      "Epoch 429 : train_loss 1.3341396993001302\n",
      "Epoch 429 : test_loss 1.2878458251953124\n",
      "Epoch 430 : train_loss 1.3470640835232204\n",
      "Epoch 430 : test_loss 1.3401851806640626\n",
      "Epoch 431 : train_loss 1.3252340223524306\n",
      "Epoch 431 : test_loss 1.3484363098144532\n",
      "Epoch 432 : train_loss 1.3453104281955295\n",
      "Epoch 432 : test_loss 1.2831272888183594\n",
      "Epoch 433 : train_loss 1.3415711602105034\n",
      "Epoch 433 : test_loss 1.310457321166992\n",
      "Epoch 434 : train_loss 1.3444514533148872\n",
      "Epoch 434 : test_loss 1.311956756591797\n",
      "Epoch 435 : train_loss 1.3342671067979601\n",
      "Epoch 435 : test_loss 1.3343430328369141\n",
      "Epoch 436 : train_loss 1.3301326599121093\n",
      "Epoch 436 : test_loss 1.3246962127685546\n",
      "Epoch 437 : train_loss 1.3236290961371528\n",
      "Epoch 437 : test_loss 1.3054661254882813\n",
      "Epoch 438 : train_loss 1.3432092692057291\n",
      "Epoch 438 : test_loss 1.2975365142822266\n",
      "Epoch 439 : train_loss 1.3311334465874567\n",
      "Epoch 439 : test_loss 1.2936009521484375\n",
      "Epoch 440 : train_loss 1.3348309360080295\n",
      "Epoch 440 : test_loss 1.3075710906982423\n",
      "Epoch 441 : train_loss 1.318944600423177\n",
      "Epoch 441 : test_loss 1.2991240234375\n",
      "Epoch 442 : train_loss 1.3270102267795139\n",
      "Epoch 442 : test_loss 1.3024458160400392\n",
      "Epoch 443 : train_loss 1.343513892279731\n",
      "Epoch 443 : test_loss 1.3238126525878906\n",
      "Epoch 444 : train_loss 1.3207318216959636\n",
      "Epoch 444 : test_loss 1.3415677185058594\n",
      "Epoch 445 : train_loss 1.3359951171875\n",
      "Epoch 445 : test_loss 1.308909164428711\n",
      "Epoch 446 : train_loss 1.3423556586371528\n",
      "Epoch 446 : test_loss 1.3027174072265626\n",
      "Epoch 447 : train_loss 1.3343550042046441\n",
      "Epoch 447 : test_loss 1.372760009765625\n",
      "Epoch 448 : train_loss 1.3302835693359376\n",
      "Epoch 448 : test_loss 1.318298065185547\n",
      "Epoch 449 : train_loss 1.3336405266655815\n",
      "Epoch 449 : test_loss 1.340149658203125\n",
      "Epoch 450 : train_loss 1.3346124572753906\n",
      "Epoch 450 : test_loss 1.3049746398925781\n",
      "Epoch 451 : train_loss 1.3365128106011284\n",
      "Epoch 451 : test_loss 1.3268155212402344\n",
      "Epoch 452 : train_loss 1.337218271891276\n",
      "Epoch 452 : test_loss 1.2815632019042968\n",
      "Epoch 453 : train_loss 1.33471676296658\n",
      "Epoch 453 : test_loss 1.343799102783203\n",
      "Epoch 454 : train_loss 1.3227400309244792\n",
      "Epoch 454 : test_loss 1.3564324645996093\n",
      "Epoch 455 : train_loss 1.341880622016059\n",
      "Epoch 455 : test_loss 1.300203582763672\n",
      "Epoch 456 : train_loss 1.3358228861490886\n",
      "Epoch 456 : test_loss 1.3414835815429687\n",
      "Epoch 457 : train_loss 1.3293509182400174\n",
      "Epoch 457 : test_loss 1.320260986328125\n",
      "Epoch 458 : train_loss 1.311476820203993\n",
      "Epoch 458 : test_loss 1.3010750732421874\n",
      "Epoch 459 : train_loss 1.3333062947591146\n",
      "Epoch 459 : test_loss 1.326740234375\n",
      "Epoch 460 : train_loss 1.3396706034342447\n",
      "Epoch 460 : test_loss 1.284438018798828\n",
      "Epoch 461 : train_loss 1.3404499749077692\n",
      "Epoch 461 : test_loss 1.3098432006835938\n",
      "Epoch 462 : train_loss 1.3247959493001302\n",
      "Epoch 462 : test_loss 1.3406776733398438\n",
      "saved model\n",
      "Epoch 463 : train_loss 1.318581058078342\n",
      "Epoch 463 : test_loss 1.2557342529296875\n",
      "Epoch 464 : train_loss 1.3249114312065973\n",
      "Epoch 464 : test_loss 1.3124246215820312\n",
      "Epoch 465 : train_loss 1.3248989800347222\n",
      "Epoch 465 : test_loss 1.3216701049804687\n",
      "Epoch 466 : train_loss 1.3178768378363714\n",
      "Epoch 466 : test_loss 1.3141484985351561\n",
      "Epoch 467 : train_loss 1.3286095547146268\n",
      "Epoch 467 : test_loss 1.3216844482421874\n",
      "Epoch 468 : train_loss 1.3365340067545572\n",
      "Epoch 468 : test_loss 1.3105898742675781\n",
      "Epoch 469 : train_loss 1.3332132263183594\n",
      "Epoch 469 : test_loss 1.3029308471679688\n",
      "Epoch 470 : train_loss 1.3342362874348959\n",
      "Epoch 470 : test_loss 1.3280066833496094\n",
      "Epoch 471 : train_loss 1.3361089647081164\n",
      "Epoch 471 : test_loss 1.3058135986328125\n",
      "Epoch 472 : train_loss 1.3474351162380642\n",
      "Epoch 472 : test_loss 1.310020004272461\n",
      "Epoch 473 : train_loss 1.3387884555392795\n",
      "Epoch 473 : test_loss 1.3179355621337892\n",
      "Epoch 474 : train_loss 1.331752956814236\n",
      "Epoch 474 : test_loss 1.3095689392089844\n",
      "Epoch 475 : train_loss 1.3249503445095485\n",
      "Epoch 475 : test_loss 1.3088168334960937\n",
      "Epoch 476 : train_loss 1.326009043375651\n",
      "Epoch 476 : test_loss 1.3167130737304686\n",
      "Epoch 477 : train_loss 1.3390293579101562\n",
      "Epoch 477 : test_loss 1.3085342407226563\n",
      "Epoch 478 : train_loss 1.3054207899305557\n",
      "Epoch 478 : test_loss 1.260377197265625\n",
      "Epoch 479 : train_loss 1.322192128499349\n",
      "Epoch 479 : test_loss 1.3395054626464844\n",
      "Epoch 480 : train_loss 1.3464050869411892\n",
      "Epoch 480 : test_loss 1.2927885131835937\n",
      "Epoch 481 : train_loss 1.3227901000976563\n",
      "Epoch 481 : test_loss 1.3203480224609374\n",
      "Epoch 482 : train_loss 1.3436698099772135\n",
      "Epoch 482 : test_loss 1.2986471405029296\n",
      "Epoch 483 : train_loss 1.3279247063530817\n",
      "Epoch 483 : test_loss 1.282787353515625\n",
      "Epoch 484 : train_loss 1.3227410549587673\n",
      "Epoch 484 : test_loss 1.3730108337402345\n",
      "Epoch 485 : train_loss 1.3129440138075086\n",
      "Epoch 485 : test_loss 1.3206155548095704\n",
      "Epoch 486 : train_loss 1.3302251790364583\n",
      "Epoch 486 : test_loss 1.3024993591308593\n",
      "Epoch 487 : train_loss 1.318336198594835\n",
      "Epoch 487 : test_loss 1.3008928833007813\n",
      "Epoch 488 : train_loss 1.3243086954752603\n",
      "Epoch 488 : test_loss 1.327046600341797\n",
      "Epoch 489 : train_loss 1.3382656860351563\n",
      "Epoch 489 : test_loss 1.359123489379883\n",
      "Epoch 490 : train_loss 1.3141734686957465\n",
      "Epoch 490 : test_loss 1.342655792236328\n",
      "Epoch 491 : train_loss 1.3280174323187934\n",
      "Epoch 491 : test_loss 1.3268002166748047\n",
      "Epoch 492 : train_loss 1.3191387091742621\n",
      "Epoch 492 : test_loss 1.3359714050292968\n",
      "Epoch 493 : train_loss 1.3388209126790365\n",
      "Epoch 493 : test_loss 1.3093746948242186\n",
      "Epoch 494 : train_loss 1.333295379638672\n",
      "Epoch 494 : test_loss 1.26939990234375\n",
      "Epoch 495 : train_loss 1.3246922302246094\n",
      "Epoch 495 : test_loss 1.2672434539794921\n",
      "Epoch 496 : train_loss 1.3350902981228299\n",
      "Epoch 496 : test_loss 1.3077062377929687\n",
      "Epoch 497 : train_loss 1.3318864474826388\n",
      "Epoch 497 : test_loss 1.3232322845458984\n",
      "Epoch 498 : train_loss 1.3234959852430555\n",
      "Epoch 498 : test_loss 1.2860420989990233\n",
      "Epoch 499 : train_loss 1.3244466925726996\n",
      "Epoch 499 : test_loss 1.3080060424804687\n",
      "Epoch 500 : train_loss 1.3322959459092882\n",
      "Epoch 500 : test_loss 1.312955810546875\n",
      "Epoch 501 : train_loss 1.3405296393500434\n",
      "Epoch 501 : test_loss 1.3123694915771484\n",
      "Epoch 502 : train_loss 1.332781724717882\n",
      "Epoch 502 : test_loss 1.313046875\n",
      "Epoch 503 : train_loss 1.3289907633463542\n",
      "Epoch 503 : test_loss 1.317208969116211\n",
      "Epoch 504 : train_loss 1.3247530992296006\n",
      "Epoch 504 : test_loss 1.311917724609375\n",
      "Epoch 505 : train_loss 1.3384700317382812\n",
      "Epoch 505 : test_loss 1.3164920349121094\n",
      "Epoch 506 : train_loss 1.32503859117296\n",
      "Epoch 506 : test_loss 1.2813236846923828\n",
      "Epoch 507 : train_loss 1.3335590311686198\n",
      "Epoch 507 : test_loss 1.3066947021484374\n",
      "Epoch 508 : train_loss 1.3339996032714845\n",
      "Epoch 508 : test_loss 1.2856348876953125\n",
      "Epoch 509 : train_loss 1.3230685594346787\n",
      "Epoch 509 : test_loss 1.3252121887207031\n",
      "Epoch 510 : train_loss 1.331301317003038\n",
      "Epoch 510 : test_loss 1.3206129608154298\n",
      "Epoch 511 : train_loss 1.3158675740559895\n",
      "Epoch 511 : test_loss 1.3133013305664063\n",
      "Epoch 512 : train_loss 1.328065890842014\n",
      "Epoch 512 : test_loss 1.3116767883300782\n",
      "Epoch 513 : train_loss 1.3236738179524739\n",
      "Epoch 513 : test_loss 1.3495918273925782\n",
      "saved model\n",
      "Epoch 514 : train_loss 1.3304213595920138\n",
      "Epoch 514 : test_loss 1.2424030609130858\n",
      "Epoch 515 : train_loss 1.3265312771267361\n",
      "Epoch 515 : test_loss 1.298434066772461\n",
      "Epoch 516 : train_loss 1.3378280198838977\n",
      "Epoch 516 : test_loss 1.3103829040527344\n",
      "Epoch 517 : train_loss 1.3290333523220486\n",
      "Epoch 517 : test_loss 1.3456634826660157\n",
      "Epoch 518 : train_loss 1.3266668904622396\n",
      "Epoch 518 : test_loss 1.363195770263672\n",
      "Epoch 519 : train_loss 1.3210005628797743\n",
      "Epoch 519 : test_loss 1.2966882934570312\n",
      "Epoch 520 : train_loss 1.3192256842719183\n",
      "Epoch 520 : test_loss 1.3350330505371093\n",
      "Epoch 521 : train_loss 1.3117600911458334\n",
      "Epoch 521 : test_loss 1.3166163482666016\n",
      "Epoch 522 : train_loss 1.3269643927680121\n",
      "Epoch 522 : test_loss 1.2884715270996094\n",
      "Epoch 523 : train_loss 1.3395640665690105\n",
      "Epoch 523 : test_loss 1.2961791076660156\n",
      "Epoch 524 : train_loss 1.3196864725748698\n",
      "Epoch 524 : test_loss 1.3537962951660156\n",
      "Epoch 525 : train_loss 1.3374280700683594\n",
      "Epoch 525 : test_loss 1.2952495727539062\n",
      "Epoch 526 : train_loss 1.3186880764431423\n",
      "Epoch 526 : test_loss 1.3124518890380859\n",
      "Epoch 527 : train_loss 1.324891133626302\n",
      "Epoch 527 : test_loss 1.3336937866210938\n",
      "Epoch 528 : train_loss 1.3349965616861978\n",
      "Epoch 528 : test_loss 1.2844317474365234\n",
      "Epoch 529 : train_loss 1.3209686991373697\n",
      "Epoch 529 : test_loss 1.290306869506836\n",
      "Epoch 530 : train_loss 1.335386739095052\n",
      "Epoch 530 : test_loss 1.3463612060546875\n",
      "Epoch 531 : train_loss 1.3152157864040799\n",
      "Epoch 531 : test_loss 1.3078876342773438\n",
      "Epoch 532 : train_loss 1.335640838623047\n",
      "Epoch 532 : test_loss 1.3385277099609374\n",
      "Epoch 533 : train_loss 1.3311133965386284\n",
      "Epoch 533 : test_loss 1.2921600189208984\n",
      "Epoch 534 : train_loss 1.3167251993815103\n",
      "Epoch 534 : test_loss 1.332455352783203\n",
      "Epoch 535 : train_loss 1.330463382297092\n",
      "Epoch 535 : test_loss 1.330412567138672\n",
      "Epoch 536 : train_loss 1.3248001505533855\n",
      "Epoch 536 : test_loss 1.319103271484375\n",
      "Epoch 537 : train_loss 1.3292220255533853\n",
      "Epoch 537 : test_loss 1.2766222229003907\n",
      "Epoch 538 : train_loss 1.304147440592448\n",
      "Epoch 538 : test_loss 1.3252781219482421\n",
      "Epoch 539 : train_loss 1.315520246717665\n",
      "Epoch 539 : test_loss 1.295420150756836\n",
      "Epoch 540 : train_loss 1.3219342515733508\n",
      "Epoch 540 : test_loss 1.3105588836669921\n",
      "Epoch 541 : train_loss 1.331521250406901\n",
      "Epoch 541 : test_loss 1.3212977600097657\n",
      "Epoch 542 : train_loss 1.3294737514919706\n",
      "Epoch 542 : test_loss 1.2896127014160157\n",
      "Epoch 543 : train_loss 1.3272130635579427\n",
      "Epoch 543 : test_loss 1.2982603759765625\n",
      "Epoch 544 : train_loss 1.3287312215169271\n",
      "Epoch 544 : test_loss 1.3227574310302734\n",
      "Epoch 545 : train_loss 1.3396081848144532\n",
      "Epoch 545 : test_loss 1.3047710266113282\n",
      "Epoch 546 : train_loss 1.3226834682888455\n",
      "Epoch 546 : test_loss 1.323765411376953\n",
      "Epoch 547 : train_loss 1.3174959343804253\n",
      "Epoch 547 : test_loss 1.297969512939453\n",
      "Epoch 548 : train_loss 1.3171946546766493\n",
      "Epoch 548 : test_loss 1.325623291015625\n",
      "Epoch 549 : train_loss 1.3159539761013455\n",
      "Epoch 549 : test_loss 1.3088892517089843\n",
      "Epoch 550 : train_loss 1.326982398139106\n",
      "Epoch 550 : test_loss 1.346066864013672\n",
      "Epoch 551 : train_loss 1.3167548319498699\n",
      "Epoch 551 : test_loss 1.2872959442138672\n",
      "Epoch 552 : train_loss 1.3074966159396701\n",
      "Epoch 552 : test_loss 1.310327178955078\n",
      "Epoch 553 : train_loss 1.3306407877604167\n",
      "Epoch 553 : test_loss 1.2976189270019531\n",
      "Epoch 554 : train_loss 1.3328777601453994\n",
      "Epoch 554 : test_loss 1.2922423706054686\n",
      "Epoch 555 : train_loss 1.325976789686415\n",
      "Epoch 555 : test_loss 1.2914962463378907\n",
      "Epoch 556 : train_loss 1.3392798292371961\n",
      "Epoch 556 : test_loss 1.2941536865234375\n",
      "Epoch 557 : train_loss 1.3092304924858942\n",
      "Epoch 557 : test_loss 1.3017216796875\n",
      "Epoch 558 : train_loss 1.3314999254014757\n",
      "Epoch 558 : test_loss 1.2962515258789062\n",
      "Epoch 559 : train_loss 1.314863243950738\n",
      "Epoch 559 : test_loss 1.3315167388916016\n",
      "Epoch 560 : train_loss 1.3207868347167968\n",
      "Epoch 560 : test_loss 1.2634109191894531\n",
      "Epoch 561 : train_loss 1.325303958468967\n",
      "Epoch 561 : test_loss 1.323914520263672\n",
      "Epoch 562 : train_loss 1.334626451280382\n",
      "Epoch 562 : test_loss 1.287648727416992\n",
      "Epoch 563 : train_loss 1.3104912414550782\n",
      "Epoch 563 : test_loss 1.2696728363037109\n",
      "Epoch 564 : train_loss 1.3154930352105034\n",
      "Epoch 564 : test_loss 1.3253099365234375\n",
      "Epoch 565 : train_loss 1.3331156548394096\n",
      "Epoch 565 : test_loss 1.2816874694824218\n",
      "Epoch 566 : train_loss 1.3282696804470486\n",
      "Epoch 566 : test_loss 1.3042327117919923\n",
      "Epoch 567 : train_loss 1.329789289686415\n",
      "Epoch 567 : test_loss 1.287813751220703\n",
      "Epoch 568 : train_loss 1.3230724962022569\n",
      "Epoch 568 : test_loss 1.300293914794922\n",
      "Epoch 569 : train_loss 1.3213000081380208\n",
      "Epoch 569 : test_loss 1.2885052642822266\n",
      "Epoch 570 : train_loss 1.3258136189778646\n",
      "Epoch 570 : test_loss 1.3261013793945313\n",
      "Epoch 571 : train_loss 1.3151925455729168\n",
      "Epoch 571 : test_loss 1.30870556640625\n",
      "Epoch 572 : train_loss 1.3208361206054688\n",
      "Epoch 572 : test_loss 1.2947068786621094\n",
      "Epoch 573 : train_loss 1.3274093526204427\n",
      "Epoch 573 : test_loss 1.3434148254394531\n",
      "Epoch 574 : train_loss 1.3140960489908855\n",
      "Epoch 574 : test_loss 1.3050480041503907\n",
      "Epoch 575 : train_loss 1.3232560662163628\n",
      "Epoch 575 : test_loss 1.3270526733398438\n",
      "Epoch 576 : train_loss 1.3362085266113282\n",
      "Epoch 576 : test_loss 1.3346292419433594\n",
      "Epoch 577 : train_loss 1.3276248033311633\n",
      "Epoch 577 : test_loss 1.301885986328125\n",
      "Epoch 578 : train_loss 1.3302044915093316\n",
      "Epoch 578 : test_loss 1.2901555480957032\n",
      "Epoch 579 : train_loss 1.306087124294705\n",
      "Epoch 579 : test_loss 1.299607666015625\n",
      "Epoch 580 : train_loss 1.321483425564236\n",
      "Epoch 580 : test_loss 1.3297248992919921\n",
      "Epoch 581 : train_loss 1.3309713338216145\n",
      "Epoch 581 : test_loss 1.346887191772461\n",
      "Epoch 582 : train_loss 1.3277887268066406\n",
      "Epoch 582 : test_loss 1.2988473510742187\n",
      "Epoch 583 : train_loss 1.325749247233073\n",
      "Epoch 583 : test_loss 1.3406450347900392\n",
      "Epoch 584 : train_loss 1.317839586046007\n",
      "Epoch 584 : test_loss 1.3168786010742188\n",
      "Epoch 585 : train_loss 1.3318746914333768\n",
      "Epoch 585 : test_loss 1.3384413146972656\n",
      "Epoch 586 : train_loss 1.3423501281738281\n",
      "Epoch 586 : test_loss 1.2685313415527344\n",
      "Epoch 587 : train_loss 1.3106465894911024\n",
      "Epoch 587 : test_loss 1.329301223754883\n",
      "Epoch 588 : train_loss 1.3195501064724393\n",
      "Epoch 588 : test_loss 1.296770767211914\n",
      "Epoch 589 : train_loss 1.3094509752061632\n",
      "Epoch 589 : test_loss 1.3302338256835937\n",
      "Epoch 590 : train_loss 1.3246717054578994\n",
      "Epoch 590 : test_loss 1.2766500854492187\n",
      "Epoch 591 : train_loss 1.3223952026367187\n",
      "Epoch 591 : test_loss 1.29151025390625\n",
      "Epoch 592 : train_loss 1.3218556857638888\n",
      "Epoch 592 : test_loss 1.2796213836669923\n",
      "Epoch 593 : train_loss 1.3235108066134982\n",
      "Epoch 593 : test_loss 1.327569366455078\n",
      "Epoch 594 : train_loss 1.3254368760850694\n",
      "Epoch 594 : test_loss 1.3316572570800782\n",
      "Epoch 595 : train_loss 1.341936041937934\n",
      "Epoch 595 : test_loss 1.3396400451660155\n",
      "Epoch 596 : train_loss 1.3208772752549913\n",
      "Epoch 596 : test_loss 1.341418670654297\n",
      "Epoch 597 : train_loss 1.3159122144911024\n",
      "Epoch 597 : test_loss 1.2903237915039063\n",
      "Epoch 598 : train_loss 1.301053731282552\n",
      "Epoch 598 : test_loss 1.343844451904297\n",
      "Epoch 599 : train_loss 1.3085983581542968\n",
      "Epoch 599 : test_loss 1.325677017211914\n",
      "Epoch 600 : train_loss 1.3246673244900173\n",
      "Epoch 600 : test_loss 1.2626763458251953\n",
      "Epoch 601 : train_loss 1.3328722364637586\n",
      "Epoch 601 : test_loss 1.3034761962890624\n",
      "Epoch 602 : train_loss 1.330757805718316\n",
      "Epoch 602 : test_loss 1.3061693572998048\n",
      "Epoch 603 : train_loss 1.3147259555392796\n",
      "Epoch 603 : test_loss 1.3132849578857422\n",
      "Epoch 604 : train_loss 1.3185445387098524\n",
      "Epoch 604 : test_loss 1.2964331970214844\n",
      "Epoch 605 : train_loss 1.3172119683159722\n",
      "Epoch 605 : test_loss 1.3513869171142578\n",
      "Epoch 606 : train_loss 1.3279982096354166\n",
      "Epoch 606 : test_loss 1.3107560119628907\n",
      "Epoch 607 : train_loss 1.321830105251736\n",
      "Epoch 607 : test_loss 1.2685213928222656\n",
      "Epoch 608 : train_loss 1.312600599500868\n",
      "Epoch 608 : test_loss 1.2917774658203125\n",
      "Epoch 609 : train_loss 1.3236489630805122\n",
      "Epoch 609 : test_loss 1.3106629028320314\n",
      "Epoch 610 : train_loss 1.3198296813964843\n",
      "Epoch 610 : test_loss 1.2907386932373046\n",
      "Epoch 611 : train_loss 1.3147465515136718\n",
      "Epoch 611 : test_loss 1.283452362060547\n",
      "Epoch 612 : train_loss 1.3149268731011285\n",
      "Epoch 612 : test_loss 1.3458332214355468\n",
      "Epoch 613 : train_loss 1.3250340067545572\n",
      "Epoch 613 : test_loss 1.3274225311279297\n",
      "Epoch 614 : train_loss 1.3190423177083332\n",
      "Epoch 614 : test_loss 1.347012237548828\n",
      "Epoch 615 : train_loss 1.3132501152886285\n",
      "Epoch 615 : test_loss 1.3145675354003907\n",
      "Epoch 616 : train_loss 1.306238271077474\n",
      "Epoch 616 : test_loss 1.3261277160644531\n",
      "Epoch 617 : train_loss 1.3068355746799045\n",
      "Epoch 617 : test_loss 1.3385856475830078\n",
      "Epoch 618 : train_loss 1.3273487447102865\n",
      "Epoch 618 : test_loss 1.2887638397216796\n",
      "Epoch 619 : train_loss 1.3207135416666667\n",
      "Epoch 619 : test_loss 1.2946905364990235\n",
      "Epoch 620 : train_loss 1.322258782280816\n",
      "Epoch 620 : test_loss 1.345389175415039\n",
      "Epoch 621 : train_loss 1.322068122016059\n",
      "Epoch 621 : test_loss 1.261189239501953\n",
      "Epoch 622 : train_loss 1.3178142700195312\n",
      "Epoch 622 : test_loss 1.2617788848876954\n",
      "Epoch 623 : train_loss 1.323484612358941\n",
      "Epoch 623 : test_loss 1.250268814086914\n",
      "Epoch 624 : train_loss 1.3308547668457031\n",
      "Epoch 624 : test_loss 1.2767642517089843\n",
      "Epoch 625 : train_loss 1.3176101616753473\n",
      "Epoch 625 : test_loss 1.2626394958496094\n",
      "Epoch 626 : train_loss 1.3300853576660157\n",
      "Epoch 626 : test_loss 1.268221160888672\n",
      "Epoch 627 : train_loss 1.3117648417154948\n",
      "Epoch 627 : test_loss 1.2778210296630859\n",
      "Epoch 628 : train_loss 1.3280829705132378\n",
      "Epoch 628 : test_loss 1.2627400817871093\n",
      "Epoch 629 : train_loss 1.3264092983669704\n",
      "Epoch 629 : test_loss 1.2943299865722657\n",
      "Epoch 630 : train_loss 1.3191177469889324\n",
      "Epoch 630 : test_loss 1.2702177734375\n",
      "Epoch 631 : train_loss 1.322088836669922\n",
      "Epoch 631 : test_loss 1.2876927032470704\n",
      "Epoch 632 : train_loss 1.3100284932454427\n",
      "Epoch 632 : test_loss 1.3008777770996094\n",
      "Epoch 633 : train_loss 1.3127114020453559\n",
      "Epoch 633 : test_loss 1.2995894775390624\n",
      "Epoch 634 : train_loss 1.3112645467122397\n",
      "Epoch 634 : test_loss 1.311071044921875\n",
      "Epoch 635 : train_loss 1.3040377332899304\n",
      "Epoch 635 : test_loss 1.2876533813476563\n",
      "Epoch 636 : train_loss 1.3188594835069445\n",
      "Epoch 636 : test_loss 1.3253933410644532\n",
      "Epoch 637 : train_loss 1.3098938937717013\n",
      "Epoch 637 : test_loss 1.2673060150146485\n",
      "Epoch 638 : train_loss 1.3201656867133247\n",
      "Epoch 638 : test_loss 1.3126326293945312\n",
      "Epoch 639 : train_loss 1.319626441107856\n",
      "Epoch 639 : test_loss 1.32918603515625\n",
      "Epoch 640 : train_loss 1.3276485799153646\n",
      "Epoch 640 : test_loss 1.3432481384277344\n",
      "Epoch 641 : train_loss 1.3001181979709202\n",
      "Epoch 641 : test_loss 1.254217742919922\n",
      "Epoch 642 : train_loss 1.3169728224012587\n",
      "Epoch 642 : test_loss 1.319380340576172\n",
      "Epoch 643 : train_loss 1.3172699652777777\n",
      "Epoch 643 : test_loss 1.3296170043945312\n",
      "Epoch 644 : train_loss 1.322515875922309\n",
      "Epoch 644 : test_loss 1.346900604248047\n",
      "Epoch 645 : train_loss 1.3228801981608074\n",
      "Epoch 645 : test_loss 1.3105075073242187\n",
      "Epoch 646 : train_loss 1.3160737067328558\n",
      "Epoch 646 : test_loss 1.2860640869140625\n",
      "Epoch 647 : train_loss 1.3208135342068141\n",
      "Epoch 647 : test_loss 1.3014102783203125\n",
      "Epoch 648 : train_loss 1.3299545186360677\n",
      "Epoch 648 : test_loss 1.3059249877929688\n",
      "Epoch 649 : train_loss 1.3222916056315104\n",
      "Epoch 649 : test_loss 1.271238006591797\n",
      "Epoch 650 : train_loss 1.3274546949598525\n",
      "Epoch 650 : test_loss 1.301379913330078\n",
      "Epoch 651 : train_loss 1.3274185485839844\n",
      "Epoch 651 : test_loss 1.3496898193359375\n",
      "Epoch 652 : train_loss 1.3085584988064236\n",
      "Epoch 652 : test_loss 1.2840572509765624\n",
      "Epoch 653 : train_loss 1.3088731011284722\n",
      "Epoch 653 : test_loss 1.301518096923828\n",
      "Epoch 654 : train_loss 1.3162540079752605\n",
      "Epoch 654 : test_loss 1.3382840118408204\n",
      "Epoch 655 : train_loss 1.323006147596571\n",
      "Epoch 655 : test_loss 1.2773910522460938\n",
      "Epoch 656 : train_loss 1.3106390991210937\n",
      "Epoch 656 : test_loss 1.319219253540039\n",
      "Epoch 657 : train_loss 1.336618160671658\n",
      "Epoch 657 : test_loss 1.3144719848632813\n",
      "Epoch 658 : train_loss 1.3196114637586807\n",
      "Epoch 658 : test_loss 1.2793735046386718\n",
      "Epoch 659 : train_loss 1.30624808078342\n",
      "Epoch 659 : test_loss 1.2593329162597657\n",
      "Epoch 660 : train_loss 1.310862328423394\n",
      "Epoch 660 : test_loss 1.2827706756591797\n",
      "Epoch 661 : train_loss 1.323493916829427\n",
      "Epoch 661 : test_loss 1.2791641845703126\n",
      "Epoch 662 : train_loss 1.3211095682779948\n",
      "Epoch 662 : test_loss 1.320579071044922\n",
      "Epoch 663 : train_loss 1.319424072265625\n",
      "Epoch 663 : test_loss 1.2817976531982422\n",
      "Epoch 664 : train_loss 1.3271125386555989\n",
      "Epoch 664 : test_loss 1.348367889404297\n",
      "Epoch 665 : train_loss 1.3265396253797743\n",
      "Epoch 665 : test_loss 1.3261788024902343\n",
      "Epoch 666 : train_loss 1.3254900546603732\n",
      "Epoch 666 : test_loss 1.2740218353271484\n",
      "Epoch 667 : train_loss 1.3239512159559461\n",
      "Epoch 667 : test_loss 1.3283187408447266\n",
      "Epoch 668 : train_loss 1.311871344672309\n",
      "Epoch 668 : test_loss 1.2811607818603516\n",
      "Epoch 669 : train_loss 1.3171087239583332\n",
      "Epoch 669 : test_loss 1.3130054931640625\n",
      "Epoch 670 : train_loss 1.321054433186849\n",
      "Epoch 670 : test_loss 1.2915714111328125\n",
      "Epoch 671 : train_loss 1.3236063368055555\n",
      "Epoch 671 : test_loss 1.2937100982666017\n",
      "saved model\n",
      "Epoch 672 : train_loss 1.3210830586751303\n",
      "Epoch 672 : test_loss 1.2308487854003907\n",
      "Epoch 673 : train_loss 1.3290299784342448\n",
      "Epoch 673 : test_loss 1.3059251403808594\n",
      "Epoch 674 : train_loss 1.3147902255588109\n",
      "Epoch 674 : test_loss 1.2981712646484376\n",
      "Epoch 675 : train_loss 1.3223978441026476\n",
      "Epoch 675 : test_loss 1.2928053894042968\n",
      "Epoch 676 : train_loss 1.3167077094184028\n",
      "Epoch 676 : test_loss 1.2607740173339843\n",
      "Epoch 677 : train_loss 1.3340520799424913\n",
      "Epoch 677 : test_loss 1.3708646240234375\n",
      "Epoch 678 : train_loss 1.3254965006510417\n",
      "Epoch 678 : test_loss 1.2983602142333985\n",
      "Epoch 679 : train_loss 1.320986334906684\n",
      "Epoch 679 : test_loss 1.3017318420410156\n",
      "Epoch 680 : train_loss 1.3259242248535157\n",
      "Epoch 680 : test_loss 1.290643524169922\n",
      "Epoch 681 : train_loss 1.3198228963216145\n",
      "Epoch 681 : test_loss 1.264351776123047\n",
      "Epoch 682 : train_loss 1.3195169982910155\n",
      "Epoch 682 : test_loss 1.3198811950683593\n",
      "Epoch 683 : train_loss 1.3180172593858508\n",
      "Epoch 683 : test_loss 1.3148829498291015\n",
      "Epoch 684 : train_loss 1.329452151828342\n",
      "Epoch 684 : test_loss 1.2945164337158204\n",
      "Epoch 685 : train_loss 1.3155826314290364\n",
      "Epoch 685 : test_loss 1.3311032409667969\n",
      "Epoch 686 : train_loss 1.3099052734375\n",
      "Epoch 686 : test_loss 1.3386973876953125\n",
      "Epoch 687 : train_loss 1.3168086920844184\n",
      "Epoch 687 : test_loss 1.29883056640625\n",
      "Epoch 688 : train_loss 1.3221725531684028\n",
      "Epoch 688 : test_loss 1.2342512512207031\n",
      "Epoch 689 : train_loss 1.3252484300401475\n",
      "Epoch 689 : test_loss 1.2742874145507812\n",
      "Epoch 690 : train_loss 1.3278548855251735\n",
      "Epoch 690 : test_loss 1.3547725830078126\n",
      "Epoch 691 : train_loss 1.3296998731825087\n",
      "Epoch 691 : test_loss 1.354302001953125\n",
      "Epoch 692 : train_loss 1.3094742465549045\n",
      "Epoch 692 : test_loss 1.307840377807617\n",
      "Epoch 693 : train_loss 1.3282877943250868\n",
      "Epoch 693 : test_loss 1.2853876647949218\n",
      "Epoch 694 : train_loss 1.3009997389051648\n",
      "Epoch 694 : test_loss 1.2935402526855468\n",
      "Epoch 695 : train_loss 1.3173877122667101\n",
      "Epoch 695 : test_loss 1.3000787811279297\n",
      "Epoch 696 : train_loss 1.3157095201280382\n",
      "Epoch 696 : test_loss 1.35498779296875\n",
      "Epoch 697 : train_loss 1.3262282341851128\n",
      "Epoch 697 : test_loss 1.2736848754882812\n",
      "Epoch 698 : train_loss 1.3146355624728732\n",
      "Epoch 698 : test_loss 1.3104825744628905\n",
      "Epoch 699 : train_loss 1.311357703314887\n",
      "Epoch 699 : test_loss 1.304152618408203\n",
      "Epoch 700 : train_loss 1.3084016554090712\n",
      "Epoch 700 : test_loss 1.2890798950195312\n",
      "Epoch 701 : train_loss 1.3089679463704427\n",
      "Epoch 701 : test_loss 1.3211109924316407\n",
      "Epoch 702 : train_loss 1.3200575425889758\n",
      "Epoch 702 : test_loss 1.2810535125732423\n",
      "Epoch 703 : train_loss 1.335410892062717\n",
      "Epoch 703 : test_loss 1.349327178955078\n",
      "Epoch 704 : train_loss 1.314790788438585\n",
      "Epoch 704 : test_loss 1.279189956665039\n",
      "Epoch 705 : train_loss 1.3138863186306424\n",
      "Epoch 705 : test_loss 1.3009021301269532\n",
      "Epoch 706 : train_loss 1.328946329752604\n",
      "Epoch 706 : test_loss 1.3045060119628906\n",
      "Epoch 707 : train_loss 1.310323503282335\n",
      "Epoch 707 : test_loss 1.2674665985107423\n",
      "Epoch 708 : train_loss 1.3188897128634982\n",
      "Epoch 708 : test_loss 1.341277587890625\n",
      "Epoch 709 : train_loss 1.3152212999131945\n",
      "Epoch 709 : test_loss 1.2776764526367188\n",
      "Epoch 710 : train_loss 1.3080781182183159\n",
      "Epoch 710 : test_loss 1.3468934020996093\n",
      "Epoch 711 : train_loss 1.3166155022515191\n",
      "Epoch 711 : test_loss 1.2637400817871094\n",
      "Epoch 712 : train_loss 1.326818101671007\n",
      "Epoch 712 : test_loss 1.3024017333984375\n",
      "Epoch 713 : train_loss 1.3308829583062065\n",
      "Epoch 713 : test_loss 1.2793392028808594\n",
      "Epoch 714 : train_loss 1.321121090359158\n",
      "Epoch 714 : test_loss 1.3005948333740234\n",
      "Epoch 715 : train_loss 1.320531962076823\n",
      "Epoch 715 : test_loss 1.3023940734863282\n",
      "Epoch 716 : train_loss 1.308461466471354\n",
      "Epoch 716 : test_loss 1.2715150756835938\n",
      "Epoch 717 : train_loss 1.314403778076172\n",
      "Epoch 717 : test_loss 1.3238295288085937\n",
      "Epoch 718 : train_loss 1.3130048014322917\n",
      "Epoch 718 : test_loss 1.2984651489257812\n",
      "Epoch 719 : train_loss 1.3131751437717014\n",
      "Epoch 719 : test_loss 1.2558555145263672\n",
      "Epoch 720 : train_loss 1.327148189968533\n",
      "Epoch 720 : test_loss 1.2899519958496093\n",
      "Epoch 721 : train_loss 1.3117321234809027\n",
      "Epoch 721 : test_loss 1.2767568359375\n",
      "Epoch 722 : train_loss 1.3197103000217014\n",
      "Epoch 722 : test_loss 1.306178436279297\n",
      "Epoch 723 : train_loss 1.3152152709960938\n",
      "Epoch 723 : test_loss 1.2712575073242187\n",
      "Epoch 724 : train_loss 1.3173592563205294\n",
      "Epoch 724 : test_loss 1.2758329162597657\n",
      "Epoch 725 : train_loss 1.3139848192003039\n",
      "Epoch 725 : test_loss 1.2970457153320312\n",
      "Epoch 726 : train_loss 1.3228199734157986\n",
      "Epoch 726 : test_loss 1.2940325622558593\n",
      "Epoch 727 : train_loss 1.318401343451606\n",
      "Epoch 727 : test_loss 1.3332723541259766\n",
      "Epoch 728 : train_loss 1.3174544135199653\n",
      "Epoch 728 : test_loss 1.2838150024414063\n",
      "Epoch 729 : train_loss 1.3083548923068575\n",
      "Epoch 729 : test_loss 1.3229781799316407\n",
      "Epoch 730 : train_loss 1.3197745598687065\n",
      "Epoch 730 : test_loss 1.3222873840332032\n",
      "Epoch 731 : train_loss 1.3188624572753906\n",
      "Epoch 731 : test_loss 1.2656876831054686\n",
      "Epoch 732 : train_loss 1.3049157240125868\n",
      "Epoch 732 : test_loss 1.2864328002929688\n",
      "Epoch 733 : train_loss 1.3233333672417535\n",
      "Epoch 733 : test_loss 1.2997082824707031\n",
      "Epoch 734 : train_loss 1.311274176703559\n",
      "Epoch 734 : test_loss 1.2762183990478515\n",
      "Epoch 735 : train_loss 1.3154552171495226\n",
      "Epoch 735 : test_loss 1.3183882141113281\n",
      "Epoch 736 : train_loss 1.3206409573025173\n",
      "Epoch 736 : test_loss 1.2805935516357423\n",
      "Epoch 737 : train_loss 1.317147233751085\n",
      "Epoch 737 : test_loss 1.3207583770751954\n",
      "Epoch 738 : train_loss 1.3089231940375434\n",
      "Epoch 738 : test_loss 1.277115234375\n",
      "Epoch 739 : train_loss 1.3106624213324654\n",
      "Epoch 739 : test_loss 1.3325383758544922\n",
      "Epoch 740 : train_loss 1.316509284125434\n",
      "Epoch 740 : test_loss 1.3096954193115233\n",
      "Epoch 741 : train_loss 1.29939990234375\n",
      "Epoch 741 : test_loss 1.3071954956054688\n",
      "Epoch 742 : train_loss 1.3207823927137587\n",
      "Epoch 742 : test_loss 1.2482953796386718\n",
      "Epoch 743 : train_loss 1.3111684977213542\n",
      "Epoch 743 : test_loss 1.2889201354980468\n",
      "Epoch 744 : train_loss 1.2852823079427083\n",
      "Epoch 744 : test_loss 1.3248063354492188\n",
      "Epoch 745 : train_loss 1.3108211839463975\n",
      "Epoch 745 : test_loss 1.3199425659179687\n",
      "Epoch 746 : train_loss 1.309224850124783\n",
      "Epoch 746 : test_loss 1.2991598815917969\n",
      "Epoch 747 : train_loss 1.312359388563368\n",
      "Epoch 747 : test_loss 1.2949724731445313\n",
      "Epoch 748 : train_loss 1.3144763624403213\n",
      "Epoch 748 : test_loss 1.2773870544433594\n",
      "Epoch 749 : train_loss 1.321109120686849\n",
      "Epoch 749 : test_loss 1.2760446929931641\n",
      "Epoch 750 : train_loss 1.3153800387912327\n",
      "Epoch 750 : test_loss 1.303732666015625\n",
      "Epoch 751 : train_loss 1.3169660949707032\n",
      "Epoch 751 : test_loss 1.2949893798828125\n",
      "Epoch 752 : train_loss 1.3214399787055122\n",
      "Epoch 752 : test_loss 1.3289259033203125\n",
      "Epoch 753 : train_loss 1.3127388814290364\n",
      "Epoch 753 : test_loss 1.2655751037597656\n",
      "Epoch 754 : train_loss 1.3089628533257378\n",
      "Epoch 754 : test_loss 1.299064208984375\n",
      "Epoch 755 : train_loss 1.313452362060547\n",
      "Epoch 755 : test_loss 1.3300765838623048\n",
      "Epoch 756 : train_loss 1.3251614481608074\n",
      "Epoch 756 : test_loss 1.3080310363769532\n",
      "Epoch 757 : train_loss 1.3000119086371529\n",
      "Epoch 757 : test_loss 1.2940924072265625\n",
      "Epoch 758 : train_loss 1.3045476989746094\n",
      "Epoch 758 : test_loss 1.2768667755126952\n",
      "Epoch 759 : train_loss 1.3181351250542535\n",
      "Epoch 759 : test_loss 1.3172404174804688\n",
      "Epoch 760 : train_loss 1.3213175082736546\n",
      "Epoch 760 : test_loss 1.2679223937988282\n",
      "Epoch 761 : train_loss 1.324157704671224\n",
      "Epoch 761 : test_loss 1.3263501586914062\n",
      "Epoch 762 : train_loss 1.319177944607205\n",
      "Epoch 762 : test_loss 1.2982822418212892\n",
      "Epoch 763 : train_loss 1.3189890814887153\n",
      "Epoch 763 : test_loss 1.3222279968261719\n",
      "Epoch 764 : train_loss 1.3067721489800348\n",
      "Epoch 764 : test_loss 1.3264683532714843\n",
      "Epoch 765 : train_loss 1.3190879855685764\n",
      "Epoch 765 : test_loss 1.3313895568847656\n",
      "Epoch 766 : train_loss 1.312316633436415\n",
      "Epoch 766 : test_loss 1.2743434448242188\n",
      "Epoch 767 : train_loss 1.3188607788085938\n",
      "Epoch 767 : test_loss 1.2758204498291015\n",
      "Epoch 768 : train_loss 1.3177651977539062\n",
      "Epoch 768 : test_loss 1.3050338745117187\n",
      "Epoch 769 : train_loss 1.3204176330566406\n",
      "Epoch 769 : test_loss 1.353080841064453\n",
      "Epoch 770 : train_loss 1.2959017232259114\n",
      "Epoch 770 : test_loss 1.3320032653808593\n",
      "Epoch 771 : train_loss 1.3050287543402779\n",
      "Epoch 771 : test_loss 1.2703097839355468\n",
      "Epoch 772 : train_loss 1.3058768717447917\n",
      "Epoch 772 : test_loss 1.310478500366211\n",
      "Epoch 773 : train_loss 1.3099313659667968\n",
      "Epoch 773 : test_loss 1.2785536041259766\n",
      "Epoch 774 : train_loss 1.3137738477918837\n",
      "Epoch 774 : test_loss 1.2761436767578125\n",
      "Epoch 775 : train_loss 1.2859559122721353\n",
      "Epoch 775 : test_loss 1.2850518951416015\n",
      "Epoch 776 : train_loss 1.3147418077256945\n",
      "Epoch 776 : test_loss 1.2840901794433595\n",
      "Epoch 777 : train_loss 1.3064276360405815\n",
      "Epoch 777 : test_loss 1.2766107940673828\n",
      "Epoch 778 : train_loss 1.316337870279948\n",
      "Epoch 778 : test_loss 1.2911706237792968\n",
      "Epoch 779 : train_loss 1.314808383517795\n",
      "Epoch 779 : test_loss 1.3113873138427734\n",
      "Epoch 780 : train_loss 1.3039814317491318\n",
      "Epoch 780 : test_loss 1.2947042541503906\n",
      "Epoch 781 : train_loss 1.3184634738498264\n",
      "Epoch 781 : test_loss 1.2718040008544922\n",
      "Epoch 782 : train_loss 1.3139082573784722\n",
      "Epoch 782 : test_loss 1.3317705841064453\n",
      "Epoch 783 : train_loss 1.3204198811848957\n",
      "Epoch 783 : test_loss 1.2643616943359375\n",
      "Epoch 784 : train_loss 1.3018944295247397\n",
      "Epoch 784 : test_loss 1.2896621246337892\n",
      "Epoch 785 : train_loss 1.3115188123914931\n",
      "Epoch 785 : test_loss 1.2648073120117187\n",
      "Epoch 786 : train_loss 1.3101468336317275\n",
      "Epoch 786 : test_loss 1.3012828674316406\n",
      "Epoch 787 : train_loss 1.3099335361056859\n",
      "Epoch 787 : test_loss 1.2345984802246093\n",
      "Epoch 788 : train_loss 1.304364956325955\n",
      "Epoch 788 : test_loss 1.2925333404541015\n",
      "Epoch 789 : train_loss 1.3047434285481772\n",
      "Epoch 789 : test_loss 1.3018941040039063\n",
      "Epoch 790 : train_loss 1.3053791097005207\n",
      "Epoch 790 : test_loss 1.281007354736328\n",
      "Epoch 791 : train_loss 1.328342017279731\n",
      "Epoch 791 : test_loss 1.311639404296875\n",
      "Epoch 792 : train_loss 1.3155586751302084\n",
      "Epoch 792 : test_loss 1.3160823516845703\n",
      "Epoch 793 : train_loss 1.311971950954861\n",
      "Epoch 793 : test_loss 1.2913175964355468\n",
      "Epoch 794 : train_loss 1.3242765163845487\n",
      "Epoch 794 : test_loss 1.3086025848388672\n",
      "Epoch 795 : train_loss 1.311465318467882\n",
      "Epoch 795 : test_loss 1.2891936645507813\n",
      "Epoch 796 : train_loss 1.303192881266276\n",
      "Epoch 796 : test_loss 1.3215712890625\n",
      "Epoch 797 : train_loss 1.313935292561849\n",
      "Epoch 797 : test_loss 1.2618880615234376\n",
      "Epoch 798 : train_loss 1.3117423163519966\n",
      "Epoch 798 : test_loss 1.2926686248779298\n",
      "Epoch 799 : train_loss 1.3178065151638454\n",
      "Epoch 799 : test_loss 1.2688038482666015\n",
      "Epoch 800 : train_loss 1.3115816006130643\n",
      "Epoch 800 : test_loss 1.2954298400878905\n",
      "Epoch 801 : train_loss 1.3169925876193576\n",
      "Epoch 801 : test_loss 1.3014208374023437\n",
      "Epoch 802 : train_loss 1.3112598164876301\n",
      "Epoch 802 : test_loss 1.27556103515625\n",
      "Epoch 803 : train_loss 1.3140291035970053\n",
      "Epoch 803 : test_loss 1.2886319885253905\n",
      "Epoch 804 : train_loss 1.3161776563856338\n",
      "Epoch 804 : test_loss 1.3040999603271484\n",
      "Epoch 805 : train_loss 1.316145999484592\n",
      "Epoch 805 : test_loss 1.289129638671875\n",
      "Epoch 806 : train_loss 1.3135231492784287\n",
      "Epoch 806 : test_loss 1.3471286010742187\n",
      "Epoch 807 : train_loss 1.3156830613878039\n",
      "Epoch 807 : test_loss 1.2722674407958985\n",
      "Epoch 808 : train_loss 1.311499301486545\n",
      "Epoch 808 : test_loss 1.2986644287109375\n",
      "Epoch 809 : train_loss 1.3192298414442274\n",
      "Epoch 809 : test_loss 1.3085449371337892\n",
      "Epoch 810 : train_loss 1.302037587483724\n",
      "Epoch 810 : test_loss 1.3118444213867186\n",
      "Epoch 811 : train_loss 1.3107787000868056\n",
      "Epoch 811 : test_loss 1.289786331176758\n",
      "Epoch 812 : train_loss 1.3121901652018229\n",
      "Epoch 812 : test_loss 1.2951068115234374\n",
      "Epoch 813 : train_loss 1.326995107014974\n",
      "Epoch 813 : test_loss 1.3140922698974609\n",
      "Epoch 814 : train_loss 1.3137091776529948\n",
      "Epoch 814 : test_loss 1.3481340026855468\n",
      "Epoch 815 : train_loss 1.2978566419813369\n",
      "Epoch 815 : test_loss 1.265770965576172\n",
      "Epoch 816 : train_loss 1.3131769612630209\n",
      "Epoch 816 : test_loss 1.3355386352539063\n",
      "Epoch 817 : train_loss 1.3106170620388455\n",
      "Epoch 817 : test_loss 1.3058576049804687\n",
      "Epoch 818 : train_loss 1.3084017164442274\n",
      "Epoch 818 : test_loss 1.3117360076904296\n",
      "Epoch 819 : train_loss 1.3042931111653646\n",
      "Epoch 819 : test_loss 1.306838165283203\n",
      "Epoch 820 : train_loss 1.3148578626844618\n",
      "Epoch 820 : test_loss 1.3486486206054686\n",
      "Epoch 821 : train_loss 1.3088815511067708\n",
      "Epoch 821 : test_loss 1.2961795806884766\n",
      "Epoch 822 : train_loss 1.2971988118489584\n",
      "Epoch 822 : test_loss 1.3320416870117187\n",
      "Epoch 823 : train_loss 1.3212722778320312\n",
      "Epoch 823 : test_loss 1.289563491821289\n",
      "Epoch 824 : train_loss 1.3030086534288194\n",
      "Epoch 824 : test_loss 1.3068557739257813\n",
      "Epoch 825 : train_loss 1.3149198065863714\n",
      "Epoch 825 : test_loss 1.2649127197265626\n",
      "Epoch 826 : train_loss 1.2987892083062067\n",
      "Epoch 826 : test_loss 1.3183185424804686\n",
      "Epoch 827 : train_loss 1.3028634067111544\n",
      "Epoch 827 : test_loss 1.3014192504882813\n",
      "Epoch 828 : train_loss 1.3173765258789063\n",
      "Epoch 828 : test_loss 1.3198699340820312\n",
      "Epoch 829 : train_loss 1.3218988477918836\n",
      "Epoch 829 : test_loss 1.291996810913086\n",
      "Epoch 830 : train_loss 1.3054500291612414\n",
      "Epoch 830 : test_loss 1.3634699401855468\n",
      "Epoch 831 : train_loss 1.3063287556966146\n",
      "Epoch 831 : test_loss 1.2477227325439453\n",
      "Epoch 832 : train_loss 1.3152651774088542\n",
      "Epoch 832 : test_loss 1.2761560516357422\n",
      "Epoch 833 : train_loss 1.3178633083767362\n",
      "Epoch 833 : test_loss 1.2804232482910156\n",
      "Epoch 834 : train_loss 1.313883307562934\n",
      "Epoch 834 : test_loss 1.2732198791503906\n",
      "Epoch 835 : train_loss 1.32409230211046\n",
      "Epoch 835 : test_loss 1.2962283935546874\n",
      "Epoch 836 : train_loss 1.2943892957899306\n",
      "Epoch 836 : test_loss 1.2548159637451173\n",
      "Epoch 837 : train_loss 1.3126653001573352\n",
      "Epoch 837 : test_loss 1.319038330078125\n",
      "Epoch 838 : train_loss 1.311957994249132\n",
      "Epoch 838 : test_loss 1.3119927825927735\n",
      "Epoch 839 : train_loss 1.3125577121310763\n",
      "Epoch 839 : test_loss 1.3057912902832032\n",
      "Epoch 840 : train_loss 1.3240071987575954\n",
      "Epoch 840 : test_loss 1.2945522766113282\n",
      "Epoch 841 : train_loss 1.2992125651041666\n",
      "Epoch 841 : test_loss 1.2535889434814453\n",
      "Epoch 842 : train_loss 1.3003096686469184\n",
      "Epoch 842 : test_loss 1.2992540435791016\n",
      "Epoch 843 : train_loss 1.302376729329427\n",
      "Epoch 843 : test_loss 1.3027334747314454\n",
      "Epoch 844 : train_loss 1.3179519551595051\n",
      "Epoch 844 : test_loss 1.31849658203125\n",
      "Epoch 845 : train_loss 1.2923756510416666\n",
      "Epoch 845 : test_loss 1.3072056884765626\n",
      "Epoch 846 : train_loss 1.3110440029568142\n",
      "Epoch 846 : test_loss 1.2848062286376953\n",
      "Epoch 847 : train_loss 1.3031639641655817\n",
      "Epoch 847 : test_loss 1.2680233154296876\n",
      "Epoch 848 : train_loss 1.3043035820855036\n",
      "Epoch 848 : test_loss 1.3543219299316407\n",
      "Epoch 849 : train_loss 1.3063304782443577\n",
      "Epoch 849 : test_loss 1.2419552001953125\n",
      "Epoch 850 : train_loss 1.322249254014757\n",
      "Epoch 850 : test_loss 1.3240851745605469\n",
      "Epoch 851 : train_loss 1.3109909023708768\n",
      "Epoch 851 : test_loss 1.3494066925048829\n",
      "Epoch 852 : train_loss 1.3062398579915364\n",
      "Epoch 852 : test_loss 1.3189181518554687\n",
      "Epoch 853 : train_loss 1.3125642395019532\n",
      "Epoch 853 : test_loss 1.300748779296875\n",
      "Epoch 854 : train_loss 1.3285680677625868\n",
      "Epoch 854 : test_loss 1.2907516479492187\n",
      "Epoch 855 : train_loss 1.3045934210883245\n",
      "Epoch 855 : test_loss 1.3025908813476563\n",
      "Epoch 856 : train_loss 1.305116923014323\n",
      "Epoch 856 : test_loss 1.293587661743164\n",
      "Epoch 857 : train_loss 1.3058014424641926\n",
      "Epoch 857 : test_loss 1.2649051666259765\n",
      "Epoch 858 : train_loss 1.291916242811415\n",
      "Epoch 858 : test_loss 1.288633255004883\n",
      "Epoch 859 : train_loss 1.3237558085123697\n",
      "Epoch 859 : test_loss 1.3131724548339845\n",
      "Epoch 860 : train_loss 1.3042728203667535\n",
      "Epoch 860 : test_loss 1.276067398071289\n",
      "Epoch 861 : train_loss 1.302990963406033\n",
      "Epoch 861 : test_loss 1.2919427337646485\n",
      "Epoch 862 : train_loss 1.2943810255262587\n",
      "Epoch 862 : test_loss 1.305198486328125\n",
      "Epoch 863 : train_loss 1.2950127597384982\n",
      "Epoch 863 : test_loss 1.2907942810058595\n",
      "Epoch 864 : train_loss 1.2922525872124566\n",
      "Epoch 864 : test_loss 1.2763263702392578\n",
      "Epoch 865 : train_loss 1.3085453491210937\n",
      "Epoch 865 : test_loss 1.3096778564453124\n",
      "Epoch 866 : train_loss 1.3201280517578124\n",
      "Epoch 866 : test_loss 1.3141235961914062\n",
      "Epoch 867 : train_loss 1.3139284125434028\n",
      "Epoch 867 : test_loss 1.2867206115722656\n",
      "Epoch 868 : train_loss 1.3037169291178385\n",
      "Epoch 868 : test_loss 1.3285047912597656\n",
      "Epoch 869 : train_loss 1.3048615146213107\n",
      "Epoch 869 : test_loss 1.279352264404297\n",
      "Epoch 870 : train_loss 1.3143650682237413\n",
      "Epoch 870 : test_loss 1.2815175476074219\n",
      "Epoch 871 : train_loss 1.3003222249348958\n",
      "Epoch 871 : test_loss 1.2605364685058593\n",
      "Epoch 872 : train_loss 1.3124500901963976\n",
      "Epoch 872 : test_loss 1.3296653747558593\n",
      "Epoch 873 : train_loss 1.2992677035861546\n",
      "Epoch 873 : test_loss 1.279579803466797\n",
      "Epoch 874 : train_loss 1.315967797173394\n",
      "Epoch 874 : test_loss 1.2567803649902343\n",
      "Epoch 875 : train_loss 1.2892279493543837\n",
      "Epoch 875 : test_loss 1.2728499450683595\n",
      "Epoch 876 : train_loss 1.307816426595052\n",
      "Epoch 876 : test_loss 1.2517965545654297\n",
      "Epoch 877 : train_loss 1.305743157280816\n",
      "Epoch 877 : test_loss 1.2686324005126952\n",
      "Epoch 878 : train_loss 1.3105780131022136\n",
      "Epoch 878 : test_loss 1.2837154541015625\n",
      "Epoch 879 : train_loss 1.3053040161132812\n",
      "Epoch 879 : test_loss 1.2448185424804687\n",
      "Epoch 880 : train_loss 1.3115010443793402\n",
      "Epoch 880 : test_loss 1.2948870086669921\n",
      "Epoch 881 : train_loss 1.3068822191026477\n",
      "Epoch 881 : test_loss 1.2708738708496095\n",
      "Epoch 882 : train_loss 1.3064468688964843\n",
      "Epoch 882 : test_loss 1.3240294494628906\n",
      "Epoch 883 : train_loss 1.3162084791395399\n",
      "Epoch 883 : test_loss 1.3358092651367188\n",
      "Epoch 884 : train_loss 1.3029796515570746\n",
      "Epoch 884 : test_loss 1.3273620147705079\n",
      "Epoch 885 : train_loss 1.3002896253797742\n",
      "Epoch 885 : test_loss 1.282582305908203\n",
      "Epoch 886 : train_loss 1.3012663302951388\n",
      "Epoch 886 : test_loss 1.2897339477539063\n",
      "Epoch 887 : train_loss 1.3069656948513455\n",
      "Epoch 887 : test_loss 1.2921836547851562\n",
      "Epoch 888 : train_loss 1.3205663791232638\n",
      "Epoch 888 : test_loss 1.2820528106689453\n",
      "Epoch 889 : train_loss 1.3070175882975261\n",
      "Epoch 889 : test_loss 1.2986275329589845\n",
      "Epoch 890 : train_loss 1.300413048638238\n",
      "Epoch 890 : test_loss 1.261023468017578\n",
      "Epoch 891 : train_loss 1.3186034478081596\n",
      "Epoch 891 : test_loss 1.2873447113037109\n",
      "Epoch 892 : train_loss 1.2939647386338975\n",
      "Epoch 892 : test_loss 1.2876034851074218\n",
      "Epoch 893 : train_loss 1.3132168952094183\n",
      "Epoch 893 : test_loss 1.2936805114746093\n",
      "Epoch 894 : train_loss 1.2997918735080296\n",
      "Epoch 894 : test_loss 1.2962745361328125\n",
      "Epoch 895 : train_loss 1.2995452609592013\n",
      "Epoch 895 : test_loss 1.243722640991211\n",
      "Epoch 896 : train_loss 1.3273870273166233\n",
      "Epoch 896 : test_loss 1.2988201599121094\n",
      "Epoch 897 : train_loss 1.2911646118164062\n",
      "Epoch 897 : test_loss 1.296083023071289\n",
      "Epoch 898 : train_loss 1.3075582546657987\n",
      "Epoch 898 : test_loss 1.263570327758789\n",
      "Epoch 899 : train_loss 1.3179508599175347\n",
      "Epoch 899 : test_loss 1.2546221008300782\n",
      "Epoch 900 : train_loss 1.3030661790635851\n",
      "Epoch 900 : test_loss 1.2588554992675782\n",
      "Epoch 901 : train_loss 1.3100970357259114\n",
      "Epoch 901 : test_loss 1.3152445678710938\n",
      "Epoch 902 : train_loss 1.3113751051161024\n",
      "Epoch 902 : test_loss 1.2777984771728517\n",
      "Epoch 903 : train_loss 1.300616953531901\n",
      "Epoch 903 : test_loss 1.3371965026855468\n",
      "Epoch 904 : train_loss 1.3000490044487847\n",
      "Epoch 904 : test_loss 1.3298662872314453\n",
      "Epoch 905 : train_loss 1.3084718661838108\n",
      "Epoch 905 : test_loss 1.2691939392089844\n",
      "Epoch 906 : train_loss 1.3074539252387152\n",
      "Epoch 906 : test_loss 1.265975601196289\n",
      "Epoch 907 : train_loss 1.3129739244249132\n",
      "Epoch 907 : test_loss 1.280485107421875\n",
      "Epoch 908 : train_loss 1.2995839504665798\n",
      "Epoch 908 : test_loss 1.30966552734375\n",
      "Epoch 909 : train_loss 1.3207723761664496\n",
      "Epoch 909 : test_loss 1.2848111572265626\n",
      "Epoch 910 : train_loss 1.3101340535481771\n",
      "Epoch 910 : test_loss 1.2756830291748047\n",
      "Epoch 911 : train_loss 1.312755618625217\n",
      "Epoch 911 : test_loss 1.256020477294922\n",
      "Epoch 912 : train_loss 1.2871819898817274\n",
      "Epoch 912 : test_loss 1.2949886474609376\n",
      "Epoch 913 : train_loss 1.326338656955295\n",
      "Epoch 913 : test_loss 1.297318405151367\n",
      "Epoch 914 : train_loss 1.3177345106336806\n",
      "Epoch 914 : test_loss 1.287171142578125\n",
      "Epoch 915 : train_loss 1.3170526936848959\n",
      "Epoch 915 : test_loss 1.2831786499023436\n",
      "Epoch 916 : train_loss 1.3188992377387152\n",
      "Epoch 916 : test_loss 1.3061063232421875\n",
      "Epoch 917 : train_loss 1.304019765218099\n",
      "Epoch 917 : test_loss 1.254868896484375\n",
      "Epoch 918 : train_loss 1.2926344943576389\n",
      "Epoch 918 : test_loss 1.3317144470214843\n",
      "Epoch 919 : train_loss 1.3059844258626303\n",
      "Epoch 919 : test_loss 1.2722161865234376\n",
      "Epoch 920 : train_loss 1.3095795457628039\n",
      "Epoch 920 : test_loss 1.2850322875976563\n",
      "Epoch 921 : train_loss 1.3026097920735678\n",
      "Epoch 921 : test_loss 1.3023246002197266\n",
      "Epoch 922 : train_loss 1.3127108222113715\n",
      "Epoch 922 : test_loss 1.2592936859130859\n",
      "Epoch 923 : train_loss 1.2774130011664497\n",
      "Epoch 923 : test_loss 1.2639207000732422\n",
      "Epoch 924 : train_loss 1.307577175564236\n",
      "Epoch 924 : test_loss 1.283126953125\n",
      "Epoch 925 : train_loss 1.2890630560980902\n",
      "Epoch 925 : test_loss 1.2960401000976562\n",
      "Epoch 926 : train_loss 1.3112503085666232\n",
      "Epoch 926 : test_loss 1.2674324798583985\n",
      "Epoch 927 : train_loss 1.307245357937283\n",
      "Epoch 927 : test_loss 1.3260118713378906\n",
      "Epoch 928 : train_loss 1.3180763854980468\n",
      "Epoch 928 : test_loss 1.318316177368164\n",
      "Epoch 929 : train_loss 1.3069109293619792\n",
      "Epoch 929 : test_loss 1.2860436706542968\n",
      "Epoch 930 : train_loss 1.3038187459309896\n",
      "Epoch 930 : test_loss 1.2691154479980469\n",
      "Epoch 931 : train_loss 1.305668199327257\n",
      "Epoch 931 : test_loss 1.2898984985351563\n",
      "Epoch 932 : train_loss 1.303616963704427\n",
      "Epoch 932 : test_loss 1.2813162841796875\n",
      "Epoch 933 : train_loss 1.3125730963812934\n",
      "Epoch 933 : test_loss 1.3231729888916015\n",
      "Epoch 934 : train_loss 1.3139160766601563\n",
      "Epoch 934 : test_loss 1.2761328430175782\n",
      "Epoch 935 : train_loss 1.3106978827582465\n",
      "Epoch 935 : test_loss 1.30927392578125\n",
      "Epoch 936 : train_loss 1.3060652737087675\n",
      "Epoch 936 : test_loss 1.3196889038085937\n",
      "Epoch 937 : train_loss 1.3050961439344617\n",
      "Epoch 937 : test_loss 1.2873327178955078\n",
      "Epoch 938 : train_loss 1.3015687934027778\n",
      "Epoch 938 : test_loss 1.2861565704345703\n",
      "Epoch 939 : train_loss 1.3108931138780382\n",
      "Epoch 939 : test_loss 1.2863173370361327\n",
      "Epoch 940 : train_loss 1.3223948330349393\n",
      "Epoch 940 : test_loss 1.2634821014404296\n",
      "Epoch 941 : train_loss 1.3106509365505643\n",
      "Epoch 941 : test_loss 1.2862543029785156\n",
      "Epoch 942 : train_loss 1.3157622714572483\n",
      "Epoch 942 : test_loss 1.3099869689941406\n",
      "Epoch 943 : train_loss 1.3048325771755642\n",
      "Epoch 943 : test_loss 1.2809360961914062\n",
      "Epoch 944 : train_loss 1.3023632032606336\n",
      "Epoch 944 : test_loss 1.3019603881835937\n",
      "Epoch 945 : train_loss 1.3089385579427084\n",
      "Epoch 945 : test_loss 1.2699246826171875\n",
      "Epoch 946 : train_loss 1.3037614101833768\n",
      "Epoch 946 : test_loss 1.2982627563476563\n",
      "Epoch 947 : train_loss 1.312127427842882\n",
      "Epoch 947 : test_loss 1.301946762084961\n",
      "Epoch 948 : train_loss 1.2944940558539497\n",
      "Epoch 948 : test_loss 1.2765471801757813\n",
      "Epoch 949 : train_loss 1.3212110154893664\n",
      "Epoch 949 : test_loss 1.277140594482422\n",
      "Epoch 950 : train_loss 1.3018289659288194\n",
      "Epoch 950 : test_loss 1.2800196533203125\n",
      "Epoch 951 : train_loss 1.310774159749349\n",
      "Epoch 951 : test_loss 1.298582275390625\n",
      "Epoch 952 : train_loss 1.3084275817871094\n",
      "Epoch 952 : test_loss 1.255661361694336\n",
      "Epoch 953 : train_loss 1.3051930779351129\n",
      "Epoch 953 : test_loss 1.2705716552734374\n",
      "Epoch 954 : train_loss 1.3019662441677518\n",
      "Epoch 954 : test_loss 1.3034843292236329\n",
      "Epoch 955 : train_loss 1.3014825236002605\n",
      "Epoch 955 : test_loss 1.316072998046875\n",
      "Epoch 956 : train_loss 1.3051149156358508\n",
      "Epoch 956 : test_loss 1.2807278594970704\n",
      "Epoch 957 : train_loss 1.31730076429579\n",
      "Epoch 957 : test_loss 1.291465118408203\n",
      "Epoch 958 : train_loss 1.312114267985026\n",
      "Epoch 958 : test_loss 1.296832275390625\n",
      "Epoch 959 : train_loss 1.3068588019476997\n",
      "Epoch 959 : test_loss 1.3095846252441405\n",
      "Epoch 960 : train_loss 1.304193572998047\n",
      "Epoch 960 : test_loss 1.3116788330078124\n",
      "Epoch 961 : train_loss 1.2981712849934897\n",
      "Epoch 961 : test_loss 1.2739203186035157\n",
      "Epoch 962 : train_loss 1.3095704481336805\n",
      "Epoch 962 : test_loss 1.3058773498535157\n",
      "Epoch 963 : train_loss 1.3068104485405816\n",
      "Epoch 963 : test_loss 1.2720789184570314\n",
      "Epoch 964 : train_loss 1.295605719672309\n",
      "Epoch 964 : test_loss 1.3055408630371095\n",
      "Epoch 965 : train_loss 1.3058597513834636\n",
      "Epoch 965 : test_loss 1.3035076904296874\n",
      "Epoch 966 : train_loss 1.3178574930826823\n",
      "Epoch 966 : test_loss 1.2651788024902344\n",
      "Epoch 967 : train_loss 1.2946030951605902\n",
      "Epoch 967 : test_loss 1.3239759063720704\n",
      "Epoch 968 : train_loss 1.3168128492567275\n",
      "Epoch 968 : test_loss 1.2792967987060546\n",
      "Epoch 969 : train_loss 1.3112093166775174\n",
      "Epoch 969 : test_loss 1.270606704711914\n",
      "Epoch 970 : train_loss 1.2994197353786892\n",
      "Epoch 970 : test_loss 1.3133921508789062\n",
      "Epoch 971 : train_loss 1.3201363525390626\n",
      "Epoch 971 : test_loss 1.297075164794922\n",
      "Epoch 972 : train_loss 1.3170470479329428\n",
      "Epoch 972 : test_loss 1.321992660522461\n",
      "Epoch 973 : train_loss 1.3035533243815105\n",
      "Epoch 973 : test_loss 1.2715558624267578\n",
      "Epoch 974 : train_loss 1.3090104064941406\n",
      "Epoch 974 : test_loss 1.2648427276611327\n",
      "Epoch 975 : train_loss 1.3033422275119357\n",
      "Epoch 975 : test_loss 1.2822791290283204\n",
      "Epoch 976 : train_loss 1.304567867702908\n",
      "Epoch 976 : test_loss 1.2760288391113281\n",
      "Epoch 977 : train_loss 1.305813730875651\n",
      "Epoch 977 : test_loss 1.283833023071289\n",
      "Epoch 978 : train_loss 1.3082107849121094\n",
      "Epoch 978 : test_loss 1.253303939819336\n",
      "Epoch 979 : train_loss 1.313796132405599\n",
      "Epoch 979 : test_loss 1.2859857788085938\n",
      "Epoch 980 : train_loss 1.3076520148383246\n",
      "Epoch 980 : test_loss 1.3172019958496093\n",
      "Epoch 981 : train_loss 1.3040940585666232\n",
      "Epoch 981 : test_loss 1.3235885009765624\n",
      "Epoch 982 : train_loss 1.308928219265408\n",
      "Epoch 982 : test_loss 1.2591766204833985\n",
      "Epoch 983 : train_loss 1.2968336859809029\n",
      "Epoch 983 : test_loss 1.2618846740722656\n",
      "Epoch 984 : train_loss 1.3023956637912326\n",
      "Epoch 984 : test_loss 1.315635025024414\n",
      "Epoch 985 : train_loss 1.2871946512858072\n",
      "Epoch 985 : test_loss 1.3171330413818358\n",
      "Epoch 986 : train_loss 1.301250956217448\n",
      "Epoch 986 : test_loss 1.2849771118164062\n",
      "Epoch 987 : train_loss 1.3045646260579427\n",
      "Epoch 987 : test_loss 1.3135325927734376\n",
      "Epoch 988 : train_loss 1.2948119201660155\n",
      "Epoch 988 : test_loss 1.3085015869140626\n",
      "Epoch 989 : train_loss 1.2976209954155815\n",
      "Epoch 989 : test_loss 1.2890862731933594\n",
      "Epoch 990 : train_loss 1.3082448187934028\n",
      "Epoch 990 : test_loss 1.2670326690673828\n",
      "Epoch 991 : train_loss 1.3045809461805555\n",
      "Epoch 991 : test_loss 1.282609619140625\n",
      "Epoch 992 : train_loss 1.3060001085069444\n",
      "Epoch 992 : test_loss 1.2650392303466798\n",
      "Epoch 993 : train_loss 1.2942729085286457\n",
      "Epoch 993 : test_loss 1.2988904876708984\n",
      "Epoch 994 : train_loss 1.2936178487141927\n",
      "Epoch 994 : test_loss 1.3091732025146485\n",
      "Epoch 995 : train_loss 1.3073051283094619\n",
      "Epoch 995 : test_loss 1.3160565795898438\n",
      "Epoch 996 : train_loss 1.3152029418945312\n",
      "Epoch 996 : test_loss 1.305173553466797\n",
      "Epoch 997 : train_loss 1.3018407355414496\n",
      "Epoch 997 : test_loss 1.3020338897705077\n",
      "Epoch 998 : train_loss 1.2981431138780382\n",
      "Epoch 998 : test_loss 1.275186965942383\n",
      "Epoch 999 : train_loss 1.3108763563368055\n",
      "Epoch 999 : test_loss 1.2950938262939453\n"
     ]
    }
   ],
   "source": [
    "# Now create a dataloader and train the flows\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "model = my_model()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=1e-5, amsgrad=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer,'min')\n",
    "\n",
    "lr = []\n",
    "losses = []\n",
    "\n",
    "\n",
    "n_train = 9000\n",
    "n_test = N - n_train\n",
    "data_train = TensorDataset(x[:n_train], y[:n_train])\n",
    "data_test = TensorDataset(x[n_train:], y[n_train:])\n",
    "train_loader = DataLoader(data_train, 300)\n",
    "test_loader = DataLoader(data_test, 300, shuffle=False)\n",
    "\n",
    "best_test_loss = np.inf\n",
    "for epoch in range(1000):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for data_ in train_loader:\n",
    "        data_ = [d.cuda() for d in data_]\n",
    "        loss,dict = model.compute_kld(data_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    for data_ in test_loader:\n",
    "        data_ = [d.cuda() for d in data_]\n",
    "        with torch.no_grad():\n",
    "            loss,dict = model.compute_kld(data_)\n",
    "            test_loss += loss.item()\n",
    "    if test_loss < best_test_loss:\n",
    "        print('saved model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        best_test_loss = test_loss  \n",
    "    scheduler.step(test_loss)\n",
    "    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    losses.append(test_loss)\n",
    "    print(f'Epoch {epoch} : train_loss {train_loss/len(data_train)}')\n",
    "    print(f'Epoch {epoch} : test_loss {test_loss/len(data_test)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model\n",
      "Epoch 0 : train_loss 6.962843451605902\n",
      "Epoch 0 : test_loss 6.830992065429688\n",
      "saved model\n",
      "Epoch 1 : train_loss 6.791622884114584\n",
      "Epoch 1 : test_loss 6.6606904296875\n",
      "saved model\n",
      "Epoch 2 : train_loss 6.611930962456597\n",
      "Epoch 2 : test_loss 6.506016357421875\n",
      "saved model\n",
      "Epoch 3 : train_loss 6.463098931206598\n",
      "Epoch 3 : test_loss 6.3498558349609375\n",
      "saved model\n",
      "Epoch 4 : train_loss 6.321701917860243\n",
      "Epoch 4 : test_loss 6.228921264648437\n",
      "saved model\n",
      "Epoch 5 : train_loss 6.21149016655816\n",
      "Epoch 5 : test_loss 6.145284484863281\n",
      "saved model\n",
      "Epoch 6 : train_loss 6.126471245659722\n",
      "Epoch 6 : test_loss 6.041126037597656\n",
      "saved model\n",
      "Epoch 7 : train_loss 6.0501984320746525\n",
      "Epoch 7 : test_loss 5.990611267089844\n",
      "saved model\n",
      "Epoch 8 : train_loss 5.996305460611979\n",
      "Epoch 8 : test_loss 5.951221862792969\n",
      "saved model\n",
      "Epoch 9 : train_loss 5.967364759657118\n",
      "Epoch 9 : test_loss 5.92538916015625\n",
      "saved model\n",
      "Epoch 10 : train_loss 5.936485039605035\n",
      "Epoch 10 : test_loss 5.887101135253906\n",
      "saved model\n",
      "Epoch 11 : train_loss 5.919546034071181\n",
      "Epoch 11 : test_loss 5.860193176269531\n",
      "saved model\n",
      "Epoch 12 : train_loss 5.899316528320313\n",
      "Epoch 12 : test_loss 5.854329833984375\n",
      "saved model\n",
      "Epoch 13 : train_loss 5.887372395833333\n",
      "Epoch 13 : test_loss 5.836351989746094\n",
      "saved model\n",
      "Epoch 14 : train_loss 5.873372504340278\n",
      "Epoch 14 : test_loss 5.810130310058594\n",
      "Epoch 15 : train_loss 5.855400675455729\n",
      "Epoch 15 : test_loss 5.8120730590820315\n",
      "saved model\n",
      "Epoch 16 : train_loss 5.831895521375868\n",
      "Epoch 16 : test_loss 5.7653720092773435\n",
      "saved model\n",
      "Epoch 17 : train_loss 5.808888319227431\n",
      "Epoch 17 : test_loss 5.759743041992188\n",
      "saved model\n",
      "Epoch 18 : train_loss 5.7832211507161455\n",
      "Epoch 18 : test_loss 5.73024951171875\n",
      "saved model\n",
      "Epoch 19 : train_loss 5.759069742838542\n",
      "Epoch 19 : test_loss 5.699439697265625\n",
      "saved model\n",
      "Epoch 20 : train_loss 5.717593221028646\n",
      "Epoch 20 : test_loss 5.6682392578125\n",
      "saved model\n",
      "Epoch 21 : train_loss 5.693960272894965\n",
      "Epoch 21 : test_loss 5.6356640625\n",
      "saved model\n",
      "Epoch 22 : train_loss 5.658587415907118\n",
      "Epoch 22 : test_loss 5.608702331542969\n",
      "saved model\n",
      "Epoch 23 : train_loss 5.620909559461806\n",
      "Epoch 23 : test_loss 5.555483764648438\n",
      "saved model\n",
      "Epoch 24 : train_loss 5.5888427734375\n",
      "Epoch 24 : test_loss 5.528430541992187\n",
      "saved model\n",
      "Epoch 25 : train_loss 5.553322998046875\n",
      "Epoch 25 : test_loss 5.493742919921875\n",
      "saved model\n",
      "Epoch 26 : train_loss 5.521428276909722\n",
      "Epoch 26 : test_loss 5.469413818359375\n",
      "saved model\n",
      "Epoch 27 : train_loss 5.482338636610243\n",
      "Epoch 27 : test_loss 5.429801147460937\n",
      "saved model\n",
      "Epoch 28 : train_loss 5.450626925998264\n",
      "Epoch 28 : test_loss 5.409153442382813\n",
      "saved model\n",
      "Epoch 29 : train_loss 5.419139553493924\n",
      "Epoch 29 : test_loss 5.35615380859375\n",
      "saved model\n",
      "Epoch 30 : train_loss 5.38836168077257\n",
      "Epoch 30 : test_loss 5.339345581054688\n",
      "saved model\n",
      "Epoch 31 : train_loss 5.360910101996528\n",
      "Epoch 31 : test_loss 5.326525451660157\n",
      "saved model\n",
      "Epoch 32 : train_loss 5.3343365342881945\n",
      "Epoch 32 : test_loss 5.281244506835938\n",
      "saved model\n",
      "Epoch 33 : train_loss 5.305917249891493\n",
      "Epoch 33 : test_loss 5.241522705078125\n",
      "saved model\n",
      "Epoch 34 : train_loss 5.274644829644097\n",
      "Epoch 34 : test_loss 5.234727111816406\n",
      "saved model\n",
      "Epoch 35 : train_loss 5.248678344726563\n",
      "Epoch 35 : test_loss 5.202351379394531\n",
      "saved model\n",
      "Epoch 36 : train_loss 5.222060397677952\n",
      "Epoch 36 : test_loss 5.170845520019531\n",
      "saved model\n",
      "Epoch 37 : train_loss 5.201408298068577\n",
      "Epoch 37 : test_loss 5.163538818359375\n",
      "saved model\n",
      "Epoch 38 : train_loss 5.178807169596354\n",
      "Epoch 38 : test_loss 5.127799743652344\n",
      "saved model\n",
      "Epoch 39 : train_loss 5.1491304660373265\n",
      "Epoch 39 : test_loss 5.0957511596679685\n",
      "saved model\n",
      "Epoch 40 : train_loss 5.129150105794271\n",
      "Epoch 40 : test_loss 5.087947143554688\n",
      "saved model\n",
      "Epoch 41 : train_loss 5.102618855794271\n",
      "Epoch 41 : test_loss 5.054193969726563\n",
      "saved model\n",
      "Epoch 42 : train_loss 5.086833062065972\n",
      "Epoch 42 : test_loss 5.028689575195313\n",
      "saved model\n",
      "Epoch 43 : train_loss 5.0568512641059025\n",
      "Epoch 43 : test_loss 5.022043334960937\n",
      "saved model\n",
      "Epoch 44 : train_loss 5.042074340820313\n",
      "Epoch 44 : test_loss 4.975584533691406\n",
      "saved model\n",
      "Epoch 45 : train_loss 5.020599324544271\n",
      "Epoch 45 : test_loss 4.97422314453125\n",
      "saved model\n",
      "Epoch 46 : train_loss 4.999731580946181\n",
      "Epoch 46 : test_loss 4.934752502441406\n",
      "saved model\n",
      "Epoch 47 : train_loss 4.978871595594618\n",
      "Epoch 47 : test_loss 4.920059265136719\n",
      "saved model\n",
      "Epoch 48 : train_loss 4.954181423611111\n",
      "Epoch 48 : test_loss 4.910774780273438\n",
      "saved model\n",
      "Epoch 49 : train_loss 4.938585856119792\n",
      "Epoch 49 : test_loss 4.892331420898437\n",
      "saved model\n",
      "Epoch 50 : train_loss 4.914813598632812\n",
      "Epoch 50 : test_loss 4.852761352539063\n",
      "Epoch 51 : train_loss 4.897272935655382\n",
      "Epoch 51 : test_loss 4.8564755249023435\n",
      "saved model\n",
      "Epoch 52 : train_loss 4.872833414713542\n",
      "Epoch 52 : test_loss 4.817047485351562\n",
      "saved model\n",
      "Epoch 53 : train_loss 4.854080268012153\n",
      "Epoch 53 : test_loss 4.792959808349609\n",
      "Epoch 54 : train_loss 4.83756305609809\n",
      "Epoch 54 : test_loss 4.804141357421875\n",
      "saved model\n",
      "Epoch 55 : train_loss 4.81877057562934\n",
      "Epoch 55 : test_loss 4.760329315185547\n",
      "saved model\n",
      "Epoch 56 : train_loss 4.797111206054687\n",
      "Epoch 56 : test_loss 4.7265146484375\n",
      "saved model\n",
      "Epoch 57 : train_loss 4.776022827148437\n",
      "Epoch 57 : test_loss 4.713508911132813\n",
      "saved model\n",
      "Epoch 58 : train_loss 4.7560013970269095\n",
      "Epoch 58 : test_loss 4.677101501464843\n",
      "Epoch 59 : train_loss 4.733017903645833\n",
      "Epoch 59 : test_loss 4.685810729980469\n",
      "saved model\n",
      "Epoch 60 : train_loss 4.712786159939236\n",
      "Epoch 60 : test_loss 4.660309692382812\n",
      "saved model\n",
      "Epoch 61 : train_loss 4.683804063585069\n",
      "Epoch 61 : test_loss 4.631375579833985\n",
      "saved model\n",
      "Epoch 62 : train_loss 4.670014458550347\n",
      "Epoch 62 : test_loss 4.6241739501953125\n",
      "saved model\n",
      "Epoch 63 : train_loss 4.652727376302083\n",
      "Epoch 63 : test_loss 4.6054969482421875\n",
      "saved model\n",
      "Epoch 64 : train_loss 4.629263088650173\n",
      "Epoch 64 : test_loss 4.59204931640625\n",
      "saved model\n",
      "Epoch 65 : train_loss 4.6129542507595485\n",
      "Epoch 65 : test_loss 4.56302474975586\n",
      "saved model\n",
      "Epoch 66 : train_loss 4.5939432508680556\n",
      "Epoch 66 : test_loss 4.537766906738281\n",
      "saved model\n",
      "Epoch 67 : train_loss 4.571530571831597\n",
      "Epoch 67 : test_loss 4.531258972167969\n",
      "saved model\n",
      "Epoch 68 : train_loss 4.554575520833334\n",
      "Epoch 68 : test_loss 4.487057434082031\n",
      "Epoch 69 : train_loss 4.531983452690972\n",
      "Epoch 69 : test_loss 4.490962768554687\n",
      "saved model\n",
      "Epoch 70 : train_loss 4.511565443250868\n",
      "Epoch 70 : test_loss 4.466605590820312\n",
      "saved model\n",
      "Epoch 71 : train_loss 4.489916558159722\n",
      "Epoch 71 : test_loss 4.4521441040039065\n",
      "saved model\n",
      "Epoch 72 : train_loss 4.4748792453342014\n",
      "Epoch 72 : test_loss 4.438979736328125\n",
      "saved model\n",
      "Epoch 73 : train_loss 4.4528086615668405\n",
      "Epoch 73 : test_loss 4.3835947265625\n",
      "Epoch 74 : train_loss 4.439323391384549\n",
      "Epoch 74 : test_loss 4.386327911376953\n",
      "saved model\n",
      "Epoch 75 : train_loss 4.4079137505425345\n",
      "Epoch 75 : test_loss 4.372431701660156\n",
      "saved model\n",
      "Epoch 76 : train_loss 4.394118082682292\n",
      "Epoch 76 : test_loss 4.343607543945312\n",
      "saved model\n",
      "Epoch 77 : train_loss 4.375283203125\n",
      "Epoch 77 : test_loss 4.331488464355469\n",
      "saved model\n",
      "Epoch 78 : train_loss 4.358471530490451\n",
      "Epoch 78 : test_loss 4.31808837890625\n",
      "saved model\n",
      "Epoch 79 : train_loss 4.338604248046875\n",
      "Epoch 79 : test_loss 4.293860778808594\n",
      "saved model\n",
      "Epoch 80 : train_loss 4.319032335069444\n",
      "Epoch 80 : test_loss 4.271803314208984\n",
      "saved model\n",
      "Epoch 81 : train_loss 4.299650838216146\n",
      "Epoch 81 : test_loss 4.263816284179687\n",
      "saved model\n",
      "Epoch 82 : train_loss 4.280524820963541\n",
      "Epoch 82 : test_loss 4.222108337402344\n",
      "Epoch 83 : train_loss 4.257025227864584\n",
      "Epoch 83 : test_loss 4.222325439453125\n",
      "saved model\n",
      "Epoch 84 : train_loss 4.229832817925347\n",
      "Epoch 84 : test_loss 4.200834411621094\n",
      "saved model\n",
      "Epoch 85 : train_loss 4.216466430664062\n",
      "Epoch 85 : test_loss 4.171488342285156\n",
      "saved model\n",
      "Epoch 86 : train_loss 4.19564885796441\n",
      "Epoch 86 : test_loss 4.14827978515625\n",
      "saved model\n",
      "Epoch 87 : train_loss 4.178304050021701\n",
      "Epoch 87 : test_loss 4.142461975097656\n",
      "saved model\n",
      "Epoch 88 : train_loss 4.156284749348958\n",
      "Epoch 88 : test_loss 4.1319242858886716\n",
      "saved model\n",
      "Epoch 89 : train_loss 4.134564860026042\n",
      "Epoch 89 : test_loss 4.089372253417968\n",
      "saved model\n",
      "Epoch 90 : train_loss 4.113471801757813\n",
      "Epoch 90 : test_loss 4.081752655029297\n",
      "saved model\n",
      "Epoch 91 : train_loss 4.09180419921875\n",
      "Epoch 91 : test_loss 4.052496551513672\n",
      "saved model\n",
      "Epoch 92 : train_loss 4.075579535590278\n",
      "Epoch 92 : test_loss 4.039272521972657\n",
      "saved model\n",
      "Epoch 93 : train_loss 4.051066297743056\n",
      "Epoch 93 : test_loss 3.995702392578125\n",
      "Epoch 94 : train_loss 4.0367131618923615\n",
      "Epoch 94 : test_loss 3.99660302734375\n",
      "saved model\n",
      "Epoch 95 : train_loss 4.015146172417535\n",
      "Epoch 95 : test_loss 3.9740339050292968\n",
      "saved model\n",
      "Epoch 96 : train_loss 3.991103108723958\n",
      "Epoch 96 : test_loss 3.9491572265625\n",
      "saved model\n",
      "Epoch 97 : train_loss 3.9722901882595485\n",
      "Epoch 97 : test_loss 3.9424554138183594\n",
      "saved model\n",
      "Epoch 98 : train_loss 3.9539974229600694\n",
      "Epoch 98 : test_loss 3.9163240966796873\n",
      "saved model\n",
      "Epoch 99 : train_loss 3.9346048719618056\n",
      "Epoch 99 : test_loss 3.899346984863281\n",
      "saved model\n",
      "Epoch 100 : train_loss 3.9169605848524305\n",
      "Epoch 100 : test_loss 3.862615234375\n",
      "saved model\n",
      "Epoch 101 : train_loss 3.888676445855035\n",
      "Epoch 101 : test_loss 3.845377380371094\n",
      "saved model\n",
      "Epoch 102 : train_loss 3.8771333279079863\n",
      "Epoch 102 : test_loss 3.8326835327148436\n",
      "saved model\n",
      "Epoch 103 : train_loss 3.856186726888021\n",
      "Epoch 103 : test_loss 3.820976501464844\n",
      "saved model\n",
      "Epoch 104 : train_loss 3.8365052625868055\n",
      "Epoch 104 : test_loss 3.790930450439453\n",
      "saved model\n",
      "Epoch 105 : train_loss 3.816613972981771\n",
      "Epoch 105 : test_loss 3.7896184997558593\n",
      "saved model\n",
      "Epoch 106 : train_loss 3.798656032986111\n",
      "Epoch 106 : test_loss 3.7593074340820314\n",
      "saved model\n",
      "Epoch 107 : train_loss 3.7824576009114583\n",
      "Epoch 107 : test_loss 3.7482584228515625\n",
      "saved model\n",
      "Epoch 108 : train_loss 3.7608076985677084\n",
      "Epoch 108 : test_loss 3.721783630371094\n",
      "saved model\n",
      "Epoch 109 : train_loss 3.7380924207899304\n",
      "Epoch 109 : test_loss 3.7128181762695314\n",
      "saved model\n",
      "Epoch 110 : train_loss 3.7260272759331596\n",
      "Epoch 110 : test_loss 3.6778641357421873\n",
      "saved model\n",
      "Epoch 111 : train_loss 3.7006317409939236\n",
      "Epoch 111 : test_loss 3.673847961425781\n",
      "saved model\n",
      "Epoch 112 : train_loss 3.685844482421875\n",
      "Epoch 112 : test_loss 3.632650726318359\n",
      "saved model\n",
      "Epoch 113 : train_loss 3.6704947645399306\n",
      "Epoch 113 : test_loss 3.623637939453125\n",
      "saved model\n",
      "Epoch 114 : train_loss 3.6472408040364583\n",
      "Epoch 114 : test_loss 3.5991749572753906\n",
      "saved model\n",
      "Epoch 115 : train_loss 3.631375257703993\n",
      "Epoch 115 : test_loss 3.575924987792969\n",
      "saved model\n",
      "Epoch 116 : train_loss 3.60780224609375\n",
      "Epoch 116 : test_loss 3.5682894287109375\n",
      "saved model\n",
      "Epoch 117 : train_loss 3.5916828884548613\n",
      "Epoch 117 : test_loss 3.538868347167969\n",
      "Epoch 118 : train_loss 3.565729200575087\n",
      "Epoch 118 : test_loss 3.544878723144531\n",
      "saved model\n",
      "Epoch 119 : train_loss 3.550269015842014\n",
      "Epoch 119 : test_loss 3.507219085693359\n",
      "saved model\n",
      "Epoch 120 : train_loss 3.5331947292751735\n",
      "Epoch 120 : test_loss 3.497253723144531\n",
      "saved model\n",
      "Epoch 121 : train_loss 3.514469448513455\n",
      "Epoch 121 : test_loss 3.467075927734375\n",
      "saved model\n",
      "Epoch 122 : train_loss 3.493408901638455\n",
      "Epoch 122 : test_loss 3.444172119140625\n",
      "saved model\n",
      "Epoch 123 : train_loss 3.468913784450955\n",
      "Epoch 123 : test_loss 3.443338653564453\n",
      "saved model\n",
      "Epoch 124 : train_loss 3.4504013671875\n",
      "Epoch 124 : test_loss 3.4073744812011717\n",
      "saved model\n",
      "Epoch 125 : train_loss 3.4315854424370658\n",
      "Epoch 125 : test_loss 3.3804837646484374\n",
      "saved model\n",
      "Epoch 126 : train_loss 3.4158455878363716\n",
      "Epoch 126 : test_loss 3.376550354003906\n",
      "saved model\n",
      "Epoch 127 : train_loss 3.391842753092448\n",
      "Epoch 127 : test_loss 3.3623936767578124\n",
      "saved model\n",
      "Epoch 128 : train_loss 3.3715743882921005\n",
      "Epoch 128 : test_loss 3.34539404296875\n",
      "saved model\n",
      "Epoch 129 : train_loss 3.3537286716037324\n",
      "Epoch 129 : test_loss 3.3137600402832033\n",
      "saved model\n",
      "Epoch 130 : train_loss 3.3364537692599825\n",
      "Epoch 130 : test_loss 3.3009757690429686\n",
      "saved model\n",
      "Epoch 131 : train_loss 3.3154047241210938\n",
      "Epoch 131 : test_loss 3.2825552368164064\n",
      "saved model\n",
      "Epoch 132 : train_loss 3.3011602851019965\n",
      "Epoch 132 : test_loss 3.243332489013672\n",
      "saved model\n",
      "Epoch 133 : train_loss 3.281098158094618\n",
      "Epoch 133 : test_loss 3.239656982421875\n",
      "saved model\n",
      "Epoch 134 : train_loss 3.256467732747396\n",
      "Epoch 134 : test_loss 3.211735290527344\n",
      "Epoch 135 : train_loss 3.235654439290365\n",
      "Epoch 135 : test_loss 3.2140662536621094\n",
      "saved model\n",
      "Epoch 136 : train_loss 3.220650187174479\n",
      "Epoch 136 : test_loss 3.1806593322753907\n",
      "saved model\n",
      "Epoch 137 : train_loss 3.193294250488281\n",
      "Epoch 137 : test_loss 3.164268768310547\n",
      "saved model\n",
      "Epoch 138 : train_loss 3.171140448676215\n",
      "Epoch 138 : test_loss 3.1551043395996095\n",
      "saved model\n",
      "Epoch 139 : train_loss 3.158549309624566\n",
      "Epoch 139 : test_loss 3.1233713989257814\n",
      "saved model\n",
      "Epoch 140 : train_loss 3.1418558281792537\n",
      "Epoch 140 : test_loss 3.091549133300781\n",
      "saved model\n",
      "Epoch 141 : train_loss 3.118827412923177\n",
      "Epoch 141 : test_loss 3.077988098144531\n",
      "saved model\n",
      "Epoch 142 : train_loss 3.1009768880208335\n",
      "Epoch 142 : test_loss 3.0581160278320314\n",
      "saved model\n",
      "Epoch 143 : train_loss 3.078728515625\n",
      "Epoch 143 : test_loss 3.0468960876464846\n",
      "saved model\n",
      "Epoch 144 : train_loss 3.0613256022135418\n",
      "Epoch 144 : test_loss 3.0134664306640624\n",
      "saved model\n",
      "Epoch 145 : train_loss 3.0446588270399304\n",
      "Epoch 145 : test_loss 3.0051109008789063\n",
      "saved model\n",
      "Epoch 146 : train_loss 3.0207313435872396\n",
      "Epoch 146 : test_loss 2.965625244140625\n",
      "Epoch 147 : train_loss 2.9970714518229165\n",
      "Epoch 147 : test_loss 2.9728748779296876\n",
      "saved model\n",
      "Epoch 148 : train_loss 2.98514446343316\n",
      "Epoch 148 : test_loss 2.94622509765625\n",
      "saved model\n",
      "Epoch 149 : train_loss 2.9669015502929685\n",
      "Epoch 149 : test_loss 2.9317807006835936\n",
      "saved model\n",
      "Epoch 150 : train_loss 2.94781103515625\n",
      "Epoch 150 : test_loss 2.9014820251464846\n",
      "saved model\n",
      "Epoch 151 : train_loss 2.9310414021809894\n",
      "Epoch 151 : test_loss 2.896230895996094\n",
      "saved model\n",
      "Epoch 152 : train_loss 2.9108822631835936\n",
      "Epoch 152 : test_loss 2.8622118225097655\n",
      "Epoch 153 : train_loss 2.897292032877604\n",
      "Epoch 153 : test_loss 2.8689979248046873\n",
      "saved model\n",
      "Epoch 154 : train_loss 2.870765672471788\n",
      "Epoch 154 : test_loss 2.839062713623047\n",
      "saved model\n",
      "Epoch 155 : train_loss 2.8539972330729166\n",
      "Epoch 155 : test_loss 2.8287463073730468\n",
      "saved model\n",
      "Epoch 156 : train_loss 2.83060055202908\n",
      "Epoch 156 : test_loss 2.8044584655761717\n",
      "saved model\n",
      "Epoch 157 : train_loss 2.818708970811632\n",
      "Epoch 157 : test_loss 2.774691619873047\n",
      "saved model\n",
      "Epoch 158 : train_loss 2.795166042751736\n",
      "Epoch 158 : test_loss 2.763650604248047\n",
      "saved model\n",
      "Epoch 159 : train_loss 2.7766874457465276\n",
      "Epoch 159 : test_loss 2.742547332763672\n",
      "Epoch 160 : train_loss 2.763042914496528\n",
      "Epoch 160 : test_loss 2.7437640380859376\n",
      "saved model\n",
      "Epoch 161 : train_loss 2.7418529120551214\n",
      "Epoch 161 : test_loss 2.7012645568847655\n",
      "saved model\n",
      "Epoch 162 : train_loss 2.7215497029622395\n",
      "Epoch 162 : test_loss 2.681814208984375\n",
      "saved model\n",
      "Epoch 163 : train_loss 2.7104870334201387\n",
      "Epoch 163 : test_loss 2.6743424072265625\n",
      "saved model\n",
      "Epoch 164 : train_loss 2.6909060736762154\n",
      "Epoch 164 : test_loss 2.638776580810547\n",
      "saved model\n",
      "Epoch 165 : train_loss 2.672372829861111\n",
      "Epoch 165 : test_loss 2.636204406738281\n",
      "saved model\n",
      "Epoch 166 : train_loss 2.6475499538845484\n",
      "Epoch 166 : test_loss 2.6192796020507814\n",
      "saved model\n",
      "Epoch 167 : train_loss 2.6339014417860245\n",
      "Epoch 167 : test_loss 2.597879913330078\n",
      "saved model\n",
      "Epoch 168 : train_loss 2.6120607435438368\n",
      "Epoch 168 : test_loss 2.5655216064453126\n",
      "saved model\n",
      "Epoch 169 : train_loss 2.601729234483507\n",
      "Epoch 169 : test_loss 2.5540758056640627\n",
      "saved model\n",
      "Epoch 170 : train_loss 2.5767474500868057\n",
      "Epoch 170 : test_loss 2.542305755615234\n",
      "saved model\n",
      "Epoch 171 : train_loss 2.5568943956163195\n",
      "Epoch 171 : test_loss 2.5135252380371096\n",
      "saved model\n",
      "Epoch 172 : train_loss 2.535461113823785\n",
      "Epoch 172 : test_loss 2.500561065673828\n",
      "saved model\n",
      "Epoch 173 : train_loss 2.5205767822265623\n",
      "Epoch 173 : test_loss 2.4824505615234376\n",
      "saved model\n",
      "Epoch 174 : train_loss 2.5011297946506077\n",
      "Epoch 174 : test_loss 2.476700897216797\n",
      "saved model\n",
      "Epoch 175 : train_loss 2.483711188422309\n",
      "Epoch 175 : test_loss 2.455886169433594\n",
      "saved model\n",
      "Epoch 176 : train_loss 2.4781646525065106\n",
      "Epoch 176 : test_loss 2.4387515563964843\n",
      "saved model\n",
      "Epoch 177 : train_loss 2.4544959513346356\n",
      "Epoch 177 : test_loss 2.4189451293945314\n",
      "saved model\n",
      "Epoch 178 : train_loss 2.4364423014322916\n",
      "Epoch 178 : test_loss 2.3982807006835936\n",
      "Epoch 179 : train_loss 2.4228131578233505\n",
      "Epoch 179 : test_loss 2.4066910400390626\n",
      "saved model\n",
      "Epoch 180 : train_loss 2.4006764594184027\n",
      "Epoch 180 : test_loss 2.357205047607422\n",
      "saved model\n",
      "Epoch 181 : train_loss 2.3790251057942706\n",
      "Epoch 181 : test_loss 2.3384225463867185\n",
      "Epoch 182 : train_loss 2.3580319010416666\n",
      "Epoch 182 : test_loss 2.3417115783691407\n",
      "saved model\n",
      "Epoch 183 : train_loss 2.353094190809462\n",
      "Epoch 183 : test_loss 2.328990997314453\n",
      "saved model\n",
      "Epoch 184 : train_loss 2.334828877766927\n",
      "Epoch 184 : test_loss 2.3205633850097658\n",
      "saved model\n",
      "Epoch 185 : train_loss 2.3266185370551216\n",
      "Epoch 185 : test_loss 2.2712805633544924\n",
      "saved model\n",
      "Epoch 186 : train_loss 2.2951060791015623\n",
      "Epoch 186 : test_loss 2.270514862060547\n",
      "saved model\n",
      "Epoch 187 : train_loss 2.2823744710286458\n",
      "Epoch 187 : test_loss 2.2409954833984376\n",
      "Epoch 188 : train_loss 2.2791655815972223\n",
      "Epoch 188 : test_loss 2.253630844116211\n",
      "saved model\n",
      "Epoch 189 : train_loss 2.2577393391927085\n",
      "Epoch 189 : test_loss 2.215369354248047\n",
      "saved model\n",
      "Epoch 190 : train_loss 2.2290134616427952\n",
      "Epoch 190 : test_loss 2.202806121826172\n",
      "saved model\n",
      "Epoch 191 : train_loss 2.214248514811198\n",
      "Epoch 191 : test_loss 2.194218566894531\n",
      "saved model\n",
      "Epoch 192 : train_loss 2.208559875488281\n",
      "Epoch 192 : test_loss 2.1678665466308593\n",
      "saved model\n",
      "Epoch 193 : train_loss 2.1948822564019097\n",
      "Epoch 193 : test_loss 2.1631932067871094\n",
      "saved model\n",
      "Epoch 194 : train_loss 2.1727960205078123\n",
      "Epoch 194 : test_loss 2.1453809204101564\n",
      "saved model\n",
      "Epoch 195 : train_loss 2.1405906168619793\n",
      "Epoch 195 : test_loss 2.112384490966797\n",
      "Epoch 196 : train_loss 2.140663330078125\n",
      "Epoch 196 : test_loss 2.1210294342041016\n",
      "Epoch 197 : train_loss 2.1455716417100694\n",
      "Epoch 197 : test_loss 2.1337486572265627\n",
      "saved model\n",
      "Epoch 198 : train_loss 2.1319680921766495\n",
      "Epoch 198 : test_loss 2.0837838592529296\n",
      "saved model\n",
      "Epoch 199 : train_loss 2.107608201768663\n",
      "Epoch 199 : test_loss 2.065306182861328\n",
      "saved model\n",
      "Epoch 200 : train_loss 2.0714534776475695\n",
      "Epoch 200 : test_loss 2.059202896118164\n",
      "saved model\n",
      "Epoch 201 : train_loss 2.074162116156684\n",
      "Epoch 201 : test_loss 2.0224615478515626\n",
      "Epoch 202 : train_loss 2.0885601806640626\n",
      "Epoch 202 : test_loss 2.055315673828125\n",
      "saved model\n",
      "Epoch 203 : train_loss 2.048717861599392\n",
      "Epoch 203 : test_loss 2.0135741577148436\n",
      "saved model\n",
      "Epoch 204 : train_loss 2.0150025634765627\n",
      "Epoch 204 : test_loss 1.9602001190185547\n",
      "Epoch 205 : train_loss 2.010727735731337\n",
      "Epoch 205 : test_loss 1.9931161499023438\n",
      "Epoch 206 : train_loss 2.028687255859375\n",
      "Epoch 206 : test_loss 1.9984998474121094\n",
      "Epoch 207 : train_loss 2.0346331854926216\n",
      "Epoch 207 : test_loss 1.9912431335449219\n",
      "Epoch 208 : train_loss 2.0166028442382813\n",
      "Epoch 208 : test_loss 1.9737102203369141\n",
      "saved model\n",
      "Epoch 209 : train_loss 1.9872753974066841\n",
      "Epoch 209 : test_loss 1.9535902862548828\n",
      "saved model\n",
      "Epoch 210 : train_loss 1.9539431627061632\n",
      "Epoch 210 : test_loss 1.886985107421875\n",
      "Epoch 211 : train_loss 1.9346436292860243\n",
      "Epoch 211 : test_loss 1.9370212707519532\n",
      "Epoch 212 : train_loss 1.9413213568793404\n",
      "Epoch 212 : test_loss 1.9202866973876953\n",
      "Epoch 213 : train_loss 1.967573221842448\n",
      "Epoch 213 : test_loss 1.9522445068359375\n",
      "Epoch 214 : train_loss 1.9454024590386285\n",
      "Epoch 214 : test_loss 1.8976558227539062\n",
      "saved model\n",
      "Epoch 215 : train_loss 1.8941517401801216\n",
      "Epoch 215 : test_loss 1.8616910552978516\n",
      "saved model\n",
      "Epoch 216 : train_loss 1.839049058702257\n",
      "Epoch 216 : test_loss 1.8088938903808593\n",
      "Epoch 217 : train_loss 1.8327859836154514\n",
      "Epoch 217 : test_loss 1.8307400665283202\n",
      "Epoch 218 : train_loss 1.8494613579644097\n",
      "Epoch 218 : test_loss 1.8389190521240235\n",
      "Epoch 219 : train_loss 1.8835728420681424\n",
      "Epoch 219 : test_loss 1.8579474487304688\n",
      "Epoch 220 : train_loss 1.8657754991319444\n",
      "Epoch 220 : test_loss 1.8432875061035157\n",
      "saved model\n",
      "Epoch 221 : train_loss 1.8305347561306424\n",
      "Epoch 221 : test_loss 1.7943992919921874\n",
      "Epoch 222 : train_loss 1.8365752393934462\n",
      "Epoch 222 : test_loss 1.810861343383789\n",
      "Epoch 223 : train_loss 1.861498514811198\n",
      "Epoch 223 : test_loss 1.819461151123047\n",
      "saved model\n",
      "Epoch 224 : train_loss 1.849966288248698\n",
      "Epoch 224 : test_loss 1.7755377502441407\n",
      "saved model\n",
      "Epoch 225 : train_loss 1.7788152838812934\n",
      "Epoch 225 : test_loss 1.7124056854248046\n",
      "saved model\n",
      "Epoch 226 : train_loss 1.7229340074327257\n",
      "Epoch 226 : test_loss 1.6765300903320313\n",
      "Epoch 227 : train_loss 1.7066937696668836\n",
      "Epoch 227 : test_loss 1.7268336181640624\n",
      "Epoch 228 : train_loss 1.7225606045193143\n",
      "Epoch 228 : test_loss 1.7334063720703126\n",
      "Epoch 229 : train_loss 1.745870361328125\n",
      "Epoch 229 : test_loss 1.7459917907714844\n",
      "Epoch 230 : train_loss 1.7486871236165364\n",
      "Epoch 230 : test_loss 1.698583770751953\n",
      "Epoch 231 : train_loss 1.720929243299696\n",
      "Epoch 231 : test_loss 1.7144212799072265\n",
      "Epoch 232 : train_loss 1.698777570936415\n",
      "Epoch 232 : test_loss 1.71317529296875\n",
      "Epoch 233 : train_loss 1.7191531711154513\n",
      "Epoch 233 : test_loss 1.6998035888671874\n",
      "Epoch 234 : train_loss 1.7430398796929254\n",
      "Epoch 234 : test_loss 1.7146191253662109\n",
      "Epoch 235 : train_loss 1.7501380004882812\n",
      "Epoch 235 : test_loss 1.681622039794922\n",
      "Epoch 236 : train_loss 1.7132865905761718\n",
      "Epoch 236 : test_loss 1.6962299041748048\n",
      "saved model\n",
      "Epoch 237 : train_loss 1.6731929490831163\n",
      "Epoch 237 : test_loss 1.5899175720214844\n",
      "Epoch 238 : train_loss 1.656246348063151\n",
      "Epoch 238 : test_loss 1.6012441864013671\n",
      "Epoch 239 : train_loss 1.6737737087673612\n",
      "Epoch 239 : test_loss 1.6723012390136718\n",
      "Epoch 240 : train_loss 1.683462880452474\n",
      "Epoch 240 : test_loss 1.6398321838378906\n",
      "Epoch 241 : train_loss 1.6668861253526475\n",
      "Epoch 241 : test_loss 1.6299119415283203\n",
      "saved model\n",
      "Epoch 242 : train_loss 1.6324754503038195\n",
      "Epoch 242 : test_loss 1.5863521728515626\n",
      "saved model\n",
      "Epoch 243 : train_loss 1.5867535841200087\n",
      "Epoch 243 : test_loss 1.5659779052734375\n",
      "Epoch 244 : train_loss 1.5841317240397135\n",
      "Epoch 244 : test_loss 1.6005994873046876\n",
      "Epoch 245 : train_loss 1.6261895989312065\n",
      "Epoch 245 : test_loss 1.650930374145508\n",
      "Epoch 246 : train_loss 1.6887749532063803\n",
      "Epoch 246 : test_loss 1.6872313537597656\n",
      "Epoch 247 : train_loss 1.721828121609158\n",
      "Epoch 247 : test_loss 1.6864940490722655\n",
      "Epoch 248 : train_loss 1.6807829996744792\n",
      "Epoch 248 : test_loss 1.5943843383789063\n",
      "saved model\n",
      "Epoch 249 : train_loss 1.5929472283257378\n",
      "Epoch 249 : test_loss 1.547382568359375\n",
      "saved model\n",
      "Epoch 250 : train_loss 1.4978960537380643\n",
      "Epoch 250 : test_loss 1.4519043884277343\n",
      "Epoch 251 : train_loss 1.4775016513400607\n",
      "Epoch 251 : test_loss 1.4831827545166016\n",
      "Epoch 252 : train_loss 1.5272279527452257\n",
      "Epoch 252 : test_loss 1.5140535888671875\n",
      "Epoch 253 : train_loss 1.60914796617296\n",
      "Epoch 253 : test_loss 1.6143705749511719\n",
      "Epoch 254 : train_loss 1.6449155748155382\n",
      "Epoch 254 : test_loss 1.5916046752929687\n",
      "Epoch 255 : train_loss 1.624072041829427\n",
      "Epoch 255 : test_loss 1.5315573272705079\n",
      "Epoch 256 : train_loss 1.5438450113932292\n",
      "Epoch 256 : test_loss 1.4952475128173828\n",
      "Epoch 257 : train_loss 1.4963833584255641\n",
      "Epoch 257 : test_loss 1.4644485473632813\n",
      "Epoch 258 : train_loss 1.4906326836480035\n",
      "Epoch 258 : test_loss 1.5022691192626954\n",
      "Epoch 259 : train_loss 1.5461618414984808\n",
      "Epoch 259 : test_loss 1.5014277191162109\n",
      "Epoch 260 : train_loss 1.5887389763726127\n",
      "Epoch 260 : test_loss 1.6098941040039063\n",
      "Epoch 261 : train_loss 1.6089966871473524\n",
      "Epoch 261 : test_loss 1.6162607879638673\n",
      "Epoch 262 : train_loss 1.6052709418402777\n",
      "Epoch 262 : test_loss 1.5660303344726563\n",
      "Epoch 263 : train_loss 1.5528130425347222\n",
      "Epoch 263 : test_loss 1.5310633544921874\n",
      "Epoch 264 : train_loss 1.522089870876736\n",
      "Epoch 264 : test_loss 1.5633351135253906\n",
      "Epoch 265 : train_loss 1.51088621351454\n",
      "Epoch 265 : test_loss 1.4956660919189453\n",
      "Epoch 266 : train_loss 1.5191213853624133\n",
      "Epoch 266 : test_loss 1.5668011169433593\n",
      "Epoch 267 : train_loss 1.561848147922092\n",
      "Epoch 267 : test_loss 1.5497582702636719\n",
      "Epoch 268 : train_loss 1.569923099093967\n",
      "Epoch 268 : test_loss 1.5397510070800782\n",
      "Epoch 269 : train_loss 1.5103013610839844\n",
      "Epoch 269 : test_loss 1.4711227722167968\n",
      "saved model\n",
      "Epoch 270 : train_loss 1.4859529961480036\n",
      "Epoch 270 : test_loss 1.3926206970214843\n",
      "Epoch 271 : train_loss 1.4915985243055556\n",
      "Epoch 271 : test_loss 1.4599232482910156\n",
      "Epoch 272 : train_loss 1.5156800435384115\n",
      "Epoch 272 : test_loss 1.5352371978759765\n",
      "Epoch 273 : train_loss 1.5226773478190103\n",
      "Epoch 273 : test_loss 1.4654347229003906\n",
      "Epoch 274 : train_loss 1.5634132249620225\n",
      "Epoch 274 : test_loss 1.5382961730957032\n",
      "Epoch 275 : train_loss 1.5905225931803386\n",
      "Epoch 275 : test_loss 1.5935193328857422\n",
      "Epoch 276 : train_loss 1.595389438205295\n",
      "Epoch 276 : test_loss 1.5590174255371094\n",
      "Epoch 277 : train_loss 1.5488126186794704\n",
      "Epoch 277 : test_loss 1.4718634033203124\n",
      "Epoch 278 : train_loss 1.4817565239800348\n",
      "Epoch 278 : test_loss 1.4357064971923827\n",
      "saved model\n",
      "Epoch 279 : train_loss 1.431496070014106\n",
      "Epoch 279 : test_loss 1.3564488525390626\n",
      "Epoch 280 : train_loss 1.4377489929199219\n",
      "Epoch 280 : test_loss 1.5002289276123046\n",
      "Epoch 281 : train_loss 1.542702884250217\n",
      "Epoch 281 : test_loss 1.5897189331054689\n",
      "Epoch 282 : train_loss 1.6250238138834636\n",
      "Epoch 282 : test_loss 1.5797852783203126\n",
      "Epoch 283 : train_loss 1.6322922498914931\n",
      "Epoch 283 : test_loss 1.6486683654785157\n",
      "Epoch 284 : train_loss 1.5207500881618923\n",
      "Epoch 284 : test_loss 1.4105084838867188\n",
      "Epoch 285 : train_loss 1.4337580329047308\n",
      "Epoch 285 : test_loss 1.4216383056640625\n",
      "Epoch 286 : train_loss 1.402531039767795\n",
      "Epoch 286 : test_loss 1.446998733520508\n",
      "Epoch 287 : train_loss 1.506081309000651\n",
      "Epoch 287 : test_loss 1.5341685943603516\n",
      "Epoch 288 : train_loss 1.652332987467448\n",
      "Epoch 288 : test_loss 1.7039114685058594\n",
      "Epoch 289 : train_loss 1.7830882161458332\n",
      "Epoch 289 : test_loss 1.7989593353271485\n",
      "Epoch 290 : train_loss 1.8314885898166233\n",
      "Epoch 290 : test_loss 1.8019465789794922\n",
      "Epoch 291 : train_loss 1.767934078640408\n",
      "Epoch 291 : test_loss 1.7540322875976562\n",
      "Epoch 292 : train_loss 1.6336543646918402\n",
      "Epoch 292 : test_loss 1.5701343383789061\n",
      "Epoch 293 : train_loss 1.5081510077582465\n",
      "Epoch 293 : test_loss 1.3571759185791015\n",
      "Epoch 294 : train_loss 1.4091686062282986\n",
      "Epoch 294 : test_loss 1.3905089111328126\n",
      "Epoch 295 : train_loss 1.3991627875434027\n",
      "Epoch 295 : test_loss 1.4280573120117188\n",
      "Epoch 296 : train_loss 1.4619314609103733\n",
      "Epoch 296 : test_loss 1.5257125701904297\n",
      "Epoch 297 : train_loss 1.6194688551161025\n",
      "Epoch 297 : test_loss 1.6683642425537109\n",
      "Epoch 298 : train_loss 1.7630821024576824\n",
      "Epoch 298 : test_loss 1.8527082214355468\n",
      "Epoch 299 : train_loss 1.8220716925726996\n",
      "Epoch 299 : test_loss 1.7242215270996093\n",
      "Epoch 300 : train_loss 1.7948174438476563\n",
      "Epoch 300 : test_loss 1.626131622314453\n",
      "Epoch 301 : train_loss 1.6424802619086372\n",
      "Epoch 301 : test_loss 1.5987979278564453\n",
      "Epoch 302 : train_loss 1.4999289686414932\n",
      "Epoch 302 : test_loss 1.3830783233642578\n",
      "Epoch 303 : train_loss 1.4802891981336805\n",
      "Epoch 303 : test_loss 1.4695126495361328\n",
      "Epoch 304 : train_loss 1.6057620340983072\n",
      "Epoch 304 : test_loss 1.618029541015625\n",
      "Epoch 305 : train_loss 1.7918529629177518\n",
      "Epoch 305 : test_loss 1.8466183319091798\n",
      "Epoch 306 : train_loss 1.9003509250217014\n",
      "Epoch 306 : test_loss 1.9593072814941406\n",
      "Epoch 307 : train_loss 1.8607335883246527\n",
      "Epoch 307 : test_loss 1.835993423461914\n",
      "Epoch 308 : train_loss 1.785824951171875\n",
      "Epoch 308 : test_loss 1.7431444854736329\n",
      "Epoch 309 : train_loss 1.6906344299316407\n",
      "Epoch 309 : test_loss 1.5849900817871094\n",
      "Epoch 310 : train_loss 1.5836079678005643\n",
      "Epoch 310 : test_loss 1.666459457397461\n",
      "Epoch 311 : train_loss 1.5706707628038195\n",
      "Epoch 311 : test_loss 1.630895278930664\n",
      "Epoch 312 : train_loss 1.5856937221950955\n",
      "Epoch 312 : test_loss 1.5451341552734374\n",
      "Epoch 313 : train_loss 1.6542186957465277\n",
      "Epoch 313 : test_loss 1.7455803833007812\n",
      "Epoch 314 : train_loss 1.832309597439236\n",
      "Epoch 314 : test_loss 1.8042199096679687\n",
      "Epoch 315 : train_loss 1.9083915405273437\n",
      "Epoch 315 : test_loss 1.9232099914550782\n",
      "Epoch 316 : train_loss 1.9887048780653211\n",
      "Epoch 316 : test_loss 1.914491668701172\n",
      "Epoch 317 : train_loss 1.8807842881944445\n",
      "Epoch 317 : test_loss 1.8607145233154296\n",
      "Epoch 318 : train_loss 1.6833374803331163\n",
      "Epoch 318 : test_loss 1.4947242126464844\n",
      "Epoch 319 : train_loss 1.5306704474555122\n",
      "Epoch 319 : test_loss 1.5950542755126953\n",
      "Epoch 320 : train_loss 1.6019447631835937\n",
      "Epoch 320 : test_loss 1.782986572265625\n",
      "Epoch 321 : train_loss 1.8707696601019965\n",
      "Epoch 321 : test_loss 2.0586409301757813\n",
      "Epoch 322 : train_loss 2.1398768649631075\n",
      "Epoch 322 : test_loss 2.2968602905273436\n",
      "Epoch 323 : train_loss 2.3196545817057292\n",
      "Epoch 323 : test_loss 2.4039432373046874\n",
      "Epoch 324 : train_loss 2.3014139607747395\n",
      "Epoch 324 : test_loss 2.317425048828125\n",
      "Epoch 325 : train_loss 2.1423175387912328\n",
      "Epoch 325 : test_loss 2.0365499267578127\n",
      "Epoch 326 : train_loss 1.977063690185547\n",
      "Epoch 326 : test_loss 1.852465072631836\n",
      "Epoch 327 : train_loss 1.7687210150824653\n",
      "Epoch 327 : test_loss 1.701184326171875\n",
      "Epoch 328 : train_loss 1.8068757663302952\n",
      "Epoch 328 : test_loss 1.7880308990478515\n",
      "Epoch 329 : train_loss 1.795165740966797\n",
      "Epoch 329 : test_loss 1.8748916320800781\n",
      "Epoch 330 : train_loss 1.8014171312120226\n",
      "Epoch 330 : test_loss 1.7590952606201171\n",
      "Epoch 331 : train_loss 1.777108378092448\n",
      "Epoch 331 : test_loss 1.7843711547851562\n",
      "Epoch 332 : train_loss 1.9050626831054687\n",
      "Epoch 332 : test_loss 2.0486853637695313\n",
      "Epoch 333 : train_loss 2.143926228841146\n",
      "Epoch 333 : test_loss 2.2474031372070313\n",
      "Epoch 334 : train_loss 2.4064025404188367\n",
      "Epoch 334 : test_loss 2.362459411621094\n",
      "Epoch 335 : train_loss 2.472487104627821\n",
      "Epoch 335 : test_loss 2.3256216430664063\n",
      "Epoch 336 : train_loss 2.3290514255099826\n",
      "Epoch 336 : test_loss 2.173010757446289\n",
      "Epoch 337 : train_loss 2.056183631049262\n",
      "Epoch 337 : test_loss 1.9361393280029298\n",
      "Epoch 338 : train_loss 1.7116163330078125\n",
      "Epoch 338 : test_loss 1.5370297241210937\n",
      "Epoch 339 : train_loss 1.6195404324001736\n",
      "Epoch 339 : test_loss 1.6132480163574219\n",
      "Epoch 340 : train_loss 1.6060567626953124\n",
      "Epoch 340 : test_loss 1.6377941131591798\n",
      "Epoch 341 : train_loss 1.8304952256944444\n",
      "Epoch 341 : test_loss 1.8559708862304687\n",
      "Epoch 342 : train_loss 2.2793505113389756\n",
      "Epoch 342 : test_loss 2.421198760986328\n",
      "Epoch 343 : train_loss 2.60744482421875\n",
      "Epoch 343 : test_loss 2.5849423828125\n",
      "Epoch 344 : train_loss 2.812616611056858\n",
      "Epoch 344 : test_loss 2.9140257568359376\n",
      "Epoch 345 : train_loss 2.7313149820963543\n",
      "Epoch 345 : test_loss 2.718352783203125\n",
      "Epoch 346 : train_loss 2.525141404893663\n",
      "Epoch 346 : test_loss 2.2850361938476564\n",
      "Epoch 347 : train_loss 2.1709583197699653\n",
      "Epoch 347 : test_loss 2.058472930908203\n",
      "Epoch 348 : train_loss 1.9658958468967014\n",
      "Epoch 348 : test_loss 1.8978038635253907\n",
      "Epoch 349 : train_loss 1.9417336290147569\n",
      "Epoch 349 : test_loss 1.9000135040283204\n",
      "Epoch 350 : train_loss 1.963413350423177\n",
      "Epoch 350 : test_loss 1.989730224609375\n",
      "Epoch 351 : train_loss 2.0916238505045572\n",
      "Epoch 351 : test_loss 2.102054977416992\n",
      "Epoch 352 : train_loss 2.2567948337131076\n",
      "Epoch 352 : test_loss 2.2174651489257813\n",
      "Epoch 353 : train_loss 2.3399927435980903\n",
      "Epoch 353 : test_loss 2.3066798706054685\n",
      "Epoch 354 : train_loss 2.348916090223524\n",
      "Epoch 354 : test_loss 2.257755645751953\n",
      "Epoch 355 : train_loss 2.3657269965277776\n",
      "Epoch 355 : test_loss 2.218401107788086\n",
      "Epoch 356 : train_loss 2.2696933932834202\n",
      "Epoch 356 : test_loss 2.1705873413085937\n",
      "Epoch 357 : train_loss 2.172516323513455\n",
      "Epoch 357 : test_loss 2.1076997985839845\n",
      "Epoch 358 : train_loss 2.069495429144965\n",
      "Epoch 358 : test_loss 2.0134232330322264\n",
      "Epoch 359 : train_loss 2.1705208197699655\n",
      "Epoch 359 : test_loss 2.186876953125\n",
      "Epoch 360 : train_loss 2.2099019843207466\n",
      "Epoch 360 : test_loss 2.2416622924804686\n",
      "Epoch 361 : train_loss 2.2758255750868055\n",
      "Epoch 361 : test_loss 2.339224212646484\n",
      "Epoch 362 : train_loss 2.3461326768663193\n",
      "Epoch 362 : test_loss 2.2476685485839845\n",
      "Epoch 363 : train_loss 2.3262779744466147\n",
      "Epoch 363 : test_loss 2.2715763549804686\n",
      "Epoch 364 : train_loss 2.1571309814453126\n",
      "Epoch 364 : test_loss 2.1638683166503907\n",
      "Epoch 365 : train_loss 2.168718485514323\n",
      "Epoch 365 : test_loss 2.015176025390625\n",
      "Epoch 366 : train_loss 2.1605176866319447\n",
      "Epoch 366 : test_loss 2.212992218017578\n",
      "Epoch 367 : train_loss 2.287082017686632\n",
      "Epoch 367 : test_loss 2.3433023071289063\n",
      "Epoch 368 : train_loss 2.4586611870659723\n",
      "Epoch 368 : test_loss 2.511133026123047\n",
      "Epoch 369 : train_loss 2.603357143825955\n",
      "Epoch 369 : test_loss 2.550676818847656\n",
      "Epoch 370 : train_loss 2.702734151204427\n",
      "Epoch 370 : test_loss 2.5979288330078125\n",
      "Epoch 371 : train_loss 2.709178012424045\n",
      "Epoch 371 : test_loss 2.534242706298828\n",
      "Epoch 372 : train_loss 2.601521213107639\n",
      "Epoch 372 : test_loss 2.408564392089844\n",
      "Epoch 373 : train_loss 2.4618575168185766\n",
      "Epoch 373 : test_loss 2.363177764892578\n",
      "Epoch 374 : train_loss 2.2712960476345487\n",
      "Epoch 374 : test_loss 2.101719085693359\n",
      "Epoch 375 : train_loss 2.0971571655273435\n",
      "Epoch 375 : test_loss 2.0236597442626953\n",
      "Epoch 376 : train_loss 1.9781111111111112\n",
      "Epoch 376 : test_loss 1.9520696563720703\n",
      "Epoch 377 : train_loss 1.9482211507161458\n",
      "Epoch 377 : test_loss 1.9511588592529296\n",
      "Epoch 378 : train_loss 2.005056640625\n",
      "Epoch 378 : test_loss 2.255098419189453\n",
      "Epoch 379 : train_loss 2.1505829535590277\n",
      "Epoch 379 : test_loss 2.1247203063964846\n",
      "Epoch 380 : train_loss 2.299570020887587\n",
      "Epoch 380 : test_loss 2.3405096740722655\n",
      "Epoch 381 : train_loss 2.4509439222547744\n",
      "Epoch 381 : test_loss 2.5377730712890627\n",
      "Epoch 382 : train_loss 2.5784613715277778\n",
      "Epoch 382 : test_loss 2.5575779418945315\n",
      "Epoch 383 : train_loss 2.50605524359809\n",
      "Epoch 383 : test_loss 2.3688367919921873\n",
      "Epoch 384 : train_loss 2.399181450737847\n",
      "Epoch 384 : test_loss 2.1368614501953127\n",
      "Epoch 385 : train_loss 2.1831444566514757\n",
      "Epoch 385 : test_loss 1.9877536926269532\n",
      "Epoch 386 : train_loss 1.9793816460503473\n",
      "Epoch 386 : test_loss 1.8471273498535157\n",
      "Epoch 387 : train_loss 1.7853368055555556\n",
      "Epoch 387 : test_loss 1.6123562927246093\n",
      "Epoch 388 : train_loss 1.6954478522406684\n",
      "Epoch 388 : test_loss 1.6093981018066406\n",
      "Epoch 389 : train_loss 1.7457360161675348\n",
      "Epoch 389 : test_loss 1.875914779663086\n",
      "Epoch 390 : train_loss 1.892290490044488\n",
      "Epoch 390 : test_loss 1.9159824523925781\n",
      "Epoch 391 : train_loss 2.0622855224609373\n",
      "Epoch 391 : test_loss 2.0707564697265624\n",
      "Epoch 392 : train_loss 2.3003978610568576\n",
      "Epoch 392 : test_loss 2.3284252014160156\n",
      "Epoch 393 : train_loss 2.506730760362413\n",
      "Epoch 393 : test_loss 2.6272620544433596\n",
      "Epoch 394 : train_loss 2.708602789984809\n",
      "Epoch 394 : test_loss 2.7067040710449217\n",
      "Epoch 395 : train_loss 2.777567355685764\n",
      "Epoch 395 : test_loss 2.8915965881347656\n",
      "Epoch 396 : train_loss 2.9278853013780384\n",
      "Epoch 396 : test_loss 2.871782165527344\n",
      "Epoch 397 : train_loss 2.911062093098958\n",
      "Epoch 397 : test_loss 2.8054838256835937\n",
      "Epoch 398 : train_loss 2.8496479424370658\n",
      "Epoch 398 : test_loss 2.764291809082031\n",
      "Epoch 399 : train_loss 2.6792677341037328\n",
      "Epoch 399 : test_loss 2.529769287109375\n",
      "Epoch 400 : train_loss 2.479588168674045\n",
      "Epoch 400 : test_loss 2.3925653076171876\n",
      "Epoch 401 : train_loss 2.2641120469835068\n",
      "Epoch 401 : test_loss 2.097584167480469\n",
      "Epoch 402 : train_loss 1.992546895345052\n",
      "Epoch 402 : test_loss 1.8262259216308594\n",
      "Epoch 403 : train_loss 1.7698378465440538\n",
      "Epoch 403 : test_loss 1.6165399627685546\n",
      "Epoch 404 : train_loss 1.6217682630750867\n",
      "Epoch 404 : test_loss 1.4062342681884765\n",
      "Epoch 405 : train_loss 1.515577178955078\n",
      "Epoch 405 : test_loss 1.4533433532714843\n",
      "Epoch 406 : train_loss 1.5347796902126736\n",
      "Epoch 406 : test_loss 1.4317045288085937\n",
      "Epoch 407 : train_loss 1.5614511210123698\n",
      "Epoch 407 : test_loss 1.6175935668945312\n",
      "Epoch 408 : train_loss 1.763024898952908\n",
      "Epoch 408 : test_loss 1.7435574340820312\n",
      "Epoch 409 : train_loss 1.9121187947591145\n",
      "Epoch 409 : test_loss 1.9839994354248047\n",
      "Epoch 410 : train_loss 2.082366217719184\n",
      "Epoch 410 : test_loss 2.2476736450195314\n",
      "Epoch 411 : train_loss 2.2650176832411026\n",
      "Epoch 411 : test_loss 2.22937890625\n",
      "Epoch 412 : train_loss 2.3858255683051217\n",
      "Epoch 412 : test_loss 2.3393594360351564\n",
      "Epoch 413 : train_loss 2.4819349907769097\n",
      "Epoch 413 : test_loss 2.350053649902344\n",
      "Epoch 414 : train_loss 2.440262647840712\n",
      "Epoch 414 : test_loss 2.298905517578125\n",
      "Epoch 415 : train_loss 2.3541817355685764\n",
      "Epoch 415 : test_loss 2.0658294525146483\n",
      "Epoch 416 : train_loss 2.162440775553385\n",
      "Epoch 416 : test_loss 2.1481222534179687\n",
      "Epoch 417 : train_loss 2.0727297837999132\n",
      "Epoch 417 : test_loss 1.988361083984375\n",
      "Epoch 418 : train_loss 1.887451941596137\n",
      "Epoch 418 : test_loss 1.7909781494140624\n",
      "Epoch 419 : train_loss 1.6827616984049478\n",
      "Epoch 419 : test_loss 1.5824427795410156\n",
      "Epoch 420 : train_loss 1.6293493279351128\n",
      "Epoch 420 : test_loss 1.5553675231933595\n",
      "Epoch 421 : train_loss 1.625888715955946\n",
      "Epoch 421 : test_loss 1.536272735595703\n",
      "Epoch 422 : train_loss 1.5849761522081163\n",
      "Epoch 422 : test_loss 1.4811678466796876\n",
      "Epoch 423 : train_loss 1.6386591627332898\n",
      "Epoch 423 : test_loss 1.5184794311523437\n",
      "Epoch 424 : train_loss 1.6979437221950955\n",
      "Epoch 424 : test_loss 1.741658706665039\n",
      "Epoch 425 : train_loss 1.7859976230197483\n",
      "Epoch 425 : test_loss 1.8130653381347657\n",
      "Epoch 426 : train_loss 1.8960215725368923\n",
      "Epoch 426 : test_loss 1.9362180786132812\n",
      "Epoch 427 : train_loss 2.0483232828776043\n",
      "Epoch 427 : test_loss 2.0500224304199217\n",
      "Epoch 428 : train_loss 2.126423651801215\n",
      "Epoch 428 : test_loss 2.097323547363281\n",
      "Epoch 429 : train_loss 2.242766160753038\n",
      "Epoch 429 : test_loss 2.163192077636719\n",
      "Epoch 430 : train_loss 2.251248772515191\n",
      "Epoch 430 : test_loss 2.2631224670410157\n",
      "Epoch 431 : train_loss 2.289532653808594\n",
      "Epoch 431 : test_loss 2.2748786010742186\n",
      "Epoch 432 : train_loss 2.346648484971788\n",
      "Epoch 432 : test_loss 2.1857006225585938\n",
      "Epoch 433 : train_loss 2.334718973795573\n",
      "Epoch 433 : test_loss 2.310306427001953\n",
      "Epoch 434 : train_loss 2.3228758205837674\n",
      "Epoch 434 : test_loss 2.223087890625\n",
      "Epoch 435 : train_loss 2.2884805772569443\n",
      "Epoch 435 : test_loss 2.212878128051758\n",
      "Epoch 436 : train_loss 2.2088924492730033\n",
      "Epoch 436 : test_loss 2.0917848205566405\n",
      "Epoch 437 : train_loss 2.11510348171658\n",
      "Epoch 437 : test_loss 2.0738721313476565\n",
      "Epoch 438 : train_loss 2.032468404134115\n",
      "Epoch 438 : test_loss 1.9468732299804687\n",
      "Epoch 439 : train_loss 1.972631585015191\n",
      "Epoch 439 : test_loss 1.85072314453125\n",
      "Epoch 440 : train_loss 1.8743582899305555\n",
      "Epoch 440 : test_loss 1.7781143188476562\n",
      "Epoch 441 : train_loss 1.8141843736436631\n",
      "Epoch 441 : test_loss 1.7675360565185547\n",
      "Epoch 442 : train_loss 1.7718218451605903\n",
      "Epoch 442 : test_loss 1.7110475158691407\n",
      "Epoch 443 : train_loss 1.7581373596191405\n",
      "Epoch 443 : test_loss 1.7820369567871093\n",
      "Epoch 444 : train_loss 1.7229981452094183\n",
      "Epoch 444 : test_loss 1.6726202087402344\n",
      "Epoch 445 : train_loss 1.746530005560981\n",
      "Epoch 445 : test_loss 1.6029049682617187\n",
      "Epoch 446 : train_loss 1.7449517890082464\n",
      "Epoch 446 : test_loss 1.6795578002929688\n",
      "Epoch 447 : train_loss 1.7585152893066407\n",
      "Epoch 447 : test_loss 1.8007745666503907\n",
      "Epoch 448 : train_loss 1.7956967366536458\n",
      "Epoch 448 : test_loss 1.7387412719726563\n",
      "Epoch 449 : train_loss 1.8258968641493056\n",
      "Epoch 449 : test_loss 1.78987744140625\n",
      "Epoch 450 : train_loss 1.8378101230197483\n",
      "Epoch 450 : test_loss 1.767456787109375\n",
      "Epoch 451 : train_loss 1.8400524291992189\n",
      "Epoch 451 : test_loss 1.7614495849609375\n",
      "Epoch 452 : train_loss 1.8525952589246961\n",
      "Epoch 452 : test_loss 1.6868143005371095\n",
      "Epoch 453 : train_loss 1.8580501098632813\n",
      "Epoch 453 : test_loss 1.7459543609619141\n",
      "Epoch 454 : train_loss 1.771799065483941\n",
      "Epoch 454 : test_loss 1.840932418823242\n",
      "Epoch 455 : train_loss 1.783651584201389\n",
      "Epoch 455 : test_loss 1.7379894409179688\n",
      "Epoch 456 : train_loss 1.749502668592665\n",
      "Epoch 456 : test_loss 1.741011749267578\n",
      "Epoch 457 : train_loss 1.7270214267306858\n",
      "Epoch 457 : test_loss 1.675516128540039\n",
      "Epoch 458 : train_loss 1.6702174106174046\n",
      "Epoch 458 : test_loss 1.6275329742431641\n",
      "Epoch 459 : train_loss 1.661808336046007\n",
      "Epoch 459 : test_loss 1.6773406066894532\n",
      "Epoch 460 : train_loss 1.6636495836046008\n",
      "Epoch 460 : test_loss 1.5144547729492188\n",
      "Epoch 461 : train_loss 1.6483284369574653\n",
      "Epoch 461 : test_loss 1.5758912811279298\n",
      "Epoch 462 : train_loss 1.601478515625\n",
      "Epoch 462 : test_loss 1.601889419555664\n",
      "Epoch 463 : train_loss 1.5530536261664496\n",
      "Epoch 463 : test_loss 1.4995204467773438\n",
      "Epoch 464 : train_loss 1.5816515231662327\n",
      "Epoch 464 : test_loss 1.5125044555664062\n",
      "Epoch 465 : train_loss 1.5879317457411024\n",
      "Epoch 465 : test_loss 1.580435073852539\n",
      "Epoch 466 : train_loss 1.5980113118489583\n",
      "Epoch 466 : test_loss 1.5700718841552734\n",
      "Epoch 467 : train_loss 1.6041836751302083\n",
      "Epoch 467 : test_loss 1.5568311920166016\n",
      "Epoch 468 : train_loss 1.6125624254014757\n",
      "Epoch 468 : test_loss 1.5935449523925782\n",
      "Epoch 469 : train_loss 1.6375752733018663\n",
      "Epoch 469 : test_loss 1.6501450958251953\n",
      "Epoch 470 : train_loss 1.6476840277777778\n",
      "Epoch 470 : test_loss 1.56048046875\n",
      "Epoch 471 : train_loss 1.7130779690212674\n",
      "Epoch 471 : test_loss 1.6680394897460937\n",
      "Epoch 472 : train_loss 1.7716762966579862\n",
      "Epoch 472 : test_loss 1.7026632995605469\n",
      "Epoch 473 : train_loss 1.7698787265353733\n",
      "Epoch 473 : test_loss 1.7929339447021484\n",
      "Epoch 474 : train_loss 1.8087887437608507\n",
      "Epoch 474 : test_loss 1.7704454040527344\n",
      "Epoch 475 : train_loss 1.85343069797092\n",
      "Epoch 475 : test_loss 1.8172289581298828\n",
      "Epoch 476 : train_loss 1.8620923800998264\n",
      "Epoch 476 : test_loss 1.8426372680664063\n",
      "Epoch 477 : train_loss 1.9291158515082465\n",
      "Epoch 477 : test_loss 1.8108031616210938\n",
      "Epoch 478 : train_loss 1.9217881266276042\n",
      "Epoch 478 : test_loss 1.915743911743164\n",
      "Epoch 479 : train_loss 1.933237525092231\n",
      "Epoch 479 : test_loss 1.9686908264160157\n",
      "Epoch 480 : train_loss 1.9375891960991753\n",
      "Epoch 480 : test_loss 1.9300057678222655\n",
      "Epoch 481 : train_loss 1.933603278266059\n",
      "Epoch 481 : test_loss 1.9152649841308593\n",
      "Epoch 482 : train_loss 1.935239230685764\n",
      "Epoch 482 : test_loss 1.9257842102050782\n",
      "Epoch 483 : train_loss 1.931427035861545\n",
      "Epoch 483 : test_loss 1.8839425048828125\n",
      "Epoch 484 : train_loss 1.8923721279568142\n",
      "Epoch 484 : test_loss 1.9061813659667968\n",
      "Epoch 485 : train_loss 1.876016316731771\n",
      "Epoch 485 : test_loss 1.8348843383789062\n",
      "Epoch 486 : train_loss 1.8666465420193141\n",
      "Epoch 486 : test_loss 1.7577796630859375\n",
      "Epoch 487 : train_loss 1.8198482971191405\n",
      "Epoch 487 : test_loss 1.7974426879882812\n",
      "Epoch 488 : train_loss 1.7937499152289496\n",
      "Epoch 488 : test_loss 1.7245163879394532\n",
      "Epoch 489 : train_loss 1.7551068522135416\n",
      "Epoch 489 : test_loss 1.762061065673828\n",
      "Epoch 490 : train_loss 1.7175150451660157\n",
      "Epoch 490 : test_loss 1.6826675720214843\n",
      "Epoch 491 : train_loss 1.7014305860731336\n",
      "Epoch 491 : test_loss 1.641597671508789\n",
      "Epoch 492 : train_loss 1.6688372192382812\n",
      "Epoch 492 : test_loss 1.6298630981445312\n",
      "Epoch 493 : train_loss 1.664144290500217\n",
      "Epoch 493 : test_loss 1.6022529907226561\n",
      "Epoch 494 : train_loss 1.655637227376302\n",
      "Epoch 494 : test_loss 1.5888006896972657\n",
      "Epoch 495 : train_loss 1.6320892401801215\n",
      "Epoch 495 : test_loss 1.5268306427001954\n",
      "Epoch 496 : train_loss 1.6338154975043402\n",
      "Epoch 496 : test_loss 1.5794685668945312\n",
      "Epoch 497 : train_loss 1.6306874457465277\n",
      "Epoch 497 : test_loss 1.5906151733398437\n",
      "Epoch 498 : train_loss 1.6403539157443576\n",
      "Epoch 498 : test_loss 1.6109102630615235\n",
      "Epoch 499 : train_loss 1.6398746134440103\n",
      "Epoch 499 : test_loss 1.5979270935058594\n",
      "Epoch 500 : train_loss 1.676287794325087\n",
      "Epoch 500 : test_loss 1.6338275146484376\n",
      "Epoch 501 : train_loss 1.696260230170356\n",
      "Epoch 501 : test_loss 1.6063844451904297\n",
      "Epoch 502 : train_loss 1.7172515462239584\n",
      "Epoch 502 : test_loss 1.6647401428222657\n",
      "Epoch 503 : train_loss 1.7168231845431858\n",
      "Epoch 503 : test_loss 1.6979228668212891\n",
      "Epoch 504 : train_loss 1.738525892469618\n",
      "Epoch 504 : test_loss 1.674402389526367\n",
      "Epoch 505 : train_loss 1.7523136800130208\n",
      "Epoch 505 : test_loss 1.685211181640625\n",
      "Epoch 506 : train_loss 1.7756929490831164\n",
      "Epoch 506 : test_loss 1.68059228515625\n",
      "Epoch 507 : train_loss 1.7866416897243924\n",
      "Epoch 507 : test_loss 1.6552657470703125\n",
      "Epoch 508 : train_loss 1.78873873562283\n",
      "Epoch 508 : test_loss 1.7281338500976562\n",
      "Epoch 509 : train_loss 1.771284891764323\n",
      "Epoch 509 : test_loss 1.7149367065429688\n",
      "Epoch 510 : train_loss 1.7573298916286892\n",
      "Epoch 510 : test_loss 1.6892730407714844\n",
      "Epoch 511 : train_loss 1.727421888563368\n",
      "Epoch 511 : test_loss 1.657960235595703\n",
      "Epoch 512 : train_loss 1.7378231235080295\n",
      "Epoch 512 : test_loss 1.717688232421875\n",
      "Epoch 513 : train_loss 1.7173709377712674\n",
      "Epoch 513 : test_loss 1.6721022033691406\n",
      "Epoch 514 : train_loss 1.6973015238444011\n",
      "Epoch 514 : test_loss 1.5894089508056641\n",
      "Epoch 515 : train_loss 1.6772392001681857\n",
      "Epoch 515 : test_loss 1.658053466796875\n",
      "Epoch 516 : train_loss 1.6757879096137154\n",
      "Epoch 516 : test_loss 1.5837247924804687\n",
      "Epoch 517 : train_loss 1.6438917846679688\n",
      "Epoch 517 : test_loss 1.639599365234375\n",
      "Epoch 518 : train_loss 1.6310362209743923\n",
      "Epoch 518 : test_loss 1.628786148071289\n",
      "Epoch 519 : train_loss 1.6300882703993056\n",
      "Epoch 519 : test_loss 1.5977865600585937\n",
      "Epoch 520 : train_loss 1.584731472439236\n",
      "Epoch 520 : test_loss 1.5655198974609374\n",
      "Epoch 521 : train_loss 1.5703267923990885\n",
      "Epoch 521 : test_loss 1.573812255859375\n",
      "Epoch 522 : train_loss 1.5913790147569444\n",
      "Epoch 522 : test_loss 1.5804996643066407\n",
      "Epoch 523 : train_loss 1.6218360900878905\n",
      "Epoch 523 : test_loss 1.5656280059814454\n",
      "Epoch 524 : train_loss 1.5874915568033854\n",
      "Epoch 524 : test_loss 1.6123469543457032\n",
      "Epoch 525 : train_loss 1.6225647515190973\n",
      "Epoch 525 : test_loss 1.5726724548339843\n",
      "Epoch 526 : train_loss 1.61051904296875\n",
      "Epoch 526 : test_loss 1.6115408935546875\n",
      "Epoch 527 : train_loss 1.6248163146972656\n",
      "Epoch 527 : test_loss 1.6132484741210937\n",
      "Epoch 528 : train_loss 1.6573573404947917\n",
      "Epoch 528 : test_loss 1.6253223419189453\n",
      "Epoch 529 : train_loss 1.6530182189941407\n",
      "Epoch 529 : test_loss 1.639851806640625\n",
      "Epoch 530 : train_loss 1.683597612169054\n",
      "Epoch 530 : test_loss 1.710255828857422\n",
      "Epoch 531 : train_loss 1.6747315300835504\n",
      "Epoch 531 : test_loss 1.7189574584960938\n",
      "Epoch 532 : train_loss 1.7161089850531683\n",
      "Epoch 532 : test_loss 1.7628855895996094\n",
      "Epoch 533 : train_loss 1.7440351155598959\n",
      "Epoch 533 : test_loss 1.7297693939208985\n",
      "Epoch 534 : train_loss 1.7540881958007812\n",
      "Epoch 534 : test_loss 1.7912291564941407\n",
      "Epoch 535 : train_loss 1.77979736328125\n",
      "Epoch 535 : test_loss 1.7829001617431641\n",
      "Epoch 536 : train_loss 1.7938391757541232\n",
      "Epoch 536 : test_loss 1.814165802001953\n",
      "Epoch 537 : train_loss 1.8034178602430555\n",
      "Epoch 537 : test_loss 1.787423812866211\n",
      "Epoch 538 : train_loss 1.8104654676649306\n",
      "Epoch 538 : test_loss 1.833969482421875\n",
      "Epoch 539 : train_loss 1.8434938388400608\n",
      "Epoch 539 : test_loss 1.835068374633789\n",
      "Epoch 540 : train_loss 1.8381379496256511\n",
      "Epoch 540 : test_loss 1.8371690063476562\n",
      "Epoch 541 : train_loss 1.8427831319173178\n",
      "Epoch 541 : test_loss 1.864471939086914\n",
      "Epoch 542 : train_loss 1.843965799967448\n",
      "Epoch 542 : test_loss 1.8038840637207032\n",
      "Epoch 543 : train_loss 1.8525672471788195\n",
      "Epoch 543 : test_loss 1.8570794525146483\n",
      "Epoch 544 : train_loss 1.834293429904514\n",
      "Epoch 544 : test_loss 1.8524247741699218\n",
      "Epoch 545 : train_loss 1.8553301391601562\n",
      "Epoch 545 : test_loss 1.845131134033203\n",
      "Epoch 546 : train_loss 1.847807156032986\n",
      "Epoch 546 : test_loss 1.807641845703125\n",
      "Epoch 547 : train_loss 1.823740509033203\n",
      "Epoch 547 : test_loss 1.7833094635009765\n",
      "Epoch 548 : train_loss 1.8096735975477432\n",
      "Epoch 548 : test_loss 1.8481661071777344\n",
      "Epoch 549 : train_loss 1.7991222262912325\n",
      "Epoch 549 : test_loss 1.8083336486816406\n",
      "Epoch 550 : train_loss 1.7934078504774305\n",
      "Epoch 550 : test_loss 1.8097325134277344\n",
      "Epoch 551 : train_loss 1.790012688530816\n",
      "Epoch 551 : test_loss 1.7438590393066407\n",
      "Epoch 552 : train_loss 1.7724724460177952\n",
      "Epoch 552 : test_loss 1.7778499450683594\n",
      "Epoch 553 : train_loss 1.776650363498264\n",
      "Epoch 553 : test_loss 1.7777998657226564\n",
      "Epoch 554 : train_loss 1.7548370598687066\n",
      "Epoch 554 : test_loss 1.7315022735595702\n",
      "Epoch 555 : train_loss 1.7520468004014758\n",
      "Epoch 555 : test_loss 1.744960174560547\n",
      "Epoch 556 : train_loss 1.745116702609592\n",
      "Epoch 556 : test_loss 1.7103792877197266\n",
      "Epoch 557 : train_loss 1.731556125217014\n",
      "Epoch 557 : test_loss 1.6973064880371094\n",
      "Epoch 558 : train_loss 1.7452303805881075\n",
      "Epoch 558 : test_loss 1.6958861389160156\n",
      "Epoch 559 : train_loss 1.7164816996256511\n",
      "Epoch 559 : test_loss 1.7395788879394531\n",
      "Epoch 560 : train_loss 1.7222828572591147\n",
      "Epoch 560 : test_loss 1.6988738098144531\n",
      "Epoch 561 : train_loss 1.7319865553114149\n",
      "Epoch 561 : test_loss 1.70886328125\n",
      "Epoch 562 : train_loss 1.7437514885796441\n",
      "Epoch 562 : test_loss 1.6858450775146485\n",
      "Epoch 563 : train_loss 1.7383037109375\n",
      "Epoch 563 : test_loss 1.6959307556152343\n",
      "Epoch 564 : train_loss 1.736292731391059\n",
      "Epoch 564 : test_loss 1.742993408203125\n",
      "Epoch 565 : train_loss 1.7687446153428819\n",
      "Epoch 565 : test_loss 1.711728515625\n",
      "Epoch 566 : train_loss 1.7711322326660157\n",
      "Epoch 566 : test_loss 1.7617375946044922\n",
      "Epoch 567 : train_loss 1.7847096455891927\n",
      "Epoch 567 : test_loss 1.7252626342773438\n",
      "Epoch 568 : train_loss 1.7994993184407553\n",
      "Epoch 568 : test_loss 1.7833759307861328\n",
      "Epoch 569 : train_loss 1.82027983601888\n",
      "Epoch 569 : test_loss 1.798484115600586\n",
      "Epoch 570 : train_loss 1.8296876525878907\n",
      "Epoch 570 : test_loss 1.8371572265625\n",
      "Epoch 571 : train_loss 1.8366307033962674\n",
      "Epoch 571 : test_loss 1.8774066619873047\n",
      "Epoch 572 : train_loss 1.8638275451660156\n",
      "Epoch 572 : test_loss 1.8240236968994141\n",
      "Epoch 573 : train_loss 1.8894059583875868\n",
      "Epoch 573 : test_loss 1.8984135894775391\n",
      "Epoch 574 : train_loss 1.9080320909288195\n",
      "Epoch 574 : test_loss 1.9261689758300782\n",
      "Epoch 575 : train_loss 1.933182156032986\n",
      "Epoch 575 : test_loss 1.9013500671386718\n",
      "Epoch 576 : train_loss 1.9674952697753907\n",
      "Epoch 576 : test_loss 1.9541453247070313\n",
      "Epoch 577 : train_loss 1.9753712497287326\n",
      "Epoch 577 : test_loss 1.9798148498535155\n",
      "Epoch 578 : train_loss 2.00103365749783\n",
      "Epoch 578 : test_loss 1.9643988037109374\n",
      "Epoch 579 : train_loss 1.989891608344184\n",
      "Epoch 579 : test_loss 1.9641970672607423\n",
      "Epoch 580 : train_loss 2.025495863172743\n",
      "Epoch 580 : test_loss 2.019844482421875\n",
      "Epoch 581 : train_loss 2.04841549343533\n",
      "Epoch 581 : test_loss 2.057802536010742\n",
      "Epoch 582 : train_loss 2.049148980034722\n",
      "Epoch 582 : test_loss 2.0167808380126955\n",
      "Epoch 583 : train_loss 2.0773427124023436\n",
      "Epoch 583 : test_loss 2.061072570800781\n",
      "Epoch 584 : train_loss 2.0926973673502602\n",
      "Epoch 584 : test_loss 2.0636820068359376\n",
      "Epoch 585 : train_loss 2.0897789916992187\n",
      "Epoch 585 : test_loss 2.089777618408203\n",
      "Epoch 586 : train_loss 2.10003271484375\n",
      "Epoch 586 : test_loss 2.0534505004882813\n",
      "Epoch 587 : train_loss 2.066001254611545\n",
      "Epoch 587 : test_loss 2.087615905761719\n",
      "Epoch 588 : train_loss 2.077911627875434\n",
      "Epoch 588 : test_loss 2.077417236328125\n",
      "Epoch 589 : train_loss 2.0675187038845486\n",
      "Epoch 589 : test_loss 2.068937774658203\n",
      "Epoch 590 : train_loss 2.078695054796007\n",
      "Epoch 590 : test_loss 2.0570819396972655\n",
      "Epoch 591 : train_loss 2.061972859700521\n",
      "Epoch 591 : test_loss 2.0241401824951173\n",
      "Epoch 592 : train_loss 2.0538709445529513\n",
      "Epoch 592 : test_loss 1.9932893676757812\n",
      "Epoch 593 : train_loss 2.050516818576389\n",
      "Epoch 593 : test_loss 2.0133072509765624\n",
      "Epoch 594 : train_loss 2.0269389716254342\n",
      "Epoch 594 : test_loss 2.007082489013672\n",
      "Epoch 595 : train_loss 2.025896762424045\n",
      "Epoch 595 : test_loss 1.9808919067382813\n",
      "Epoch 596 : train_loss 2.003692077636719\n",
      "Epoch 596 : test_loss 1.9866512908935547\n",
      "Epoch 597 : train_loss 1.9868737318250869\n",
      "Epoch 597 : test_loss 1.9576166381835938\n",
      "Epoch 598 : train_loss 1.9630336032443576\n",
      "Epoch 598 : test_loss 1.9597604370117188\n",
      "Epoch 599 : train_loss 1.9552599216037327\n",
      "Epoch 599 : test_loss 1.9324163208007812\n",
      "Epoch 600 : train_loss 1.9551079644097222\n",
      "Epoch 600 : test_loss 1.8864512023925781\n",
      "Epoch 601 : train_loss 1.947050265842014\n",
      "Epoch 601 : test_loss 1.9190467834472655\n",
      "Epoch 602 : train_loss 1.9318070136176215\n",
      "Epoch 602 : test_loss 1.882267807006836\n",
      "Epoch 603 : train_loss 1.9188944736056857\n",
      "Epoch 603 : test_loss 1.8889164123535156\n",
      "Epoch 604 : train_loss 1.911065168592665\n",
      "Epoch 604 : test_loss 1.8771906280517578\n",
      "Epoch 605 : train_loss 1.8965508626302083\n",
      "Epoch 605 : test_loss 1.8678601684570313\n",
      "Epoch 606 : train_loss 1.8954870062934028\n",
      "Epoch 606 : test_loss 1.8822854614257813\n",
      "Epoch 607 : train_loss 1.8978274841308593\n",
      "Epoch 607 : test_loss 1.8333502197265625\n",
      "Epoch 608 : train_loss 1.8872914598253039\n",
      "Epoch 608 : test_loss 1.8539119567871094\n",
      "Epoch 609 : train_loss 1.8849964090983073\n",
      "Epoch 609 : test_loss 1.8534553527832032\n",
      "Epoch 610 : train_loss 1.884283162434896\n",
      "Epoch 610 : test_loss 1.8501576843261718\n",
      "Epoch 611 : train_loss 1.8858007948133682\n",
      "Epoch 611 : test_loss 1.8460025024414062\n",
      "Epoch 612 : train_loss 1.8892038336859809\n",
      "Epoch 612 : test_loss 1.86003564453125\n",
      "Epoch 613 : train_loss 1.8906731635199652\n",
      "Epoch 613 : test_loss 1.8503239288330078\n",
      "Epoch 614 : train_loss 1.8908452758789063\n",
      "Epoch 614 : test_loss 1.8731218566894532\n",
      "Epoch 615 : train_loss 1.886497307671441\n",
      "Epoch 615 : test_loss 1.8399725341796875\n",
      "Epoch 616 : train_loss 1.892019524468316\n",
      "Epoch 616 : test_loss 1.8710780334472656\n",
      "Epoch 617 : train_loss 1.9045196702745226\n",
      "Epoch 617 : test_loss 1.8686609802246095\n",
      "Epoch 618 : train_loss 1.9123221571180555\n",
      "Epoch 618 : test_loss 1.879727783203125\n",
      "Epoch 619 : train_loss 1.9246721666124131\n",
      "Epoch 619 : test_loss 1.8821831512451173\n",
      "Epoch 620 : train_loss 1.934466067843967\n",
      "Epoch 620 : test_loss 1.894847442626953\n",
      "Epoch 621 : train_loss 1.9373117404513889\n",
      "Epoch 621 : test_loss 1.8734061584472657\n",
      "Epoch 622 : train_loss 1.9481471964518229\n",
      "Epoch 622 : test_loss 1.9098744812011719\n",
      "Epoch 623 : train_loss 1.954701873779297\n",
      "Epoch 623 : test_loss 1.8972908172607421\n",
      "Epoch 624 : train_loss 1.9716437411838108\n",
      "Epoch 624 : test_loss 1.922052215576172\n",
      "Epoch 625 : train_loss 1.9709305013020832\n",
      "Epoch 625 : test_loss 1.9271447906494141\n",
      "Epoch 626 : train_loss 1.9882358262803819\n",
      "Epoch 626 : test_loss 1.9459375915527344\n",
      "Epoch 627 : train_loss 1.9950753479003906\n",
      "Epoch 627 : test_loss 2.0023411254882815\n",
      "Epoch 628 : train_loss 2.008303961859809\n",
      "Epoch 628 : test_loss 1.9874817504882814\n",
      "Epoch 629 : train_loss 2.020230678982205\n",
      "Epoch 629 : test_loss 1.9973866119384767\n",
      "Epoch 630 : train_loss 2.0395148179796005\n",
      "Epoch 630 : test_loss 1.995469482421875\n",
      "Epoch 631 : train_loss 2.0465857408311634\n",
      "Epoch 631 : test_loss 2.0275648498535155\n",
      "Epoch 632 : train_loss 2.0462042236328126\n",
      "Epoch 632 : test_loss 2.0336492919921874\n",
      "Epoch 633 : train_loss 2.0576690673828124\n",
      "Epoch 633 : test_loss 2.0382130126953126\n",
      "Epoch 634 : train_loss 2.078421698676215\n",
      "Epoch 634 : test_loss 2.0657924346923826\n",
      "Epoch 635 : train_loss 2.0869912923177085\n",
      "Epoch 635 : test_loss 2.083262176513672\n",
      "Epoch 636 : train_loss 2.091038228352865\n",
      "Epoch 636 : test_loss 2.1088003387451173\n",
      "Epoch 637 : train_loss 2.1110515950520834\n",
      "Epoch 637 : test_loss 2.055840789794922\n",
      "Epoch 638 : train_loss 2.119156202528212\n",
      "Epoch 638 : test_loss 2.1111104278564454\n",
      "Epoch 639 : train_loss 2.1325618150499133\n",
      "Epoch 639 : test_loss 2.1148049621582032\n",
      "Epoch 640 : train_loss 2.1538482326931425\n",
      "Epoch 640 : test_loss 2.1491539459228517\n",
      "Epoch 641 : train_loss 2.1562701619466145\n",
      "Epoch 641 : test_loss 2.1106358337402344\n",
      "Epoch 642 : train_loss 2.166491224500868\n",
      "Epoch 642 : test_loss 2.1572916259765624\n",
      "Epoch 643 : train_loss 2.1738612331814235\n",
      "Epoch 643 : test_loss 2.1459601135253905\n",
      "Epoch 644 : train_loss 2.1847940199110245\n",
      "Epoch 644 : test_loss 2.1777431640625\n",
      "Epoch 645 : train_loss 2.187099880642361\n",
      "Epoch 645 : test_loss 2.1770902404785155\n",
      "Epoch 646 : train_loss 2.193794108072917\n",
      "Epoch 646 : test_loss 2.1708325347900392\n",
      "Epoch 647 : train_loss 2.2051923149956596\n",
      "Epoch 647 : test_loss 2.177687042236328\n",
      "Epoch 648 : train_loss 2.2155682712131077\n",
      "Epoch 648 : test_loss 2.183916809082031\n",
      "Epoch 649 : train_loss 2.204305019802517\n",
      "Epoch 649 : test_loss 2.1671495056152343\n",
      "Epoch 650 : train_loss 2.2148309529622394\n",
      "Epoch 650 : test_loss 2.1790831604003906\n",
      "Epoch 651 : train_loss 2.209810316297743\n",
      "Epoch 651 : test_loss 2.1795269317626955\n",
      "Epoch 652 : train_loss 2.200925984700521\n",
      "Epoch 652 : test_loss 2.175425628662109\n",
      "Epoch 653 : train_loss 2.199227566189236\n",
      "Epoch 653 : test_loss 2.1954218139648436\n",
      "Epoch 654 : train_loss 2.204911383734809\n",
      "Epoch 654 : test_loss 2.17178076171875\n",
      "Epoch 655 : train_loss 2.200755126953125\n",
      "Epoch 655 : test_loss 2.1510030517578125\n",
      "Epoch 656 : train_loss 2.194223626030816\n",
      "Epoch 656 : test_loss 2.183150924682617\n",
      "Epoch 657 : train_loss 2.208194139268663\n",
      "Epoch 657 : test_loss 2.19590185546875\n",
      "Epoch 658 : train_loss 2.210657965766059\n",
      "Epoch 658 : test_loss 2.161304779052734\n",
      "Epoch 659 : train_loss 2.1955474243164064\n",
      "Epoch 659 : test_loss 2.1455634155273438\n",
      "Epoch 660 : train_loss 2.17890911187066\n",
      "Epoch 660 : test_loss 2.1605470123291015\n",
      "Epoch 661 : train_loss 2.1926317749023436\n",
      "Epoch 661 : test_loss 2.159837432861328\n",
      "Epoch 662 : train_loss 2.18338375515408\n",
      "Epoch 662 : test_loss 2.13518537902832\n",
      "Epoch 663 : train_loss 2.1871370917426214\n",
      "Epoch 663 : test_loss 2.154855377197266\n",
      "Epoch 664 : train_loss 2.179536878797743\n",
      "Epoch 664 : test_loss 2.154344482421875\n",
      "Epoch 665 : train_loss 2.173226569281684\n",
      "Epoch 665 : test_loss 2.1737364044189453\n",
      "Epoch 666 : train_loss 2.172315199110243\n",
      "Epoch 666 : test_loss 2.1286053161621092\n",
      "Epoch 667 : train_loss 2.1634480726453993\n",
      "Epoch 667 : test_loss 2.136551834106445\n",
      "Epoch 668 : train_loss 2.1559669325086808\n",
      "Epoch 668 : test_loss 2.1098958892822264\n",
      "Epoch 669 : train_loss 2.1522600775824654\n",
      "Epoch 669 : test_loss 2.1467861938476562\n",
      "Epoch 670 : train_loss 2.1521107109917534\n",
      "Epoch 670 : test_loss 2.114030059814453\n",
      "Epoch 671 : train_loss 2.1507317640516495\n",
      "Epoch 671 : test_loss 2.115401107788086\n",
      "Epoch 672 : train_loss 2.140478834364149\n",
      "Epoch 672 : test_loss 2.085069274902344\n",
      "Epoch 673 : train_loss 2.144032233344184\n",
      "Epoch 673 : test_loss 2.10458642578125\n",
      "Epoch 674 : train_loss 2.1263640747070314\n",
      "Epoch 674 : test_loss 2.0868206634521482\n",
      "Epoch 675 : train_loss 2.1389546576605905\n",
      "Epoch 675 : test_loss 2.1172409057617188\n",
      "Epoch 676 : train_loss 2.13695159233941\n",
      "Epoch 676 : test_loss 2.084385955810547\n",
      "Epoch 677 : train_loss 2.1429516940646702\n",
      "Epoch 677 : test_loss 2.1020441589355467\n",
      "Epoch 678 : train_loss 2.1361600002712673\n",
      "Epoch 678 : test_loss 2.0613642883300782\n",
      "Epoch 679 : train_loss 2.1351644761827258\n",
      "Epoch 679 : test_loss 2.091742034912109\n",
      "Epoch 680 : train_loss 2.1461511366102433\n",
      "Epoch 680 : test_loss 2.103966583251953\n",
      "Epoch 681 : train_loss 2.1383401557074655\n",
      "Epoch 681 : test_loss 2.067770065307617\n",
      "Epoch 682 : train_loss 2.137362772623698\n",
      "Epoch 682 : test_loss 2.0807144165039064\n",
      "Epoch 683 : train_loss 2.134673970540365\n",
      "Epoch 683 : test_loss 2.0890435485839842\n",
      "Epoch 684 : train_loss 2.1457194213867186\n",
      "Epoch 684 : test_loss 2.0990733337402343\n",
      "Epoch 685 : train_loss 2.133542928059896\n",
      "Epoch 685 : test_loss 2.117073486328125\n",
      "Epoch 686 : train_loss 2.1447215711805554\n",
      "Epoch 686 : test_loss 2.127133544921875\n",
      "Epoch 687 : train_loss 2.142061509874132\n",
      "Epoch 687 : test_loss 2.114404525756836\n",
      "Epoch 688 : train_loss 2.1345574476453995\n",
      "Epoch 688 : test_loss 2.071238052368164\n",
      "Epoch 689 : train_loss 2.14967818874783\n",
      "Epoch 689 : test_loss 2.080439544677734\n",
      "Epoch 690 : train_loss 2.1519700453016495\n",
      "Epoch 690 : test_loss 2.1331181030273436\n",
      "Epoch 691 : train_loss 2.1470282050238714\n",
      "Epoch 691 : test_loss 2.1204616394042968\n",
      "Epoch 692 : train_loss 2.1459839206271702\n",
      "Epoch 692 : test_loss 2.137569061279297\n",
      "Epoch 693 : train_loss 2.161474175347222\n",
      "Epoch 693 : test_loss 2.10481120300293\n",
      "Epoch 694 : train_loss 2.15748440890842\n",
      "Epoch 694 : test_loss 2.1190611572265623\n",
      "Epoch 695 : train_loss 2.166796346028646\n",
      "Epoch 695 : test_loss 2.126026824951172\n",
      "Epoch 696 : train_loss 2.1781344604492188\n",
      "Epoch 696 : test_loss 2.1642286071777344\n",
      "Epoch 697 : train_loss 2.1883184814453127\n",
      "Epoch 697 : test_loss 2.1347454833984374\n",
      "Epoch 698 : train_loss 2.181922553168403\n",
      "Epoch 698 : test_loss 2.170994445800781\n",
      "Epoch 699 : train_loss 2.1946231621636283\n",
      "Epoch 699 : test_loss 2.1617290649414063\n",
      "Epoch 700 : train_loss 2.1970113593207463\n",
      "Epoch 700 : test_loss 2.1379287109375\n",
      "Epoch 701 : train_loss 2.1946087510850694\n",
      "Epoch 701 : test_loss 2.188519561767578\n",
      "Epoch 702 : train_loss 2.2125345391167537\n",
      "Epoch 702 : test_loss 2.145373748779297\n",
      "Epoch 703 : train_loss 2.2190294189453126\n",
      "Epoch 703 : test_loss 2.166988555908203\n",
      "Epoch 704 : train_loss 2.211821044921875\n",
      "Epoch 704 : test_loss 2.18009391784668\n",
      "Epoch 705 : train_loss 2.2170101047092015\n",
      "Epoch 705 : test_loss 2.186177001953125\n",
      "Epoch 706 : train_loss 2.235009256998698\n",
      "Epoch 706 : test_loss 2.185747619628906\n",
      "Epoch 707 : train_loss 2.2243382432725696\n",
      "Epoch 707 : test_loss 2.1845323181152345\n",
      "Epoch 708 : train_loss 2.22603899468316\n",
      "Epoch 708 : test_loss 2.2368665771484375\n",
      "Epoch 709 : train_loss 2.241726779513889\n",
      "Epoch 709 : test_loss 2.19035693359375\n",
      "Epoch 710 : train_loss 2.232485860188802\n",
      "Epoch 710 : test_loss 2.2253441467285158\n",
      "Epoch 711 : train_loss 2.2372201470269095\n",
      "Epoch 711 : test_loss 2.1920991821289064\n",
      "Epoch 712 : train_loss 2.2573595581054686\n",
      "Epoch 712 : test_loss 2.216060272216797\n",
      "Epoch 713 : train_loss 2.259161878797743\n",
      "Epoch 713 : test_loss 2.169531494140625\n",
      "Epoch 714 : train_loss 2.2505050252278647\n",
      "Epoch 714 : test_loss 2.2152132568359373\n",
      "Epoch 715 : train_loss 2.2609286702473956\n",
      "Epoch 715 : test_loss 2.2138917846679687\n",
      "Epoch 716 : train_loss 2.248827623155382\n",
      "Epoch 716 : test_loss 2.2010875701904298\n",
      "Epoch 717 : train_loss 2.2574407891167536\n",
      "Epoch 717 : test_loss 2.1993756103515625\n",
      "Epoch 718 : train_loss 2.2524529961480035\n",
      "Epoch 718 : test_loss 2.2081878051757813\n",
      "Epoch 719 : train_loss 2.2474815673828124\n",
      "Epoch 719 : test_loss 2.1778127136230467\n",
      "Epoch 720 : train_loss 2.26038873969184\n",
      "Epoch 720 : test_loss 2.1957726135253908\n",
      "Epoch 721 : train_loss 2.2473125949435766\n",
      "Epoch 721 : test_loss 2.209300094604492\n",
      "Epoch 722 : train_loss 2.2458516235351564\n",
      "Epoch 722 : test_loss 2.220983978271484\n",
      "Epoch 723 : train_loss 2.242438680013021\n",
      "Epoch 723 : test_loss 2.2107310791015626\n",
      "Epoch 724 : train_loss 2.230662360297309\n",
      "Epoch 724 : test_loss 2.183645446777344\n",
      "Epoch 725 : train_loss 2.242030246310764\n",
      "Epoch 725 : test_loss 2.2140224609375\n",
      "Epoch 726 : train_loss 2.2455892401801214\n",
      "Epoch 726 : test_loss 2.229048858642578\n",
      "Epoch 727 : train_loss 2.2319081149631077\n",
      "Epoch 727 : test_loss 2.184587860107422\n",
      "Epoch 728 : train_loss 2.2416660766601564\n",
      "Epoch 728 : test_loss 2.17451123046875\n",
      "Epoch 729 : train_loss 2.2297246365017362\n",
      "Epoch 729 : test_loss 2.228712158203125\n",
      "Epoch 730 : train_loss 2.2353013170030382\n",
      "Epoch 730 : test_loss 2.2010764617919922\n",
      "Epoch 731 : train_loss 2.2323120863172745\n",
      "Epoch 731 : test_loss 2.1917456665039063\n",
      "Epoch 732 : train_loss 2.2349801093207464\n",
      "Epoch 732 : test_loss 2.1967310943603517\n",
      "Epoch 733 : train_loss 2.245662360297309\n",
      "Epoch 733 : test_loss 2.2031480712890623\n",
      "Epoch 734 : train_loss 2.2406683688693576\n",
      "Epoch 734 : test_loss 2.1823914031982423\n",
      "Epoch 735 : train_loss 2.246392781575521\n",
      "Epoch 735 : test_loss 2.2252844848632813\n",
      "Epoch 736 : train_loss 2.243380364312066\n",
      "Epoch 736 : test_loss 2.2144417114257813\n",
      "Epoch 737 : train_loss 2.247777872721354\n",
      "Epoch 737 : test_loss 2.2208087768554687\n",
      "Epoch 738 : train_loss 2.236393561469184\n",
      "Epoch 738 : test_loss 2.2071070556640624\n",
      "Epoch 739 : train_loss 2.245361870659722\n",
      "Epoch 739 : test_loss 2.222289581298828\n",
      "Epoch 740 : train_loss 2.2436542154947916\n",
      "Epoch 740 : test_loss 2.200282958984375\n",
      "Epoch 741 : train_loss 2.241411607530382\n",
      "Epoch 741 : test_loss 2.2159496459960937\n",
      "Epoch 742 : train_loss 2.2446970621744793\n",
      "Epoch 742 : test_loss 2.178598373413086\n",
      "Epoch 743 : train_loss 2.253966979980469\n",
      "Epoch 743 : test_loss 2.224546844482422\n",
      "Epoch 744 : train_loss 2.2434581366644966\n",
      "Epoch 744 : test_loss 2.2367261962890623\n",
      "Epoch 745 : train_loss 2.2545597669813366\n",
      "Epoch 745 : test_loss 2.243017333984375\n",
      "Epoch 746 : train_loss 2.2587164577907988\n",
      "Epoch 746 : test_loss 2.2307706909179688\n",
      "Epoch 747 : train_loss 2.268112508138021\n",
      "Epoch 747 : test_loss 2.2292049560546876\n",
      "Epoch 748 : train_loss 2.278575954861111\n",
      "Epoch 748 : test_loss 2.234542755126953\n",
      "Epoch 749 : train_loss 2.278172159830729\n",
      "Epoch 749 : test_loss 2.243834442138672\n",
      "Epoch 750 : train_loss 2.2829134724934894\n",
      "Epoch 750 : test_loss 2.254493865966797\n",
      "Epoch 751 : train_loss 2.2955094875759547\n",
      "Epoch 751 : test_loss 2.228166015625\n",
      "Epoch 752 : train_loss 2.2996856214735244\n",
      "Epoch 752 : test_loss 2.277733093261719\n",
      "Epoch 753 : train_loss 2.293006652832031\n",
      "Epoch 753 : test_loss 2.2337550048828123\n",
      "Epoch 754 : train_loss 2.3023434109157987\n",
      "Epoch 754 : test_loss 2.266528350830078\n",
      "Epoch 755 : train_loss 2.301653245713976\n",
      "Epoch 755 : test_loss 2.263778411865234\n",
      "Epoch 756 : train_loss 2.3028768310546877\n",
      "Epoch 756 : test_loss 2.261458679199219\n",
      "Epoch 757 : train_loss 2.301724093967014\n",
      "Epoch 757 : test_loss 2.263097930908203\n",
      "Epoch 758 : train_loss 2.3038822089301214\n",
      "Epoch 758 : test_loss 2.258640319824219\n",
      "Epoch 759 : train_loss 2.318155809190538\n",
      "Epoch 759 : test_loss 2.272232727050781\n",
      "Epoch 760 : train_loss 2.316108696831597\n",
      "Epoch 760 : test_loss 2.2673680114746095\n",
      "Epoch 761 : train_loss 2.3188939548068577\n",
      "Epoch 761 : test_loss 2.280773956298828\n",
      "Epoch 762 : train_loss 2.318799045138889\n",
      "Epoch 762 : test_loss 2.2783728942871093\n",
      "Epoch 763 : train_loss 2.3246369154188367\n",
      "Epoch 763 : test_loss 2.306245300292969\n",
      "Epoch 764 : train_loss 2.3220677015516493\n",
      "Epoch 764 : test_loss 2.292622833251953\n",
      "Epoch 765 : train_loss 2.327094740125868\n",
      "Epoch 765 : test_loss 2.313684814453125\n",
      "Epoch 766 : train_loss 2.3324491170247397\n",
      "Epoch 766 : test_loss 2.2878755798339845\n",
      "Epoch 767 : train_loss 2.334945522732205\n",
      "Epoch 767 : test_loss 2.3005189208984373\n",
      "Epoch 768 : train_loss 2.335120327419705\n",
      "Epoch 768 : test_loss 2.31232275390625\n",
      "Epoch 769 : train_loss 2.3350984903971352\n",
      "Epoch 769 : test_loss 2.321432067871094\n",
      "Epoch 770 : train_loss 2.329627882215712\n",
      "Epoch 770 : test_loss 2.318041320800781\n",
      "Epoch 771 : train_loss 2.333283196343316\n",
      "Epoch 771 : test_loss 2.286331695556641\n",
      "Epoch 772 : train_loss 2.3332754855685764\n",
      "Epoch 772 : test_loss 2.3119897155761717\n",
      "Epoch 773 : train_loss 2.337508728027344\n",
      "Epoch 773 : test_loss 2.3107757263183593\n",
      "Epoch 774 : train_loss 2.3312255249023437\n",
      "Epoch 774 : test_loss 2.2990608520507814\n",
      "Epoch 775 : train_loss 2.323140150282118\n",
      "Epoch 775 : test_loss 2.293759521484375\n",
      "Epoch 776 : train_loss 2.325987026638455\n",
      "Epoch 776 : test_loss 2.3011572265625\n",
      "Epoch 777 : train_loss 2.3234737345377603\n",
      "Epoch 777 : test_loss 2.2948968505859373\n",
      "Epoch 778 : train_loss 2.3303748033311633\n",
      "Epoch 778 : test_loss 2.289527648925781\n",
      "Epoch 779 : train_loss 2.3270246310763887\n",
      "Epoch 779 : test_loss 2.3002237548828126\n",
      "Epoch 780 : train_loss 2.3197637464735243\n",
      "Epoch 780 : test_loss 2.3071346435546873\n",
      "Epoch 781 : train_loss 2.321909166124132\n",
      "Epoch 781 : test_loss 2.276655426025391\n",
      "Epoch 782 : train_loss 2.3270780843098957\n",
      "Epoch 782 : test_loss 2.2951624450683594\n",
      "Epoch 783 : train_loss 2.3269882066514755\n",
      "Epoch 783 : test_loss 2.2967945556640625\n",
      "Epoch 784 : train_loss 2.3211864217122398\n",
      "Epoch 784 : test_loss 2.281513977050781\n",
      "Epoch 785 : train_loss 2.321042229546441\n",
      "Epoch 785 : test_loss 2.285508117675781\n",
      "Epoch 786 : train_loss 2.325222568088108\n",
      "Epoch 786 : test_loss 2.2878287353515625\n",
      "Epoch 787 : train_loss 2.3218858371310764\n",
      "Epoch 787 : test_loss 2.263383575439453\n",
      "Epoch 788 : train_loss 2.318947177463108\n",
      "Epoch 788 : test_loss 2.2907005310058595\n",
      "Epoch 789 : train_loss 2.3127110188802082\n",
      "Epoch 789 : test_loss 2.308729736328125\n",
      "Epoch 790 : train_loss 2.3123268229166665\n",
      "Epoch 790 : test_loss 2.259011474609375\n",
      "Epoch 791 : train_loss 2.312348883734809\n",
      "Epoch 791 : test_loss 2.28334423828125\n",
      "Epoch 792 : train_loss 2.3039412367078995\n",
      "Epoch 792 : test_loss 2.265827423095703\n",
      "Epoch 793 : train_loss 2.294606160481771\n",
      "Epoch 793 : test_loss 2.2679559936523437\n",
      "Epoch 794 : train_loss 2.294440863715278\n",
      "Epoch 794 : test_loss 2.2628247985839844\n",
      "Epoch 795 : train_loss 2.2864495849609376\n",
      "Epoch 795 : test_loss 2.2455352172851564\n",
      "Epoch 796 : train_loss 2.2718091023763023\n",
      "Epoch 796 : test_loss 2.242782958984375\n",
      "Epoch 797 : train_loss 2.2705835571289064\n",
      "Epoch 797 : test_loss 2.2264900512695314\n",
      "Epoch 798 : train_loss 2.2659346245659724\n",
      "Epoch 798 : test_loss 2.2507730712890623\n",
      "Epoch 799 : train_loss 2.260841559516059\n",
      "Epoch 799 : test_loss 2.2258648071289064\n",
      "Epoch 800 : train_loss 2.2536626451280384\n",
      "Epoch 800 : test_loss 2.225237854003906\n",
      "Epoch 801 : train_loss 2.2526794162326387\n",
      "Epoch 801 : test_loss 2.216716995239258\n",
      "Epoch 802 : train_loss 2.2443121337890624\n",
      "Epoch 802 : test_loss 2.2084010620117187\n",
      "Epoch 803 : train_loss 2.238811767578125\n",
      "Epoch 803 : test_loss 2.2130253143310545\n",
      "Epoch 804 : train_loss 2.2411448838975696\n",
      "Epoch 804 : test_loss 2.213445770263672\n",
      "Epoch 805 : train_loss 2.2350330200195314\n",
      "Epoch 805 : test_loss 2.2144397735595702\n",
      "Epoch 806 : train_loss 2.234871419270833\n",
      "Epoch 806 : test_loss 2.205191864013672\n",
      "Epoch 807 : train_loss 2.226078586154514\n",
      "Epoch 807 : test_loss 2.2024335174560545\n",
      "Epoch 808 : train_loss 2.216449273003472\n",
      "Epoch 808 : test_loss 2.1869486083984375\n",
      "Epoch 809 : train_loss 2.2229032185872395\n",
      "Epoch 809 : test_loss 2.196429168701172\n",
      "Epoch 810 : train_loss 2.2042996961805557\n",
      "Epoch 810 : test_loss 2.1697844848632815\n",
      "Epoch 811 : train_loss 2.2052723659939235\n",
      "Epoch 811 : test_loss 2.169950241088867\n",
      "Epoch 812 : train_loss 2.1928910047743058\n",
      "Epoch 812 : test_loss 2.1667706909179687\n",
      "Epoch 813 : train_loss 2.196018052842882\n",
      "Epoch 813 : test_loss 2.178222351074219\n",
      "Epoch 814 : train_loss 2.183590799967448\n",
      "Epoch 814 : test_loss 2.166216049194336\n",
      "Epoch 815 : train_loss 2.17885504828559\n",
      "Epoch 815 : test_loss 2.14962109375\n",
      "Epoch 816 : train_loss 2.178570095486111\n",
      "Epoch 816 : test_loss 2.159830047607422\n",
      "Epoch 817 : train_loss 2.1755887925889756\n",
      "Epoch 817 : test_loss 2.1396956939697267\n",
      "Epoch 818 : train_loss 2.167481397840712\n",
      "Epoch 818 : test_loss 2.148142517089844\n",
      "Epoch 819 : train_loss 2.165062337239583\n",
      "Epoch 819 : test_loss 2.154834991455078\n",
      "Epoch 820 : train_loss 2.1705337524414063\n",
      "Epoch 820 : test_loss 2.164987503051758\n",
      "Epoch 821 : train_loss 2.165025200737847\n",
      "Epoch 821 : test_loss 2.1339397735595704\n",
      "Epoch 822 : train_loss 2.162788418240017\n",
      "Epoch 822 : test_loss 2.155229782104492\n",
      "Epoch 823 : train_loss 2.178092013888889\n",
      "Epoch 823 : test_loss 2.1293214721679687\n",
      "Epoch 824 : train_loss 2.176865736219618\n",
      "Epoch 824 : test_loss 2.1349247283935546\n",
      "Epoch 825 : train_loss 2.1719004516601563\n",
      "Epoch 825 : test_loss 2.124977294921875\n",
      "Epoch 826 : train_loss 2.1701700100368924\n",
      "Epoch 826 : test_loss 2.1124547119140624\n",
      "Epoch 827 : train_loss 2.168357611762153\n",
      "Epoch 827 : test_loss 2.1365020599365234\n",
      "Epoch 828 : train_loss 2.177624009874132\n",
      "Epoch 828 : test_loss 2.1367111206054688\n",
      "Epoch 829 : train_loss 2.183152126736111\n",
      "Epoch 829 : test_loss 2.133004455566406\n",
      "Epoch 830 : train_loss 2.178612284342448\n",
      "Epoch 830 : test_loss 2.1410040283203124\n",
      "Epoch 831 : train_loss 2.1802042575412326\n",
      "Epoch 831 : test_loss 2.107557098388672\n",
      "Epoch 832 : train_loss 2.178611551920573\n",
      "Epoch 832 : test_loss 2.141640365600586\n",
      "Epoch 833 : train_loss 2.174038757324219\n",
      "Epoch 833 : test_loss 2.13308154296875\n",
      "Epoch 834 : train_loss 2.180320339626736\n",
      "Epoch 834 : test_loss 2.124612503051758\n",
      "Epoch 835 : train_loss 2.194454026963976\n",
      "Epoch 835 : test_loss 2.1232410430908204\n",
      "Epoch 836 : train_loss 2.1731411946614583\n",
      "Epoch 836 : test_loss 2.1190904998779296\n",
      "Epoch 837 : train_loss 2.180776346842448\n",
      "Epoch 837 : test_loss 2.1491182861328126\n",
      "Epoch 838 : train_loss 2.185132046169705\n",
      "Epoch 838 : test_loss 2.14238818359375\n",
      "Epoch 839 : train_loss 2.1919893663194445\n",
      "Epoch 839 : test_loss 2.1734761962890623\n",
      "Epoch 840 : train_loss 2.202582004123264\n",
      "Epoch 840 : test_loss 2.144223083496094\n",
      "Epoch 841 : train_loss 2.1977301364474826\n",
      "Epoch 841 : test_loss 2.1325802612304687\n",
      "Epoch 842 : train_loss 2.195056070963542\n",
      "Epoch 842 : test_loss 2.153406982421875\n",
      "Epoch 843 : train_loss 2.205663567437066\n",
      "Epoch 843 : test_loss 2.1739230041503905\n",
      "Epoch 844 : train_loss 2.222349331325955\n",
      "Epoch 844 : test_loss 2.1990211791992187\n",
      "Epoch 845 : train_loss 2.2108531155056426\n",
      "Epoch 845 : test_loss 2.198683349609375\n",
      "Epoch 846 : train_loss 2.2405234375\n",
      "Epoch 846 : test_loss 2.1984798583984375\n",
      "Epoch 847 : train_loss 2.2345740966796876\n",
      "Epoch 847 : test_loss 2.1863983306884767\n",
      "Epoch 848 : train_loss 2.2369268324110245\n",
      "Epoch 848 : test_loss 2.233376312255859\n",
      "Epoch 849 : train_loss 2.2366219346788196\n",
      "Epoch 849 : test_loss 2.1805294189453126\n",
      "Epoch 850 : train_loss 2.247007066514757\n",
      "Epoch 850 : test_loss 2.2058077697753906\n",
      "Epoch 851 : train_loss 2.2481666056315106\n",
      "Epoch 851 : test_loss 2.2325132446289064\n",
      "Epoch 852 : train_loss 2.245129374186198\n",
      "Epoch 852 : test_loss 2.2186571350097655\n",
      "Epoch 853 : train_loss 2.2453765597873265\n",
      "Epoch 853 : test_loss 2.2034433898925783\n",
      "Epoch 854 : train_loss 2.2461030002170137\n",
      "Epoch 854 : test_loss 2.2002819976806642\n",
      "Epoch 855 : train_loss 2.2388258802625867\n",
      "Epoch 855 : test_loss 2.206601013183594\n",
      "Epoch 856 : train_loss 2.2448997938368054\n",
      "Epoch 856 : test_loss 2.1850385894775393\n",
      "Epoch 857 : train_loss 2.2437342461480037\n",
      "Epoch 857 : test_loss 2.1744225158691406\n",
      "Epoch 858 : train_loss 2.231338690863715\n",
      "Epoch 858 : test_loss 2.189568908691406\n",
      "Epoch 859 : train_loss 2.242708984375\n",
      "Epoch 859 : test_loss 2.212903015136719\n",
      "Epoch 860 : train_loss 2.2388600328233506\n",
      "Epoch 860 : test_loss 2.2020677490234375\n",
      "Epoch 861 : train_loss 2.2359061008029513\n",
      "Epoch 861 : test_loss 2.179343414306641\n",
      "Epoch 862 : train_loss 2.2132083943684897\n",
      "Epoch 862 : test_loss 2.1857989959716795\n",
      "Epoch 863 : train_loss 2.218910922580295\n",
      "Epoch 863 : test_loss 2.177653350830078\n",
      "Epoch 864 : train_loss 2.214581617567274\n",
      "Epoch 864 : test_loss 2.134914215087891\n",
      "Epoch 865 : train_loss 2.2129422607421874\n",
      "Epoch 865 : test_loss 2.1437164916992186\n",
      "Epoch 866 : train_loss 2.217048536512587\n",
      "Epoch 866 : test_loss 2.1708655395507814\n",
      "Epoch 867 : train_loss 2.200076904296875\n",
      "Epoch 867 : test_loss 2.1360077209472657\n",
      "Epoch 868 : train_loss 2.185868631998698\n",
      "Epoch 868 : test_loss 2.1485225524902343\n",
      "Epoch 869 : train_loss 2.1955080498589408\n",
      "Epoch 869 : test_loss 2.136286407470703\n",
      "Epoch 870 : train_loss 2.1896986490885415\n",
      "Epoch 870 : test_loss 2.1325643005371093\n",
      "Epoch 871 : train_loss 2.1752820027669273\n",
      "Epoch 871 : test_loss 2.106542663574219\n",
      "Epoch 872 : train_loss 2.166824022081163\n",
      "Epoch 872 : test_loss 2.128601821899414\n",
      "Epoch 873 : train_loss 2.1516546495225692\n",
      "Epoch 873 : test_loss 2.1130885314941406\n",
      "Epoch 874 : train_loss 2.144296854654948\n",
      "Epoch 874 : test_loss 2.0884293823242186\n",
      "Epoch 875 : train_loss 2.123432135687934\n",
      "Epoch 875 : test_loss 2.065287628173828\n",
      "Epoch 876 : train_loss 2.1231058010525174\n",
      "Epoch 876 : test_loss 2.0581754150390625\n",
      "Epoch 877 : train_loss 2.1139811537000868\n",
      "Epoch 877 : test_loss 2.053217971801758\n",
      "Epoch 878 : train_loss 2.1087892184787327\n",
      "Epoch 878 : test_loss 2.0486650390625\n",
      "Epoch 879 : train_loss 2.0916568603515624\n",
      "Epoch 879 : test_loss 2.0236651916503905\n",
      "Epoch 880 : train_loss 2.0833389960394966\n",
      "Epoch 880 : test_loss 2.0423978881835936\n",
      "Epoch 881 : train_loss 2.075073689778646\n",
      "Epoch 881 : test_loss 2.011685134887695\n",
      "Epoch 882 : train_loss 2.062320468478733\n",
      "Epoch 882 : test_loss 2.035818328857422\n",
      "Epoch 883 : train_loss 2.064481716579861\n",
      "Epoch 883 : test_loss 2.0672195892333987\n",
      "Epoch 884 : train_loss 2.0546428833007813\n",
      "Epoch 884 : test_loss 2.0215757904052736\n",
      "Epoch 885 : train_loss 2.0577561645507814\n",
      "Epoch 885 : test_loss 2.000135040283203\n",
      "Epoch 886 : train_loss 2.0448944634331596\n",
      "Epoch 886 : test_loss 2.021857421875\n",
      "Epoch 887 : train_loss 2.0345813666449653\n",
      "Epoch 887 : test_loss 2.0281231079101563\n",
      "Epoch 888 : train_loss 2.039716552734375\n",
      "Epoch 888 : test_loss 2.0074430694580077\n",
      "Epoch 889 : train_loss 2.0374296129014757\n",
      "Epoch 889 : test_loss 1.9850669250488282\n",
      "Epoch 890 : train_loss 2.0220067545572915\n",
      "Epoch 890 : test_loss 1.994249481201172\n",
      "Epoch 891 : train_loss 2.027633850097656\n",
      "Epoch 891 : test_loss 2.0023421630859377\n",
      "Epoch 892 : train_loss 2.0132083265516494\n",
      "Epoch 892 : test_loss 2.009639617919922\n",
      "Epoch 893 : train_loss 2.017531012641059\n",
      "Epoch 893 : test_loss 2.001483581542969\n",
      "Epoch 894 : train_loss 2.020443610297309\n",
      "Epoch 894 : test_loss 1.987125701904297\n",
      "Epoch 895 : train_loss 2.0164207695855034\n",
      "Epoch 895 : test_loss 1.9688526000976563\n",
      "Epoch 896 : train_loss 2.0173660956488715\n",
      "Epoch 896 : test_loss 2.0001279907226563\n",
      "Epoch 897 : train_loss 2.0031366034613716\n",
      "Epoch 897 : test_loss 1.9747334594726562\n",
      "Epoch 898 : train_loss 2.009468017578125\n",
      "Epoch 898 : test_loss 1.9609328002929687\n",
      "Epoch 899 : train_loss 2.0074089965820314\n",
      "Epoch 899 : test_loss 1.9718495178222657\n",
      "Epoch 900 : train_loss 1.9938343098958333\n",
      "Epoch 900 : test_loss 1.9666928405761719\n",
      "Epoch 901 : train_loss 2.002209954155816\n",
      "Epoch 901 : test_loss 1.9840468292236328\n",
      "Epoch 902 : train_loss 1.9949568956163195\n",
      "Epoch 902 : test_loss 1.950744384765625\n",
      "Epoch 903 : train_loss 1.9828303765190973\n",
      "Epoch 903 : test_loss 1.9585573425292968\n",
      "Epoch 904 : train_loss 1.982943400065104\n",
      "Epoch 904 : test_loss 1.9639192199707032\n",
      "Epoch 905 : train_loss 1.9796863199869792\n",
      "Epoch 905 : test_loss 1.917179214477539\n",
      "Epoch 906 : train_loss 1.9801454637315539\n",
      "Epoch 906 : test_loss 1.9582501525878906\n",
      "Epoch 907 : train_loss 1.9755639241536458\n",
      "Epoch 907 : test_loss 1.9568925628662108\n",
      "Epoch 908 : train_loss 1.9702816501193576\n",
      "Epoch 908 : test_loss 1.9380190124511718\n",
      "Epoch 909 : train_loss 1.9827421603732638\n",
      "Epoch 909 : test_loss 1.942638656616211\n",
      "Epoch 910 : train_loss 1.9722822943793403\n",
      "Epoch 910 : test_loss 1.9312557983398437\n",
      "Epoch 911 : train_loss 1.976369649251302\n",
      "Epoch 911 : test_loss 1.9200661315917968\n",
      "Epoch 912 : train_loss 1.9610786912706164\n",
      "Epoch 912 : test_loss 1.9302713623046874\n",
      "Epoch 913 : train_loss 1.9807964680989583\n",
      "Epoch 913 : test_loss 1.9379121704101563\n",
      "Epoch 914 : train_loss 1.9758765699598524\n",
      "Epoch 914 : test_loss 1.947011749267578\n",
      "Epoch 915 : train_loss 1.9809738294813368\n",
      "Epoch 915 : test_loss 1.9372221832275391\n",
      "Epoch 916 : train_loss 1.981142578125\n",
      "Epoch 916 : test_loss 1.9217056121826173\n",
      "Epoch 917 : train_loss 1.978571065266927\n",
      "Epoch 917 : test_loss 1.9331820983886718\n",
      "Epoch 918 : train_loss 1.968466800265842\n",
      "Epoch 918 : test_loss 1.9549965209960938\n",
      "Epoch 919 : train_loss 1.9820572170681423\n",
      "Epoch 919 : test_loss 1.940626983642578\n",
      "Epoch 920 : train_loss 1.980861077202691\n",
      "Epoch 920 : test_loss 1.940520736694336\n",
      "Epoch 921 : train_loss 1.985278086344401\n",
      "Epoch 921 : test_loss 1.9474808959960936\n",
      "Epoch 922 : train_loss 1.9817245415581597\n",
      "Epoch 922 : test_loss 1.9176286773681641\n",
      "Epoch 923 : train_loss 1.9693422309027777\n",
      "Epoch 923 : test_loss 1.9208351135253907\n",
      "Epoch 924 : train_loss 1.9798042602539063\n",
      "Epoch 924 : test_loss 1.9364298706054688\n",
      "Epoch 925 : train_loss 1.9662249077690972\n",
      "Epoch 925 : test_loss 1.922036102294922\n",
      "Epoch 926 : train_loss 1.9770116034613716\n",
      "Epoch 926 : test_loss 1.916630126953125\n",
      "Epoch 927 : train_loss 1.9734690280490452\n",
      "Epoch 927 : test_loss 1.9395450286865235\n",
      "Epoch 928 : train_loss 1.9727786017523872\n",
      "Epoch 928 : test_loss 1.9376736450195313\n",
      "Epoch 929 : train_loss 1.9689498562282985\n",
      "Epoch 929 : test_loss 1.9095946350097657\n",
      "Epoch 930 : train_loss 1.9629954901801214\n",
      "Epoch 930 : test_loss 1.927256378173828\n",
      "Epoch 931 : train_loss 1.9663131205240885\n",
      "Epoch 931 : test_loss 1.9320815124511719\n",
      "Epoch 932 : train_loss 1.9637378980848523\n",
      "Epoch 932 : test_loss 1.9235086059570312\n",
      "Epoch 933 : train_loss 1.9656834513346355\n",
      "Epoch 933 : test_loss 1.9245098876953124\n",
      "Epoch 934 : train_loss 1.9641648423936633\n",
      "Epoch 934 : test_loss 1.8817144775390624\n",
      "Epoch 935 : train_loss 1.9638872714572482\n",
      "Epoch 935 : test_loss 1.907785614013672\n",
      "Epoch 936 : train_loss 1.9574803805881076\n",
      "Epoch 936 : test_loss 1.9370730438232422\n",
      "Epoch 937 : train_loss 1.9489861382378473\n",
      "Epoch 937 : test_loss 1.9107510681152344\n",
      "Epoch 938 : train_loss 1.9488048299153646\n",
      "Epoch 938 : test_loss 1.9274946136474609\n",
      "Epoch 939 : train_loss 1.9510926310221355\n",
      "Epoch 939 : test_loss 1.9065064239501952\n",
      "Epoch 940 : train_loss 1.956106180826823\n",
      "Epoch 940 : test_loss 1.9080827331542969\n",
      "Epoch 941 : train_loss 1.947884524875217\n",
      "Epoch 941 : test_loss 1.8840558471679687\n",
      "Epoch 942 : train_loss 1.949959208170573\n",
      "Epoch 942 : test_loss 1.9353463134765625\n",
      "Epoch 943 : train_loss 1.9402895982530382\n",
      "Epoch 943 : test_loss 1.9182017211914062\n",
      "Epoch 944 : train_loss 1.9440011935763888\n",
      "Epoch 944 : test_loss 1.8980896759033203\n",
      "Epoch 945 : train_loss 1.9397107916937935\n",
      "Epoch 945 : test_loss 1.884128402709961\n",
      "Epoch 946 : train_loss 1.9281619466145834\n",
      "Epoch 946 : test_loss 1.8931553955078124\n",
      "Epoch 947 : train_loss 1.9316346503363715\n",
      "Epoch 947 : test_loss 1.895384033203125\n",
      "Epoch 948 : train_loss 1.928323954264323\n",
      "Epoch 948 : test_loss 1.869942855834961\n",
      "Epoch 949 : train_loss 1.9240889892578126\n",
      "Epoch 949 : test_loss 1.8802118072509766\n",
      "Epoch 950 : train_loss 1.9122319675021702\n",
      "Epoch 950 : test_loss 1.8797010040283204\n",
      "Epoch 951 : train_loss 1.9057988009982638\n",
      "Epoch 951 : test_loss 1.8659554748535156\n",
      "Epoch 952 : train_loss 1.9020621948242187\n",
      "Epoch 952 : test_loss 1.8474195556640625\n",
      "Epoch 953 : train_loss 1.8968090582953558\n",
      "Epoch 953 : test_loss 1.8325933227539062\n",
      "Epoch 954 : train_loss 1.886310814751519\n",
      "Epoch 954 : test_loss 1.8314542388916015\n",
      "Epoch 955 : train_loss 1.8728352525499132\n",
      "Epoch 955 : test_loss 1.8398452758789063\n",
      "Epoch 956 : train_loss 1.868564002143012\n",
      "Epoch 956 : test_loss 1.8316902770996093\n",
      "Epoch 957 : train_loss 1.864482191297743\n",
      "Epoch 957 : test_loss 1.8349613342285156\n",
      "Epoch 958 : train_loss 1.8581611531575521\n",
      "Epoch 958 : test_loss 1.8118185882568358\n",
      "Epoch 959 : train_loss 1.8414590589735242\n",
      "Epoch 959 : test_loss 1.804133514404297\n",
      "Epoch 960 : train_loss 1.8403404032389323\n",
      "Epoch 960 : test_loss 1.7974149169921876\n",
      "Epoch 961 : train_loss 1.8293935275607638\n",
      "Epoch 961 : test_loss 1.7837384185791016\n",
      "Epoch 962 : train_loss 1.8274841715494792\n",
      "Epoch 962 : test_loss 1.7805670623779297\n",
      "Epoch 963 : train_loss 1.8162757771809896\n",
      "Epoch 963 : test_loss 1.7774989471435547\n",
      "Epoch 964 : train_loss 1.8056602647569444\n",
      "Epoch 964 : test_loss 1.7835828704833985\n",
      "Epoch 965 : train_loss 1.8139329189724391\n",
      "Epoch 965 : test_loss 1.75612255859375\n",
      "Epoch 966 : train_loss 1.8033782653808594\n",
      "Epoch 966 : test_loss 1.7627835998535155\n",
      "Epoch 967 : train_loss 1.7895334913465712\n",
      "Epoch 967 : test_loss 1.7726647491455079\n",
      "Epoch 968 : train_loss 1.7974689059787325\n",
      "Epoch 968 : test_loss 1.7397450408935546\n",
      "Epoch 969 : train_loss 1.7901444023980035\n",
      "Epoch 969 : test_loss 1.7294151000976563\n",
      "Epoch 970 : train_loss 1.7817120768229167\n",
      "Epoch 970 : test_loss 1.7634647521972657\n",
      "Epoch 971 : train_loss 1.7921875440809463\n",
      "Epoch 971 : test_loss 1.7392120666503905\n",
      "Epoch 972 : train_loss 1.7815653584798177\n",
      "Epoch 972 : test_loss 1.7573673706054687\n",
      "Epoch 973 : train_loss 1.7708914048936633\n",
      "Epoch 973 : test_loss 1.725549530029297\n",
      "Epoch 974 : train_loss 1.7807255282931858\n",
      "Epoch 974 : test_loss 1.7172746276855468\n",
      "Epoch 975 : train_loss 1.7766158243815104\n",
      "Epoch 975 : test_loss 1.700916976928711\n",
      "Epoch 976 : train_loss 1.7626452874077692\n",
      "Epoch 976 : test_loss 1.714454391479492\n",
      "Epoch 977 : train_loss 1.777722659640842\n",
      "Epoch 977 : test_loss 1.721209991455078\n",
      "Epoch 978 : train_loss 1.7671398111979166\n",
      "Epoch 978 : test_loss 1.7015281372070312\n",
      "Epoch 979 : train_loss 1.7689066738552517\n",
      "Epoch 979 : test_loss 1.7136841430664063\n",
      "Epoch 980 : train_loss 1.7590611843532986\n",
      "Epoch 980 : test_loss 1.722246307373047\n",
      "Epoch 981 : train_loss 1.7614612494574653\n",
      "Epoch 981 : test_loss 1.7385560302734375\n",
      "Epoch 982 : train_loss 1.757310319688585\n",
      "Epoch 982 : test_loss 1.7029170532226563\n",
      "Epoch 983 : train_loss 1.753651638454861\n",
      "Epoch 983 : test_loss 1.7233257598876952\n",
      "Epoch 984 : train_loss 1.7555790134006077\n",
      "Epoch 984 : test_loss 1.737324737548828\n",
      "Epoch 985 : train_loss 1.7374540473090279\n",
      "Epoch 985 : test_loss 1.7316339111328125\n",
      "Epoch 986 : train_loss 1.7503416476779514\n",
      "Epoch 986 : test_loss 1.747073715209961\n",
      "Epoch 987 : train_loss 1.7590548468695746\n",
      "Epoch 987 : test_loss 1.7422584838867188\n",
      "Epoch 988 : train_loss 1.744403096516927\n",
      "Epoch 988 : test_loss 1.740565948486328\n",
      "Epoch 989 : train_loss 1.749684349907769\n",
      "Epoch 989 : test_loss 1.7080142364501953\n",
      "Epoch 990 : train_loss 1.7425405680338542\n",
      "Epoch 990 : test_loss 1.6873682861328125\n",
      "Epoch 991 : train_loss 1.7452502000596788\n",
      "Epoch 991 : test_loss 1.6831668548583985\n",
      "Epoch 992 : train_loss 1.7575554945203993\n",
      "Epoch 992 : test_loss 1.6950941162109374\n",
      "Epoch 993 : train_loss 1.736824974907769\n",
      "Epoch 993 : test_loss 1.6792197570800782\n",
      "Epoch 994 : train_loss 1.7416045633951822\n",
      "Epoch 994 : test_loss 1.7507491455078126\n",
      "Epoch 995 : train_loss 1.75609910413954\n",
      "Epoch 995 : test_loss 1.7120513305664062\n",
      "Epoch 996 : train_loss 1.7659410603841146\n",
      "Epoch 996 : test_loss 1.737350845336914\n",
      "Epoch 997 : train_loss 1.7324516635470921\n",
      "Epoch 997 : test_loss 1.7214001159667969\n",
      "Epoch 998 : train_loss 1.732630367702908\n",
      "Epoch 998 : test_loss 1.7035975341796874\n",
      "Epoch 999 : train_loss 1.748055667453342\n",
      "Epoch 999 : test_loss 1.7140148620605469\n"
     ]
    }
   ],
   "source": [
    "# Now create a dataloader and train the flows\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "model = my_model()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=1e-5, amsgrad=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer,'min')\n",
    "\n",
    "lr = []\n",
    "losses_no_scheduler = []\n",
    "\n",
    "\n",
    "n_train = 9000\n",
    "n_test = N - n_train\n",
    "data_train = TensorDataset(x[:n_train], y[:n_train])\n",
    "data_test = TensorDataset(x[n_train:], y[n_train:])\n",
    "train_loader = DataLoader(data_train, 300)\n",
    "test_loader = DataLoader(data_test, 300, shuffle=False)\n",
    "\n",
    "best_test_loss = np.inf\n",
    "for epoch in range(1000):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for data_ in train_loader:\n",
    "        data_ = [d.cuda() for d in data_]\n",
    "        loss,dict = model.compute_kld(data_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    for data_ in test_loader:\n",
    "        data_ = [d.cuda() for d in data_]\n",
    "        with torch.no_grad():\n",
    "            loss,dict = model.compute_kld(data_)\n",
    "            test_loss += loss.item()\n",
    "    if test_loss < best_test_loss:\n",
    "        print('saved model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        best_test_loss = test_loss  \n",
    "    # scheduler.step(test_loss)\n",
    "    lr.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    losses_no_scheduler.append(test_loss)\n",
    "    print(f'Epoch {epoch} : train_loss {train_loss/len(data_train)}')\n",
    "    print(f'Epoch {epoch} : test_loss {test_loss/len(data_test)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa441f2b3d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(losses_no_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the learned flows \n",
    "\n",
    "model = my_model()\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "idx = 0\n",
    "z_simulated_y = model.sample_from_y(y[idx], K = 500).detach().cpu()\n",
    "z_simulated_x = model.sample_from_x(x[idx], K = 500).detach().cpu()\n",
    "\n",
    "true_z_x = sample_from_uni_posterior_x(x[idx], K=500).detach().cpu()\n",
    "true_z_y = sample_from_uni_posterior_y(y[idx], K=500).detach().cpu()\n",
    "print(true_z_x.shape)\n",
    "\n",
    "print(x[idx],y[idx])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.scatter(z_simulated_x[:,0],z_simulated_x[:,1])\n",
    "ax.scatter(z_simulated_y[:,0],z_simulated_y[:,1])\n",
    "\n",
    "ax.scatter(true_z_x[0],true_z_x[1])\n",
    "ax.scatter(true_z_y[0],true_z_y[1])\n",
    "\n",
    "ax.set_xlim([-10,10])\n",
    "ax.set_ylim([-10,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4e60172799112451e675ed8f39ac039c9c5784e1c33f4b2379e9e5e54444ff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
