{ "comment" : "JMVAE model with the exact same procedure for training : linear augmentation of the regularization and no fixed encoders/decoders.",

  "experiment": "jmvae_exact/mnist_svhn",
  "model": "jnf_mnist_svhn_dcca",
  "obj": "jmvae_nf",
  "K": 1,
  "recon_losses" : ["normal", "normal"],
  "looser": false,
  "llik_scaling": 0,
  "batch_size": 128, "learning_rate" : 1e-3,
  "epochs": 6,
  "latent_dim": 20,
  "num_hidden_layers": 1,
  "use_pretrain": "",
  "learn_prior": false,
  "logp": false,
  "print_freq": 0,
  "no_analytics": false,
  "no_cuda": false,
  "seed": 1,
  "dist": "normal",
  "data_path": "../data/",
  "skip_warmup": false,
  "warmup": 3,
  "no_nf": true,
  "beta_prior": 1,
  "beta_kl": 1,
  "decrease_beta_kl": 1,
  "fix_decoders": false,
  "fix_jencoder": false,
  "no_recon": true,
  "freq_analytics": 5,
  "dcca": false,
  "device": "cuda",
  "linear_warmup": true,
  "save_joint":false,
  "wandb_experiment":"mnist_svhn"
  }